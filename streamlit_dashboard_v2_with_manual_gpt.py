"""
RAG è©•ä¼°å„€è¡¨æ¿ v2.0 - æ•´åˆäººå·¥ GPT è©•å¯©
==========================================

ç‰¹è‰²åŠŸèƒ½ï¼š
1. ä¸‰å±¤è©•ä¼°æ¶æ§‹ï¼ˆé—œéµè©ã€èªç¾©ã€GPTï¼‰
2. GPT Prompt ç”Ÿæˆå™¨ï¼ˆå¯ç›´æ¥è¤‡è£½åˆ° ChatGPTï¼‰
3. GPT å›æ‡‰è²¼ä¸Šå€ï¼ˆäººå·¥è¼¸å…¥è©•åˆ†ï¼‰
4. æ‰€æœ‰æŒ‡æ¨™åŒæ™‚å‘ˆç¾ã€å³æ™‚æ›´æ–°
5. å®Œæ•´çš„è³‡æ–™è¼¸å…¥è¼¸å‡ºæ”¯æ´

ç‰ˆæœ¬ï¼š2.0 with Manual GPT
"""

import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import numpy as np
from datetime import datetime
import json
import os
import re
import ast
from copy import deepcopy
from pathlib import Path
from typing import Any, Dict, List, Tuple

from rag_evaluation_two_models_v2 import RAGEvaluatorV2
from evaluation_history_manager import EvaluationHistoryManager
from combined_filter_tab import render_combined_filter_tab

# è¨­å®šé é¢é…ç½®
st.set_page_config(
    page_title="RAG è©•ä¼°å„€è¡¨æ¿ v2.0 - æ•´åˆäººå·¥ GPT è©•å¯©",
    #page_icon="ğŸ†š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– session state
if 'comparison_results' not in st.session_state:
    st.session_state.comparison_results = None
if 'evaluator_instance' not in st.session_state:
    st.session_state.evaluator_instance = None
if 'current_question_idx' not in st.session_state:
    st.session_state.current_question_idx = 0
if 'gpt_responses_original' not in st.session_state:
    st.session_state.gpt_responses_original = {}
if 'gpt_responses_optimized' not in st.session_state:
    st.session_state.gpt_responses_optimized = {}
if 'history_manager' not in st.session_state:
    st.session_state.history_manager = EvaluationHistoryManager()
if 'current_excel_filename' not in st.session_state:
    st.session_state.current_excel_filename = None
if 'gpt_responses_loaded' not in st.session_state:
    st.session_state.gpt_responses_loaded = False
if 'display_semantic_metric' not in st.session_state:
    st.session_state.display_semantic_metric = True
if 'gpt_selected_dimensions' not in st.session_state:
    st.session_state.gpt_selected_dimensions = ['relevance', 'completeness', 'accuracy', 'faithfulness']
if 'gpt_dimension_weights' not in st.session_state:
    st.session_state.gpt_dimension_weights = {
        'relevance': 0.25,
        'completeness': 0.25,
        'accuracy': 0.25,
        'faithfulness': 0.25,
    }

# å·¥å…·å‡½æ•¸
def split_into_sentences(text: str):
    """å°‡æ–‡å­—åˆ‡æˆå¥å­åˆ—è¡¨"""
    if not isinstance(text, str) or not text.strip():
        return []

    sentences = re.split(r'[ã€‚ï¼ï¼Ÿ!?\n\r]+', text)
    return [s.strip() for s in sentences if s.strip()]


def compute_sentence_similarity(evaluator: RAGEvaluatorV2, sentences, answer: str):
    """è¨ˆç®—æ¯å€‹å¥å­èˆ‡å›ç­”çš„èªç¾©ç›¸ä¼¼åº¦"""
    results = []
    if not evaluator or not evaluator.enable_semantic or not sentences:
        return results

    if answer is None or (isinstance(answer, float) and np.isnan(answer)) or str(answer).strip() == "":
        return [(sent, 0.0) for sent in sentences]

    for sent in sentences:
        try:
            score, _ = evaluator.calculate_semantic_similarity(sent, answer)
        except Exception:
            score = 0.0
        results.append((sent, score))

    return results


def format_reference_to_list(reference_text: str):
    """å°‡åƒè€ƒå…§å®¹æ‹†æˆä¾¿æ–¼å±•ç¤ºçš„æ¢åˆ—"""
    if not isinstance(reference_text, str):
        return []

    lines = [line.strip() for line in reference_text.splitlines() if line.strip()]
    if len(lines) <= 1:
        # è‹¥åªæœ‰å–®æ®µï¼Œç›¡é‡ä¾æ•¸å­—æˆ–é “è™Ÿå†æ‹†åˆ†
        segments = re.split(r'\d+\.|[ã€ï¼›;]', reference_text)
        lines = [seg.strip() for seg in segments if seg.strip()]
    return lines


# GPT Prompt ç”Ÿæˆå‡½æ•¸
def generate_gpt_prompt(question, reference_keywords, answer, version="optimized", question_id=1):
    """ç”Ÿæˆ GPT è©•å¯© prompt - å«æ–°ç‰ˆå››æŒ‡æ¨™èˆ‡è¨ºæ–·æ¬„ä½"""
    prompt = f"""ä½ æ˜¯ä¸€ä½åš´è¬¹çš„ LLM è¼¸å‡ºè©•å¯©å°ˆå®¶ã€‚è«‹ä¾ä¸‹è¿°ã€Œæ˜ç¢ºé‡åŒ–è¦å‰‡èˆ‡ç´šè·æ¨™æº–ã€è©•åˆ†ï¼Œä¸¦åªè¼¸å‡ºè¦å®šçš„ JSONã€‚
æ‰€æœ‰åˆ†æ•¸éƒ½å¿…é ˆå¯è¿½æº¯åˆ°ã€Œå¯æ•¸çš„åˆ†å­/åˆ†æ¯ã€ï¼Œå€é–“å…§å…è¨±ç·šæ€§å…§æ’ä¸¦å››æ¨äº”å…¥ç‚ºæ•´æ•¸ã€‚

ã€å•é¡Œ {question_id}ã€‘
{question}

ã€å¿…é ˆåŒ…å«çš„é—œéµè³‡è¨Šè¦é»ï¼ˆCoverage æ¨™çš„ï¼‰ã€‘
{reference_keywords}

ã€å¾…è©•ä¼°å›ç­”ï¼ˆ{version} ç‰ˆæœ¬ï¼‰ã€‘
{answer}

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æŒ‡æ¨™èˆ‡æ‰“åˆ†è¦å‰‡ï¼ˆå…¬å¼ â†’ ç™¾åˆ†æ¯” â†’ åˆ†æ•¸ â†’ å€é–“ç´šè·èˆ‡æ¨™æº–ï¼‰

ğŸ¯ 1) ç›¸é—œæ€§ Relevance
ç›®çš„ï¼šå›ç­”æ˜¯å¦ç·Šæ‰£å•é¡Œä¸»è»¸ï¼Œé¿å…é›¢é¡Œã€‚
è¨ˆç®—ï¼š
  - æ‹†å¥ï¼›é€å¥æ¨™è¨˜ On-Topic(1) / Off-Topic(0)
  - p = On-Topic å¥æ•¸ Ã· ç¸½å¥æ•¸
  - åˆ†æ•¸ = p Ã— 100ï¼ˆå››æ¨äº”å…¥ç‚ºæ•´æ•¸ï¼‰
ç´šè·ï¼ˆ10%â†’10åˆ†ï¼‰ï¼š
  | å€é–“ | èªªæ˜ |
|------|------|
| 90â€“100ï¼ˆpâ‰¥0.90ï¼‰ | å¹¾ä¹å…¨å¥è²¼é¡Œï¼›åƒ…æœ‰æ¥µå°‘èˆ‡ä¸»è»¸ç„¡é—œçš„å¥å­ |
| 80â€“89ï¼ˆ0.80â‰¤p<0.90ï¼‰ | é«˜åº¦è²¼é¡Œï¼›å¶æœ‰æ¬¡è¦é›¢é¡Œ |
| 70â€“79ï¼ˆ0.70â‰¤p<0.80ï¼‰ | å¤§å¤šè²¼é¡Œï¼›å­˜åœ¨å¯è¾¨è­˜çš„é›¢é¡Œç‰‡æ®µ |
| 60â€“69ï¼ˆ0.60â‰¤p<0.70ï¼‰ | éåŠè²¼é¡Œï¼›ä¸»è»¸å¯è¾¨ä½†ç„¦é»é¬†æ•£ |
| 50â€“59ï¼ˆ0.50â‰¤p<0.60ï¼‰ | è²¼é¡Œèˆ‡é›¢é¡Œç›¸è¿‘ï¼›ç„¦é»æ¨¡ç³Š |
| 40â€“49ï¼ˆ0.40â‰¤p<0.50ï¼‰ | é›¢é¡Œç‚ºå¤šï¼›è³‡è¨Šæ··é›œ |
| 30â€“39ï¼ˆ0.30â‰¤p<0.40ï¼‰ | å°‘æ•¸è²¼é¡Œï¼›ä¸»é¡Œåå·®æ˜é¡¯ |
| 20â€“29ï¼ˆ0.20â‰¤p<0.30ï¼‰ | å¹¾ä¹æœªèšç„¦ä¸»é¡Œ |
| 10â€“19ï¼ˆ0.10â‰¤p<0.20ï¼‰ | åƒ…æ¥µå°‘å…§å®¹ç›¸é—œ |
| 0â€“9ï¼ˆp<0.10ï¼‰ | å®Œå…¨é›¢é¡Œ |

ç†ç”±è¦æ±‚ï¼šåˆ—å‡ºè²¼é¡Œ/é›¢é¡Œä»£è¡¨å¥ä¸¦è§£é‡‹æ­¸é¡ä¾æ“šï¼›ä¸¦åœ¨ `score_drivers.positive`/`score_drivers.negative` ä¸­æ¨™ç¤ºå“ªäº›å¥å­æé«˜/æ‹‰ä½åˆ†æ•¸ï¼ˆè‡³å°‘å„ 1 å¥ï¼Œè‹¥å­˜åœ¨ï¼‰ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ“‹ 2) å®Œæ•´æ€§ Completenessï¼ˆCoverage + Depth/Context/Synthesisï¼‰
ç›®çš„ï¼šæ˜¯å¦æ¶µè“‹å¿…è¦è¦é»ï¼Œä¸”åœ¨è¦†è“‹åŒæ™‚å…·è¶³å¤ æ·±åº¦èˆ‡æ•´åˆã€‚
è¨ˆç®—æ­¥é©Ÿï¼š
1) ç‚ºæ¯å€‹å¿…è¦è¦é»æ¨™è¨˜ï¼šCovered / Partially / Missing
2) è¦†è“‹ç‡ q = (Covered + 0.5Ã—Partially) Ã· ç¸½è¦é»æ•¸
3) å°æ‰€æœ‰ Covered æˆ– Partially çš„è¦é»ï¼Œè©•ä¸‰å€‹è³ªé‡é¢å‘ï¼ˆå„ 0.80â€“1.00ï¼‰ï¼š
   - Depthï¼ˆæ·±åº¦ï¼‰  - Context Utilizationï¼ˆè„ˆçµ¡é‹ç”¨ï¼‰  - Information Synthesisï¼ˆæ•´åˆï¼‰
   å–å¹³å‡ç‚º k âˆˆ [0.80, 1.00]
4) è‹¥å›è¦†æ˜é¡¯ã€Œæ·ºè–„ã€ï¼ˆåƒ…åè©ç¾…åˆ—ã€ç¼ºä¹è§£é‡‹/å› æœ/ä¾‹è­‰ï¼‰ï¼Œå¼·åˆ¶ k â‰¤ 0.89ï¼ˆæœ€çµ‚åˆ†æ•¸ä¸Šé™ 89ï¼‰
5) æœ€çµ‚åˆ†æ•¸ = q Ã— 100 Ã— kï¼ˆå››æ¨äº”å…¥ç‚ºæ•´æ•¸ï¼‰

ğŸ“ˆ K ä¿‚æ•¸å“è³ªåˆ†ç´šè¡¨ï¼ˆç­‰å¯¬åŠé–‹å€é–“ï¼›ä¸‰ç¶­å…±ç”¨ï¼‰
- Excellent:   0.96 â‰¤ k â‰¤ 1.00  â€”â€” æ·±å…¥åˆ†æã€çµæ§‹æ¸…æ™°ã€å«å› æœ/ä¾‹è­‰ã€è·¨è¦é»æ•´åˆå®Œå–„
- Very Good:   0.92 â‰¤ k < 0.96 â€”â€” èªªæ˜å…·é«”ã€è„ˆçµ¡åˆç†ã€é‚è¼¯æ¸…æ¥šï¼›å¶æœ‰ç•¥ç°¡ä¹‹è™•
- Good:        0.88 â‰¤ k < 0.92 â€”â€” å¤šæ•¸è¦é»æœ‰åŸºæœ¬è§£é‡‹èˆ‡éŠœæ¥ï¼›æ•´åˆæˆ–ä¾‹è­‰ç•¥å¼±
- Fair:        0.84 â‰¤ k < 0.88 â€”â€” ä»¥åè©/å®šç¾©ç‚ºä¸»ï¼Œå°‘å› æœ/æ•´åˆï¼›åè¡¨å±¤æ•˜è¿°
- Marginal:    0.80 â‰¤ k < 0.84 â€”â€” å¤šç‚ºç¾…åˆ—æˆ–ç‰‡æ®µå¥ï¼Œç¼ºä¹æ¨è«–èˆ‡è„ˆçµ¡é€£çµ

ç¶œåˆç´šè·ï¼ˆä»¥ qÃ—k ç‚ºåŸºç¤ï¼Œ10%â†’10åˆ†ï¼‰ï¼š
  | å€é–“ | èªªæ˜ |
|------|------|
| 90â€“100ï¼ˆqÃ—kâ‰¥0.90ï¼‰ | å¹¾ä¹å…¨è¦†è“‹ï¼›èªªæ˜å……åˆ†ã€æ•´åˆä½³ï¼›ç„¡æ·ºè–„è·¡è±¡ |
| 80â€“89ï¼ˆ0.80â‰¤qÃ—k<0.90ï¼‰ | æ¥è¿‘å®Œæ•´ï¼›å°‘é‡ç´°ç¯€ä¸è¶³æˆ–é€£çµç•¥å¼± |
| 70â€“79ï¼ˆ0.70â‰¤qÃ—k<0.80ï¼‰ | å¤§éƒ¨åˆ†è¦†è“‹ï¼›å¤šè™•è§£é‡‹åç°¡ç•¥æˆ–ç¼ºä¾‹è­‰ |
| 60â€“69ï¼ˆ0.60â‰¤qÃ—k<0.70ï¼‰ | ç´„ 2/3 è¦†è“‹ï¼›éƒ¨åˆ†è¦é»æ¬ æ·±åº¦æˆ–è„ˆçµ¡ |
| 50â€“59ï¼ˆ0.50â‰¤qÃ—k<0.60ï¼‰ | è¦†è“‹æœ‰é™ï¼›ç¼ºå°‘æ ¸å¿ƒæ®µè½æˆ–æ·±åº¦ |
| 40â€“49ï¼ˆ0.40â‰¤qÃ—k<0.50ï¼‰ | è¦†è“‹ç‡ä¸è¶³ä¸€åŠï¼›å…§å®¹ç‰‡æ®µåŒ– |
| 30â€“39ï¼ˆ0.30â‰¤qÃ—k<0.40ï¼‰ | åƒ…å°‘æ•¸è¦é»è§¸åŠï¼›çµæ§‹é¬†æ•£ |
| 20â€“29ï¼ˆ0.20â‰¤qÃ—k<0.30ï¼‰ | é›¶ç¢æè¿°ï¼›ç¼ºä¹æ•´é«”è„ˆçµ¡ |
| 10â€“19ï¼ˆ0.10â‰¤qÃ—k<0.20ï¼‰ | å¹¾ä¹æœªæ¶µè“‹å¿…è¦è³‡è¨Š |
| 0â€“9ï¼ˆqÃ—k<0.10ï¼‰ | å®Œå…¨ç¼ºä¹å®Œæ•´æ€§èˆ‡æ·±åº¦ |
ç†ç”±è¦æ±‚ï¼šåˆ—å‡º Coveredï¼Partiallyï¼Missing æ¸…å–®ï¼Œä¸¦æ¨™ç¤ºä¸‰ç¶­è³ªé‡åˆ†æ•¸èˆ‡æ˜¯å¦è§¸ç™¼æ·ºè–„ä¸Šé™ï¼›åŒæ™‚åœ¨ `score_drivers.positive` ä¸­æŒ‡å‡ºåŠ åˆ†çš„é—œéµè¦é»æˆ–æ·±åº¦èªªæ˜ï¼Œåœ¨ `score_drivers.negative` ä¸­é»åç¼ºæ¼æˆ–æ·ºè–„çš„è¦é»ã€‚

ï¼ˆè¨ºæ–·è¼¸å‡ºå»ºè­°ï¼šç‚ºèˆ‡ä½ çš„ Excel å°é½Šï¼Œè«‹åŒæ™‚å›å‚³ Coverage èˆ‡ k å­æ§‹é¢ï¼Œä»¥åˆ©å„€è¡¨æ¿åˆ†è§£ï¼‰
- coverage_debug: ç›´æ¥è¼¸å‡º q èˆ‡æ¯å€‹è¦é»çš„æ¨™è¨˜çµæœ
- k_debug: è¼¸å‡ºä¸‰ç¶­åˆ†æ•¸ï¼ˆdepth/context/synthesisï¼‰èˆ‡å¹³å‡ k

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
âœ… 3) æº–ç¢ºæ€§ Accuracy / Correctness
ç›®çš„ï¼šåˆ¤æ–·å›ç­”çš„äº‹å¯¦æ­£ç¢ºæ€§ã€‚
è¨ˆç®—ï¼š
  - æ‹†è§£ç‚ºåŸå­äº‹å¯¦ S1..Snï¼Œæ¨™è¨˜ Correct / Incorrect / Unverifiable
  - r = Correct Ã· (Correct + Incorrect)
  - åˆ†æ•¸ = r Ã— 100
ç´šè·ï¼ˆ10%â†’10åˆ†ï¼‰ï¼š
| å€é–“ | èªªæ˜ |
|------|------|
| 90â€“100ï¼ˆrâ‰¥0.90ï¼‰ | å®Œå…¨æ­£ç¢ºæˆ–åƒ…æ¥µå°‘èª¤å·® |
| 80â€“89ï¼ˆ0.80â‰¤r<0.90ï¼‰ | é«˜æ­£ç¢ºç‡ï¼›è¼•å¾®éŒ¯èª¤ä¸å½±éŸ¿ä¸»æ—¨ |
| 70â€“79ï¼ˆ0.70â‰¤r<0.80ï¼‰ | å¤šæ•¸æ­£ç¢ºï¼›å€‹åˆ¥éŒ¯èª¤è¼•å¾®å½±éŸ¿å¯ä¿¡åº¦ |
| 60â€“69ï¼ˆ0.60â‰¤r<0.70ï¼‰ | ç´„ 2/3 æ­£ç¢ºï¼›éŒ¯èª¤é–‹å§‹å½±éŸ¿ç†è§£ |
| 50â€“59ï¼ˆ0.50â‰¤r<0.60ï¼‰ | æ­£ç¢ºèˆ‡éŒ¯èª¤ç›¸è¿‘ï¼›éœ€æ˜é¡¯ä¿®æ­£ |
| 40â€“49ï¼ˆ0.40â‰¤r<0.50ï¼‰ | éŒ¯èª¤åå¤šï¼›æ•´é«”å¤±æº– |
| 30â€“39ï¼ˆ0.30â‰¤r<0.40ï¼‰ | éŒ¯èª¤ç‚ºä¸»ï¼›å¯ä¿¡åº¦ä½ |
| 20â€“29ï¼ˆ0.20â‰¤r<0.30ï¼‰ | åƒ…å°‘éƒ¨åˆ†æ­£ç¢º |
| 10â€“19ï¼ˆ0.10â‰¤r<0.20ï¼‰ | å¹¾ä¹å…¨éŒ¯ |
| 0â€“9ï¼ˆr<0.10ï¼‰ | å®Œå…¨éŒ¯èª¤æˆ–è™›æ§‹ |
ç†ç”±è¦æ±‚ï¼šåˆ—å‡ºä¸»è¦æ­£ç¢º/éŒ¯èª¤äº‹å¯¦èˆ‡éŒ¯èª¤é¡å‹ï¼ˆäº‹å¯¦/æ™‚é–“/æ•¸é‡/å¼•ç”¨â€¦ï¼‰ï¼›ä¸¦åœ¨ `score_drivers.positive`/`score_drivers.negative` ä¸­åˆ†åˆ¥æ¨™è¨»åŠ åˆ†èˆ‡æ‰£åˆ†çš„äº‹å¯¦å¥ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ğŸ§­ 4) ç¯„åœéµå¾ª Scope Adherenceï¼ˆæ²¿ç”¨è¼¸å‡ºæ¬„ä½å faithfulnessï¼‰
ç›®çš„ï¼šé¿å…ã€Œéåº¦è£œå……ã€â€”â€” ä¸è¦åŠ å…¥è¶…å‡ºé¡Œç›®æˆ–å¿…è¦è¦é»çš„å†—é¤˜å…§å®¹ï¼Œå³ä½¿æ­£ç¢ºä¹Ÿæ‰£åˆ†ã€‚
æ¨™è¨»ï¼š
  - Essentialï¼šå¿…è¦è³‡è¨Šï¼ˆç›´æ¥è¦†è“‹è¦é»/å›ç­”æ‰€éœ€ï¼‰
  - Supportiveï¼šè¼”åŠ©è³‡è¨Šï¼ˆæœ‰åŠ©ç†è§£çš„å¿…è¦è£œå……ï¼‰
  - Extraneousï¼šå†—é¤˜/è¶…ç¯„åœå»¶ä¼¸
è¨ˆç®—ï¼š
  - g = (Essential + 0.5Ã—Supportive) Ã· (Essential + Supportive + Extraneous)
  - åˆ†æ•¸ = g Ã— 100ï¼ˆå››æ¨äº”å…¥ç‚ºæ•´æ•¸ï¼‰
ç´šè·ï¼ˆ10%â†’10åˆ†ï¼‰ï¼š
| å€é–“ | èªªæ˜ |
|------|------|
| 90â€“100ï¼ˆgâ‰¥0.90ï¼‰ | å¹¾ä¹å…¨ç‚ºå¿…è¦/é©åº¦æ”¯æ´å…§å®¹ï¼›ç„¡å†—é¤˜ |
| 80â€“89ï¼ˆ0.80â‰¤g<0.90ï¼‰ | å°‘é‡å†—é¤˜ï¼›ä»èšç„¦ä¸»é¡Œ |
| 70â€“79ï¼ˆ0.70â‰¤g<0.80ï¼‰ | ä¸»é«”èšç„¦ä½†å¯è¦‹å†—é¤˜ç‰‡æ®µ |
| 60â€“69ï¼ˆ0.60â‰¤g<0.70ï¼‰ | å†—é¤˜èˆ‡å¿…è¦å…§å®¹ç›¸ç•¶ |
| 50â€“59ï¼ˆ0.50â‰¤g<0.60ï¼‰ | å†—é¤˜åå¤šï¼›ç„¦é»æ¨¡ç³Š |
| 40â€“49ï¼ˆ0.40â‰¤g<0.50ï¼‰ | æ˜é¡¯å†—é¤˜ï¼›ä¸»é¡Œç¨€é‡‹ |
| 30â€“39ï¼ˆ0.30â‰¤g<0.40ï¼‰ | å†—é¤˜å¤šæ–¼å¿…è¦ï¼›åé›¢é‡é» |
| 20â€“29ï¼ˆ0.20â‰¤g<0.30ï¼‰ | å¹¾ä¹å¤šç‚ºå†—é¤˜ï¼›ä¸»è»¸æ¨¡ç³Š |
| 10â€“19ï¼ˆ0.10â‰¤g<0.20ï¼‰ | æ¥µåº¦å†—è´…ï¼›èˆ‡ä¸»é¡Œä¸ç¬¦ |
| 0â€“9ï¼ˆg<0.10ï¼‰ | å¹¾ä¹å®Œå…¨ç‚ºå†—é¤˜å…§å®¹ |
ç†ç”±è¦æ±‚ï¼šåˆ—å‡º Essentialï¼Supportiveï¼Extraneous ä»£è¡¨å¥ä¸¦è§£é‡‹ï¼›ä¸¦æ–¼ `score_drivers.positive`/`score_drivers.negative` ä¸­èªªæ˜èšç„¦æˆ–å†—é¤˜çš„å¥å­ã€‚

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# æ•´é«”åˆ†æ•¸ Overall
overall = mean(relevance, completeness, accuracy, scope_adherence)
ï¼ˆè¼¸å‡ºæ¬„ä½ä»å‘½åç‚º faithfulness ä»¥å‘ä¸‹ç›¸å®¹ï¼Œä½†èªç¾©=Scope Adherenceï¼‰

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# è«‹è¼¸å‡ºåš´æ ¼ JSONï¼ˆå‹¿å¤¾å¸¶å¤šé¤˜æ–‡å­—ï¼‰
{{
  "question_id": {question_id},
  "relevance": {{
    "score": <0-100 æ•´æ•¸>,
    "p": <0-1 å°æ•¸>,
    "on_topic_examples": ["å¥å­1", "å¥å­2"],
    "off_topic_examples": ["å¥å­A", "å¥å­B"],
    "score_drivers": {{
      "positive": ["..."],
      "negative": ["..."]
    }},
    "reasoning": "è²¼é¡Œ/é›¢é¡Œæ¯”ä¾‹èˆ‡åˆ†æ•¸ç†ç”±"
  }},
  "completeness": {{
    "score": <0-100 æ•´æ•¸>,
    "q": <0-1 å°æ•¸>,
    "k": <0.80-1.00 å°æ•¸>,
    "covered": ["è¦é»1", "è¦é»2"],
    "partially": ["è¦é»A"],
    "missing": ["è¦é»B"],
    "quality_notes": {{
      "depth": <0.80-1.00>,
      "context_utilization": <0.80-1.00>,
      "information_synthesis": <0.80-1.00>,
      "shallow_flag": <true/false>
    }},
    "coverage_debug": {{
      "points": [
        {{"label":"è¦é»1","status":"Covered"}},
        {{"label":"è¦é»2","status":"Partially"}}
      ],
      "q": <0-1 å°æ•¸>
    }},
    "k_debug": {{
      "depth": <0.80-1.00>,
      "context": <0.80-1.00>,
      "synthesis": <0.80-1.00>,
      "k_avg": <0.80-1.00>
    }},
    "score_drivers": {{
      "positive": ["..."],
      "negative": ["..."]
    }},
    "reasoning": "Score = q*100*k çš„ä¾†é¾å»è„ˆèˆ‡ç´šè·å°æ‡‰"
  }},
  "accuracy": {{
    "score": <0-100 æ•´æ•¸>,
    "r": <0-1 å°æ•¸>,
    "correct_facts": ["..."],
    "incorrect_facts": ["..."],
    "unverifiable_facts": ["..."],
    "score_drivers": {{
      "positive": ["..."],
      "negative": ["..."]
    }},
    "reasoning": "ä¸»è¦æ­£ç¢º/éŒ¯èª¤é»èˆ‡éŒ¯èª¤é¡å‹"
  }},
  "faithfulness": {{
    "score": <0-100 æ•´æ•¸>,     // èªç¾© = Scope Adherenceï¼ˆä¸éåº¦è£œå……ï¼‰
    "g": <0-1 å°æ•¸>,
    "essential": ["..."],
    "supportive": ["..."],
    "extraneous": ["..."],
    "score_drivers": {{
      "positive": ["..."],
      "negative": ["..."]
    }},
    "reasoning": "åˆ—ç¤ºå¿…è¦èˆ‡å†—é¤˜å…§å®¹ï¼Œèªªæ˜ç„¦é»æ§åˆ¶"
  }},
  "overall": <0-100 æ•´æ•¸>,
  "overall_reasoning": "å››ç¶­å¹³å‡åˆ†æ•¸èˆ‡ä¸»è¦è©•èªæ‘˜è¦"
}}"""
    return prompt


GPT_DIMENSION_KEYS = ['relevance', 'completeness', 'accuracy', 'faithfulness']


def normalize_gpt_schema(parsed):
    """å°‡ GPT è©•åˆ†çµæœçµ±ä¸€ç‚ºæ–°ç‰ˆå·¢ç‹€çµæ§‹æ ¼å¼ã€‚"""
    if not isinstance(parsed, dict):
        return parsed

    data = deepcopy(parsed)

    for dim in GPT_DIMENSION_KEYS:
        value = data.get(dim)
        reasoning_key = f"{dim}_reasoning"

        if isinstance(value, dict):
            if reasoning_key in data and 'reasoning' not in value and data[reasoning_key]:
                value['reasoning'] = data[reasoning_key]
            if reasoning_key in data:
                data.pop(reasoning_key)
            continue

        block = {}
        if isinstance(value, (int, float)):
            block['score'] = float(value)
        elif value is not None:
            try:
                block['score'] = float(value)
            except (TypeError, ValueError):
                block['raw_value'] = value

        if reasoning_key in data:
            reasoning_val = data.pop(reasoning_key)
            if isinstance(reasoning_val, str) and reasoning_val.strip():
                block['reasoning'] = reasoning_val

        data[dim] = block

    return data


def get_dimension_block(gpt_data: dict, dim: str) -> dict:
    """å®‰å…¨å–å¾—æŒ‡å®šç¶­åº¦çš„è³‡æ–™å€å¡Šï¼ˆæ”¯æ´èˆŠæœ‰æ‰å¹³æ ¼å¼ï¼‰ã€‚"""
    if not isinstance(gpt_data, dict):
        return {}

    value = gpt_data.get(dim)
    if isinstance(value, dict):
        return value

    block = {}
    if isinstance(value, (int, float)):
        block['score'] = float(value)
    elif value is not None:
        try:
            block['score'] = float(value)
        except (TypeError, ValueError):
            block['raw_value'] = value

    reasoning_key = f"{dim}_reasoning"
    reasoning_val = gpt_data.get(reasoning_key)
    if isinstance(reasoning_val, str) and reasoning_val.strip():
        block['reasoning'] = reasoning_val

    return block


def get_dimension_score(gpt_data: dict, dim: str) -> float | None:
    """å–å¾—æŒ‡å®šç¶­åº¦çš„åˆ†æ•¸ï¼ˆè‹¥ç„¡åˆ†æ•¸å‰‡å›å‚³ Noneï¼‰ã€‚"""
    block = get_dimension_block(gpt_data, dim)
    score = block.get('score')
    if isinstance(score, (int, float)):
        return float(score)
    try:
        return float(score)
    except (TypeError, ValueError):
        return None


def get_dimension_reasoning(gpt_data: dict, dim: str) -> str:
    """å–å¾—æŒ‡å®šç¶­åº¦çš„èªªæ˜æ–‡å­—ã€‚"""
    block = get_dimension_block(gpt_data, dim)
    reasoning = block.get('reasoning')
    if reasoning is None:
        return ""
    return str(reasoning).strip()


def get_dimension_metric(gpt_data: dict, dim: str, metric: str):
    """å–å¾—æŒ‡å®šç¶­åº¦ä¸‹çš„é¡å¤–æŒ‡æ¨™å€¼ï¼ˆå¦‚ pã€qã€k ç­‰ï¼‰ã€‚"""
    block = get_dimension_block(gpt_data, dim)
    return block.get(metric)


def _safe_text(value) -> str:
    if value is None or (isinstance(value, float) and np.isnan(value)):
        return ""
    return str(value)


def serialize_json_field(value) -> str:
    """å°‡ list/dict è½‰ç‚ºå­—ä¸²ï¼Œæ–¹ä¾¿å­˜å…¥è¡¨æ ¼ã€‚"""
    if value is None:
        return ""
    if isinstance(value, (list, dict)):
        try:
            return json.dumps(value, ensure_ascii=False)
        except (TypeError, ValueError):
            return json.dumps(str(value), ensure_ascii=False)
    return str(value)


def create_llm_judge_rows(
    excel_file: str,
    question_id: int,
    question_text: str,
    reference_keywords: str,
    answer_text: str,
    version_label: str,
    gpt_data: dict
) -> List[Dict[str, Any]]:
    """å°‡ GPT è©•åˆ†çš„å·¢ç‹€ JSON å±•å¹³æˆ LLM-as-Judge è¡¨æ ¼åˆ—ã€‚"""
    if not isinstance(gpt_data, dict):
        return []

    normalized = normalize_gpt_schema(gpt_data)
    base_row = {
        "timestamp": datetime.now().isoformat(),
        "excel_file": _safe_text(excel_file),
        "question_id": question_id,
        "question": _safe_text(question_text),
        "reference_keywords": _safe_text(reference_keywords),
        "answer": _safe_text(answer_text),
        "version": version_label,
    }

    rows = []
    for dim in GPT_DIMENSION_KEYS:
        block = get_dimension_block(normalized, dim)
        if not block:
            continue

        drivers = block.get('score_drivers') if isinstance(block, dict) else {}
        quality_notes = block.get('quality_notes') if isinstance(block, dict) else {}
        coverage_debug = block.get('coverage_debug') if isinstance(block, dict) else {}
        k_debug = block.get('k_debug') if isinstance(block, dict) else {}

        row = {
            **base_row,
            "dimension": dim,
            "score": block.get('score'),
            "p": block.get('p'),
            "q": block.get('q'),
            "k": block.get('k'),
            "r": block.get('r'),
            "g": block.get('g'),
            "shallow_flag": quality_notes.get('shallow_flag') if isinstance(quality_notes, dict) else None,
            "positive_drivers": serialize_json_field(drivers.get('positive') if isinstance(drivers, dict) else None),
            "negative_drivers": serialize_json_field(drivers.get('negative') if isinstance(drivers, dict) else None),
            "on_topic_examples": serialize_json_field(block.get('on_topic_examples')),
            "off_topic_examples": serialize_json_field(block.get('off_topic_examples')),
            "covered": serialize_json_field(block.get('covered')),
            "partially": serialize_json_field(block.get('partially')),
            "missing": serialize_json_field(block.get('missing')),
            "correct_facts": serialize_json_field(block.get('correct_facts')),
            "incorrect_facts": serialize_json_field(block.get('incorrect_facts')),
            "unverifiable_facts": serialize_json_field(block.get('unverifiable_facts')),
            "essential": serialize_json_field(block.get('essential')),
            "supportive": serialize_json_field(block.get('supportive')),
            "extraneous": serialize_json_field(block.get('extraneous')),
            "quality_notes": serialize_json_field(quality_notes),
            "coverage_debug": serialize_json_field(coverage_debug),
            "k_debug": serialize_json_field(k_debug),
            "reasoning": _safe_text(block.get('reasoning')),
            "raw_json": serialize_json_field(block)
        }

        rows.append(row)

    return rows


def extract_driver_examples(dim: str, block: dict) -> Tuple[List[str], List[str]]:
    """èƒå–æ¯å€‹ç¶­åº¦åŠ åˆ†/æ‰£åˆ†å¥ï¼Œè‹¥ç¼ºå°‘å‰‡å›é€€åˆ°æ—¢æœ‰æ¬„ä½ã€‚"""
    if not isinstance(block, dict):
        return [], []

    drivers = block.get('score_drivers') if isinstance(block.get('score_drivers'), dict) else {}
    positive = drivers.get('positive') if isinstance(drivers.get('positive'), list) else []
    negative = drivers.get('negative') if isinstance(drivers.get('negative'), list) else []

    fallback_map = {
        'relevance': (block.get('on_topic_examples'), block.get('off_topic_examples')),
        'completeness': (block.get('covered'), block.get('missing') or block.get('partially')),
        'accuracy': (block.get('correct_facts'), block.get('incorrect_facts') or block.get('unverifiable_facts')),
        'faithfulness': (block.get('essential'), block.get('extraneous'))
    }

    fallback = fallback_map.get(dim, ([], []))

    if not positive:
        positive = fallback[0] or []
    if not negative:
        neg_fb = fallback[1]
        if isinstance(neg_fb, list):
            negative = neg_fb
        elif isinstance(neg_fb, dict):
            negative = list(neg_fb.values())
        else:
            negative = neg_fb or []

    def ensure_list(items) -> list[str]:
        if isinstance(items, list):
            return [str(item) for item in items if item is not None]
        if isinstance(items, (tuple, set)):
            return [str(item) for item in items if item is not None]
        if items is None:
            return []
        return [str(items)]

    return ensure_list(positive), ensure_list(negative)


METRIC_LABELS = {
    'p': 'è²¼é¡Œæ¯”ä¾‹',
    'q': 'è¦†è“‹ç‡',
    'k': 'å“è³ªä¿‚æ•¸ k',
    'r': 'æ­£ç¢ºç‡',
    'g': 'ç¯„åœéµå¾ªæ¯”ä¾‹',
}

QUALITY_NOTE_LABELS = {
    'depth': 'æ·±åº¦',
    'context_utilization': 'è„ˆçµ¡',
    'information_synthesis': 'æ•´åˆ',
}


def parse_json_list_field(value) -> List[str]:
    if isinstance(value, list):
        return [str(item) for item in value if item is not None]
    if value is None:
        return []
    if isinstance(value, float) and np.isnan(value):
        return []
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return []
        try:
            parsed = json.loads(text)
            if isinstance(parsed, list):
                return [str(item) for item in parsed if item is not None]
            if isinstance(parsed, dict):
                return [f"{k}: {v}" for k, v in parsed.items()]
            return [str(parsed)]
        except json.JSONDecodeError:
            if 'ã€' in text:
                return [seg.strip() for seg in text.split('ã€') if seg.strip()]
            if '\n' in text:
                return [seg.strip() for seg in text.splitlines() if seg.strip()]
            return [text]
    return [str(value)]


def parse_json_object(value) -> Dict[str, Any]:
    if isinstance(value, dict):
        return value
    if value is None:
        return {}
    if isinstance(value, float) and np.isnan(value):
        return {}
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return {}
        try:
            parsed = json.loads(text)
            if isinstance(parsed, dict):
                return parsed
        except json.JSONDecodeError:
            return {}
    return {}


def safe_float(value) -> float | None:
    if isinstance(value, (int, float)):
        if isinstance(value, float) and np.isnan(value):
            return None
        return float(value)
    if isinstance(value, str):
        text = value.strip()
        if not text:
            return None
        try:
            return float(text)
        except ValueError:
            return None
    return None


def format_score(value: float | None) -> str:
    if value is None:
        return 'â€”'
    return f"{float(value):.0f}"


def format_delta(value: float | None) -> str:
    if value is None:
        return 'â€”'
    if abs(value) < 0.5:
        return 'Â±0'
    return f"{value:+.0f}"


def prepare_version_view(
    judge_df: pd.DataFrame | None,
    question_id: int,
    version_label: str,
    answer_text: str,
    gpt_data: dict | None
) -> Dict[str, Any]:
    view: Dict[str, Any] = {
        'version': version_label,
        'answer': _safe_text(answer_text),
        'dimensions': {},
        'overall': None,
        'overall_reasoning': ""
    }

    version_df = pd.DataFrame()
    if judge_df is not None and not judge_df.empty:
        version_mask = (
            pd.to_numeric(judge_df.get('question_id'), errors='coerce') == question_id
        ) & (
            judge_df.get('version').astype(str).str.lower() == version_label.lower()
        )
        version_df = judge_df[version_mask]

    if not version_df.empty:
        for _, row in version_df.iterrows():
            dim_key = str(row.get('dimension', '')).strip()
            if dim_key not in GPT_DIMENSION_KEYS:
                continue

            metrics: Dict[str, float] = {}
            for metric_key in ['p', 'q', 'k', 'r', 'g']:
                metric_val = safe_float(row.get(metric_key))
                if metric_val is not None:
                    metrics[metric_key] = metric_val

            shallow_flag_raw = str(row.get('shallow_flag', '')).strip().lower()
            shallow_flag = shallow_flag_raw in ('true', '1', 'yes')

            positive = parse_json_list_field(row.get('positive_drivers'))
            negative = parse_json_list_field(row.get('negative_drivers'))

            fallback_block = {
                'score_drivers': {'positive': positive, 'negative': negative},
                'on_topic_examples': parse_json_list_field(row.get('on_topic_examples')),
                'off_topic_examples': parse_json_list_field(row.get('off_topic_examples')),
                'covered': parse_json_list_field(row.get('covered')),
                'partially': parse_json_list_field(row.get('partially')),
                'missing': parse_json_list_field(row.get('missing')),
                'correct_facts': parse_json_list_field(row.get('correct_facts')),
                'incorrect_facts': parse_json_list_field(row.get('incorrect_facts')),
                'unverifiable_facts': parse_json_list_field(row.get('unverifiable_facts')),
                'essential': parse_json_list_field(row.get('essential')),
                'supportive': parse_json_list_field(row.get('supportive')),
                'extraneous': parse_json_list_field(row.get('extraneous')),
            }

            positive, negative = extract_driver_examples(dim_key, fallback_block)

            quality_notes = parse_json_object(row.get('quality_notes'))
            if 'shallow_flag' in quality_notes and not shallow_flag:
                shallow_flag = bool(quality_notes.pop('shallow_flag'))

            view['dimensions'][dim_key] = {
                'score': safe_float(row.get('score')),
                'metrics': metrics,
                'shallow_flag': shallow_flag,
                'positive': positive,
                'negative': negative,
                'reasoning': _safe_text(row.get('reasoning')),
                'quality_notes': quality_notes,
            }

    normalized_gpt = normalize_gpt_schema(gpt_data) if isinstance(gpt_data, dict) else {}

    if not view['dimensions'] and normalized_gpt:
        for dim in GPT_DIMENSION_KEYS:
            block = get_dimension_block(normalized_gpt, dim)
            if not block:
                continue

            metrics: Dict[str, float] = {}
            for metric_key in ['p', 'q', 'k', 'r', 'g']:
                metric_val = safe_float(block.get(metric_key))
                if metric_val is not None:
                    metrics[metric_key] = metric_val

            quality_notes_raw = block.get('quality_notes')
            quality_notes = quality_notes_raw if isinstance(quality_notes_raw, dict) else parse_json_object(quality_notes_raw)
            shallow_flag = bool(quality_notes.get('shallow_flag'))
            if 'shallow_flag' in quality_notes:
                quality_notes = {k: v for k, v in quality_notes.items() if k != 'shallow_flag'}

            positive, negative = extract_driver_examples(dim, block)

            view['dimensions'][dim] = {
                'score': safe_float(block.get('score')),
                'metrics': metrics,
                'shallow_flag': shallow_flag,
                'positive': positive,
                'negative': negative,
                'reasoning': _safe_text(block.get('reasoning')),
                'quality_notes': quality_notes,
            }

    if isinstance(gpt_data, dict):
        view['overall'] = safe_float(gpt_data.get('overall'))
        view['overall_reasoning'] = _safe_text(
            gpt_data.get('overall_reasoning') or gpt_data.get('reasoning')
        )

    if view['overall'] is None:
        scores = [info.get('score') for info in view['dimensions'].values() if info.get('score') is not None]
        if scores:
            view['overall'] = sum(scores) / len(scores)

    return view
# è§£æ GPT å›æ‡‰
def normalize_json_like_text(text: str) -> str:
    """å°‡å¸¸è¦‹çš„å…¨å½¢/å½å¼•è™Ÿæ›¿æ›æˆæ¨™æº– ASCII å­—å…ƒï¼Œæ–¹ä¾¿ JSON è§£æ"""
    if not isinstance(text, str):
        return text

    normalized = text

    # å…ˆè™•ç†éµåï¼šå°‡ã€Œâ€œkeyâ€ :ã€è½‰æ›ç‚ºæ¨™æº– JSON æ ¼å¼
    normalized = re.sub(r'â€œ([^â€]+)â€\s*:', lambda m: f'"{m.group(1)}":', normalized)

    # è™•ç†ä»¥å…¨å½¢å¼•è™ŸåŒ…è£¹çš„å€¼ï¼Œç¢ºä¿èµ·è¨–ä½¿ç”¨æ¨™æº–é›™å¼•è™Ÿ
    normalized = re.sub(r':\s*â€œ', ': "', normalized)
    normalized = re.sub(r'â€(?=\s*[,\n}])', '"', normalized)

    replacements = {
        'â€œ': '"',
        'â€': '"',
        'ï¼‚': '"',
        'ã€Œ': "'",
        'ã€': "'",
        'ã€': "'",
        'ã€': "'",
        'â€˜': "'",
        'â€™': "'",
        'ï¼‡': "'",
        'ï¼š': ':',
    }

    for src, target in replacements.items():
        normalized = normalized.replace(src, target)

    return normalized


def parse_gpt_response(response_text):
    """è§£æ ChatGPT çš„ JSON å›æ‡‰ï¼Œä¸¦å®¹éŒ¯è™•ç†å¸¸è¦‹çš„æ ¼å¼å•é¡Œ"""

    if not response_text:
        return {"error": "å›æ‡‰ç‚ºç©ºç™½", "raw_response": response_text}

    candidates = []
    raw_text = response_text.strip()
    candidates.append(raw_text)

    normalized_text = normalize_json_like_text(raw_text)
    if normalized_text != raw_text:
        candidates.append(normalized_text)

    for candidate in candidates:
        try:
            loaded = json.loads(candidate)
            if isinstance(loaded, dict):
                return normalize_gpt_schema(loaded)
            return loaded
        except json.JSONDecodeError:
            try:
                literal_result = ast.literal_eval(candidate)
                if isinstance(literal_result, dict):
                    return normalize_gpt_schema(literal_result)
            except (ValueError, SyntaxError):
                pass
            json_match = re.search(r'\{.*\}', candidate, re.DOTALL)
            if json_match:
                json_snippet = json_match.group().strip()
                try:
                    loaded = json.loads(json_snippet)
                    if isinstance(loaded, dict):
                        return normalize_gpt_schema(loaded)
                    return loaded
                except json.JSONDecodeError:
                    try:
                        literal_result = ast.literal_eval(json_snippet)
                        if isinstance(literal_result, dict):
                            return normalize_gpt_schema(literal_result)
                    except (ValueError, SyntaxError):
                        continue

    return {"error": "ç„¡æ³•è§£æ GPT å›æ‡‰", "raw_response": response_text}


GPT_DIMENSION_LABELS = {
    'relevance': 'ğŸ¯ ç›¸é—œæ€§',
    'completeness': 'ğŸ“‹ å®Œæ•´æ€§',
    'accuracy': 'âœ… æº–ç¢ºæ€§',
    'faithfulness': 'ğŸ”’ å¿ èª åº¦',
}

DEFAULT_GPT_DIMENSIONS = list(GPT_DIMENSION_LABELS.keys())


def get_selected_gpt_dimensions() -> list:
    """å–å¾—ç›®å‰é¸æ“‡çš„ GPT ç¶œåˆè©•åˆ†ç¶­åº¦ï¼ˆè‡³å°‘å›å‚³ä¸€å€‹ï¼‰ã€‚"""
    selected = st.session_state.get('gpt_selected_dimensions', DEFAULT_GPT_DIMENSIONS)
    if not selected:
        return DEFAULT_GPT_DIMENSIONS

    filtered = [dim for dim in selected if dim in GPT_DIMENSION_LABELS]
    return filtered or DEFAULT_GPT_DIMENSIONS


def get_gpt_dimension_weights(selected_dims: list) -> dict:
    """ä»¥ç›®å‰é¸æ“‡çš„ç¶­åº¦å›å‚³æ­¸ä¸€åŒ–æ¬Šé‡ï¼Œé è¨­å¹³å‡ã€‚"""
    weights_setting = st.session_state.get('gpt_dimension_weights', {})
    weights = {}
    total = 0.0
    for dim in selected_dims:
        value = weights_setting.get(dim)
        if isinstance(value, (int, float)) and value >= 0:
            weights[dim] = float(value)
            total += float(value)
        else:
            weights[dim] = 0.0

    if total <= 0:
        # è‹¥å…¨éƒ¨ç‚º 0 æˆ–ç„¡è¨­å®šï¼Œæ”¹æ¡å¹³å‡æ¬Šé‡
        equal_weight = 1.0 / len(selected_dims)
        return {dim: equal_weight for dim in selected_dims}

    return {dim: value / total for dim, value in weights.items()}


def compute_gpt_overall(gpt_data: dict, selected_dims: list | None = None, dim_weights: dict | None = None) -> float:
    """ä¾ç…§é¸å–çš„ç¶­åº¦é‡æ–°è¨ˆç®— GPT ç¶œåˆè©•åˆ†ã€‚"""
    if not isinstance(gpt_data, dict):
        return 0.0

    dimensions = selected_dims or get_selected_gpt_dimensions()
    weights = dim_weights or get_gpt_dimension_weights(dimensions)

    score_sum = 0.0
    applied_weight = 0.0
    for dim in dimensions:
        value = get_dimension_score(gpt_data, dim)
        if value is None:
            continue
        weight = weights.get(dim, 0.0)
        score_sum += value * weight
        applied_weight += weight

    if applied_weight > 0:
        return score_sum / applied_weight

    fallback = gpt_data.get('overall')
    try:
        return float(fallback)
    except (TypeError, ValueError):
        return 0.0


def build_combined_reasoning(gpt_data: dict) -> str:
    """å°‡å€‹åˆ¥ç¶­åº¦çš„ reasoning åˆä½µæˆå¯è®€æ–‡å­—"""
    if not isinstance(gpt_data, dict):
        return ""

    parts = []
    for dim, label in GPT_DIMENSION_LABELS.items():
        value = get_dimension_reasoning(gpt_data, dim)
        if value:
            parts.append(f"{label}: {value}")

    # éƒ¨åˆ†èˆŠè³‡æ–™å¯èƒ½åªæä¾› overall_reasoning æˆ– reasoning
    if not parts and gpt_data.get("reasoning"):
        parts.append(str(gpt_data.get("reasoning")))
    if not parts and gpt_data.get("overall_reasoning"):
        parts.append(str(gpt_data.get("overall_reasoning")))

    return "\n".join(parts)


def format_gpt_weight_summary(selected_dims: list, dim_weights: dict | None = None) -> str:
    weights = dim_weights or get_gpt_dimension_weights(selected_dims)
    parts = []
    for dim in selected_dims:
        label = GPT_DIMENSION_LABELS.get(dim, dim)
        parts.append(f"{label} {weights.get(dim, 0)*100:.0f}%")
    return 'ã€'.join(parts)

# é©—è­‰è©•åˆ†ä¸€è‡´æ€§å‡½æ•¸
def validate_scoring_consistency(parsed_response, question_text, answer_text):
    """é©—è­‰è©•åˆ†çš„é‚è¼¯ä¸€è‡´æ€§å’Œå®Œæ•´æ€§"""
    warnings = []
    errors = []

    if not isinstance(parsed_response, dict):
        return warnings, ["å›æ‡‰ä¸æ˜¯æœ‰æ•ˆçš„ JSON ç‰©ä»¶"]

    # åŸºæœ¬æ¬„ä½æª¢æŸ¥
    if 'question_id' not in parsed_response:
        warnings.append("ç¼ºå°‘ question_idï¼Œå°‡ç„¡æ³•è‡ªå‹•å°æ‡‰é¡Œè™Ÿ")

    overall = parsed_response.get('overall')
    if overall is None:
        errors.append("ç¼ºå°‘ overall åˆ†æ•¸")
    else:
        try:
            overall_value = float(overall)
            if not 0 <= overall_value <= 100:
                errors.append(f"overall åˆ†æ•¸å¿…é ˆåœ¨ 0-100 ä¹‹é–“ï¼Œç›®å‰ç‚º {overall}")
        except (TypeError, ValueError):
            errors.append(f"overall åˆ†æ•¸ç„¡æ³•è§£æç‚ºæ•¸å€¼ï¼š{overall}")

    overall_reasoning = parsed_response.get('overall_reasoning', '')
    if not isinstance(overall_reasoning, str) or not overall_reasoning.strip():
        warnings.append("overall_reasoning å»ºè­°æä¾›ç¸½çµèªªæ˜")

    dimension_scores = {}

    for dim in GPT_DIMENSION_KEYS:
        block = get_dimension_block(parsed_response, dim)

        if not block:
            errors.append(f"ç¼ºå°‘ {dim} ç¶­åº¦çš„è³‡æ–™æˆ–æ ¼å¼éŒ¯èª¤")
            continue

        score = get_dimension_score(parsed_response, dim)
        if score is None:
            errors.append(f"{dim} ç¼ºå°‘ score æ¬„ä½æˆ–ç„¡æ³•è§£æ")
            continue

        if not 0 <= score <= 100:
            errors.append(f"{dim} çš„ score å¿…é ˆä»‹æ–¼ 0-100ï¼Œç›®å‰ç‚º {score}")
        else:
            dimension_scores[dim] = score

        reasoning = get_dimension_reasoning(parsed_response, dim)
        if not reasoning:
            warnings.append(f"{dim} å»ºè­°è£œå…… reasoning èªªæ˜")

        drivers = block.get('score_drivers')
        if not isinstance(drivers, dict):
            warnings.append(f"{dim} å»ºè­°å¡«å¯« score_drivers ä»¥åˆ—å‡ºåŠ åˆ†èˆ‡æ‰£åˆ†å¥")
        else:
            pos = drivers.get('positive')
            neg = drivers.get('negative')
            if not pos:
                warnings.append(f"{dim} çš„ score_drivers.positive å»ºè­°è‡³å°‘æä¾›ä¸€é …åŠ åˆ†å› ç´ ")
            if not neg:
                warnings.append(f"{dim} çš„ score_drivers.negative å»ºè­°è‡³å°‘æä¾›ä¸€é …æ‰£åˆ†å› ç´ ")

        # ç¶­åº¦ç‰¹å®šæª¢æŸ¥
        if dim == 'relevance':
            p_val = block.get('p')
            if p_val is None:
                warnings.append("relevance å»ºè­°æä¾› p (è²¼é¡Œæ¯”ä¾‹)")
            else:
                try:
                    p_num = float(p_val)
                    if not 0 <= p_num <= 1:
                        errors.append(f"relevance.p å¿…é ˆä»‹æ–¼ 0-1ï¼Œç›®å‰ç‚º {p_val}")
                except (TypeError, ValueError):
                    errors.append(f"relevance.p ç„¡æ³•è§£æç‚ºæ•¸å€¼ï¼š{p_val}")

        if dim == 'completeness':
            q_val = block.get('q')
            k_val = block.get('k')
            if q_val is None:
                warnings.append("completeness å»ºè­°æä¾› q (è¦†è“‹ç‡)")
            else:
                try:
                    q_num = float(q_val)
                    if not 0 <= q_num <= 1:
                        errors.append(f"completeness.q å¿…é ˆä»‹æ–¼ 0-1ï¼Œç›®å‰ç‚º {q_val}")
                except (TypeError, ValueError):
                    errors.append(f"completeness.q ç„¡æ³•è§£æç‚ºæ•¸å€¼ï¼š{q_val}")

            if k_val is None:
                warnings.append("completeness å»ºè­°æä¾› k (å“è³ªä¿‚æ•¸)")
            else:
                try:
                    k_num = float(k_val)
                    if not 0.8 <= k_num <= 1.0:
                        warnings.append(f"completeness.k å»ºè­°ä»‹æ–¼ 0.80-1.00ï¼Œç›®å‰ç‚º {k_val}")
                except (TypeError, ValueError):
                    errors.append(f"completeness.k ç„¡æ³•è§£æç‚ºæ•¸å€¼ï¼š{k_val}")

        if dim == 'accuracy':
            r_val = block.get('r')
            if r_val is None:
                warnings.append("accuracy å»ºè­°æä¾› r (æ­£ç¢ºç‡)")
            else:
                try:
                    r_num = float(r_val)
                    if not 0 <= r_num <= 1:
                        errors.append(f"accuracy.r å¿…é ˆä»‹æ–¼ 0-1ï¼Œç›®å‰ç‚º {r_val}")
                except (TypeError, ValueError):
                    errors.append(f"accuracy.r ç„¡æ³•è§£æç‚ºæ•¸å€¼ï¼š{r_val}")

        if dim == 'faithfulness':
            g_val = block.get('g')
            if g_val is None:
                warnings.append("faithfulness å»ºè­°æä¾› g (ç¯„åœéµå¾ªæ¯”ä¾‹)")
            else:
                try:
                    g_num = float(g_val)
                    if not 0 <= g_num <= 1:
                        errors.append(f"faithfulness.g å¿…é ˆä»‹æ–¼ 0-1ï¼Œç›®å‰ç‚º {g_val}")
                except (TypeError, ValueError):
                    errors.append(f"faithfulness.g ç„¡æ³•è§£æç‚ºæ•¸å€¼ï¼š{g_val}")

    # æ¯”å° overall èˆ‡å››ç¶­å¹³å‡å€¼ï¼ˆè‹¥åˆ†æ•¸é½Šå…¨ï¼‰
    if overall is not None and len(dimension_scores) == len(GPT_DIMENSION_KEYS):
        expected = sum(dimension_scores.values()) / len(GPT_DIMENSION_KEYS)
        try:
            if abs(expected - float(overall)) > 2:
                warnings.append(
                    f"ç¸½åˆ†å¯èƒ½ä¸ä¸€è‡´ï¼šæœŸæœ› {expected:.1f}ï¼Œå¯¦éš› {float(overall):.1f}"
                )
        except (TypeError, ValueError):
            pass

    return warnings, errors

# è‡ªå‹•ä¿å­˜è©•ä¼°çµæœåˆ°æ­·å²ç´€éŒ„
def auto_save_evaluation(actual_question_id, results_df, weights, selected_dims=None, dim_weights=None):
    """
    è‡ªå‹•ä¿å­˜è©•ä¼°çµæœåˆ°æ­·å²ç´€éŒ„

    ç­–ç•¥ï¼š
    1. å¦‚æœå…©å€‹ç‰ˆæœ¬éƒ½æœ‰ GPT è©•åˆ† â†’ ä¿å­˜å®Œæ•´è©•ä¼°
    2. å¦‚æœåªæœ‰ä¸€å€‹ç‰ˆæœ¬æœ‰ GPT è©•åˆ† â†’ ä¿å­˜è©²ç‰ˆæœ¬ï¼ˆå¦ä¸€ç‰ˆæœ¬ç”¨ 0 å¡«å……ï¼‰

    Args:
        actual_question_id: å¯¦éš›çš„å•é¡Œåºè™Ÿï¼ˆä¾†è‡ª 'åºè™Ÿ' æ¬„ä½ï¼‰
        results_df: è©•ä¼°çµæœ DataFrame
        weights: æ¬Šé‡è¨­å®š
    """
    # æª¢æŸ¥æ˜¯å¦è‡³å°‘æœ‰ä¸€å€‹ç‰ˆæœ¬æœ‰ GPT è©•åˆ†ï¼ˆä½¿ç”¨å¯¦éš›åºè™Ÿï¼‰
    has_original = actual_question_id in st.session_state.gpt_responses_original
    has_optimized = actual_question_id in st.session_state.gpt_responses_optimized

    if not (has_original or has_optimized):
        return False  # å…©å€‹ç‰ˆæœ¬éƒ½æ²’æœ‰ GPT è©•åˆ†ï¼Œä¸ä¿å­˜

    try:
        # åœ¨ DataFrame ä¸­æŸ¥æ‰¾å°æ‡‰çš„è¡Œï¼ˆä½¿ç”¨ 'åºè™Ÿ' æ¬„ä½åŒ¹é…ï¼‰
        matching_rows = results_df[results_df['åºè™Ÿ'] == actual_question_id]
        if matching_rows.empty:
            print(f"âš ï¸ æ‰¾ä¸åˆ°åºè™Ÿ {actual_question_id} çš„å•é¡Œ")
            return False

        row = matching_rows.iloc[0]

        # æº–å‚™åŸå§‹ç‰ˆæœ¬è©•åˆ†
        selected_dims = selected_dims or get_selected_gpt_dimensions()
        dim_weights = dim_weights or get_gpt_dimension_weights(selected_dims)

        gpt_raw_original = {}
        gpt_raw_optimized = {}

        if has_original:
            gpt_orig = st.session_state.gpt_responses_original[actual_question_id]
            gpt_raw_original = deepcopy(gpt_orig)
            rel_score = get_dimension_score(gpt_orig, 'relevance')
            comp_score = get_dimension_score(gpt_orig, 'completeness')
            acc_score = get_dimension_score(gpt_orig, 'accuracy')
            faith_score = get_dimension_score(gpt_orig, 'faithfulness')
            original_scores = {
                "keyword_score": row.get('KEYWORD_COVERAGE_ORIGINAL', 0),
                "semantic_score": row.get('SEMANTIC_SIMILARITY_ORIGINAL', 0),
                "gpt_relevance": rel_score if rel_score is not None else 0,
                "gpt_completeness": comp_score if comp_score is not None else 0,
                "gpt_accuracy": acc_score if acc_score is not None else 0,
                "gpt_faithfulness": faith_score if faith_score is not None else 0,
                "gpt_overall": compute_gpt_overall(gpt_orig, selected_dims, dim_weights),
                "gpt_reasoning": build_combined_reasoning(gpt_orig),
                "final_score": row.get('FINAL_SCORE_ORIGINAL', 0)
            }
        else:
            # åŸå§‹ç‰ˆæœ¬æ²’æœ‰ GPT è©•åˆ†ï¼Œåªä¿å­˜é—œéµè©å’Œèªç¾©
            original_scores = {
                "keyword_score": row.get('KEYWORD_COVERAGE_ORIGINAL', 0),
                "semantic_score": row.get('SEMANTIC_SIMILARITY_ORIGINAL', 0),
                "gpt_relevance": 0,
                "gpt_completeness": 0,
                "gpt_accuracy": 0,
                "gpt_faithfulness": 0,
                "gpt_overall": 0,
                "gpt_reasoning": "",
                "final_score": row.get('FINAL_SCORE_ORIGINAL', 0)
            }

        # æº–å‚™å„ªåŒ–ç‰ˆæœ¬è©•åˆ†
        if has_optimized:
            gpt_opt = st.session_state.gpt_responses_optimized[actual_question_id]
            gpt_raw_optimized = deepcopy(gpt_opt)
            rel_score_opt = get_dimension_score(gpt_opt, 'relevance')
            comp_score_opt = get_dimension_score(gpt_opt, 'completeness')
            acc_score_opt = get_dimension_score(gpt_opt, 'accuracy')
            faith_score_opt = get_dimension_score(gpt_opt, 'faithfulness')
            optimized_scores = {
                "keyword_score": row.get('KEYWORD_COVERAGE_OPTIMIZED', 0),
                "semantic_score": row.get('SEMANTIC_SIMILARITY_OPTIMIZED', 0),
                "gpt_relevance": rel_score_opt if rel_score_opt is not None else 0,
                "gpt_completeness": comp_score_opt if comp_score_opt is not None else 0,
                "gpt_accuracy": acc_score_opt if acc_score_opt is not None else 0,
                "gpt_faithfulness": faith_score_opt if faith_score_opt is not None else 0,
                "gpt_overall": compute_gpt_overall(gpt_opt, selected_dims, dim_weights),
                "gpt_reasoning": build_combined_reasoning(gpt_opt),
                "final_score": row.get('FINAL_SCORE_OPTIMIZED', 0)
            }
        else:
            # å„ªåŒ–ç‰ˆæœ¬æ²’æœ‰ GPT è©•åˆ†ï¼Œåªä¿å­˜é—œéµè©å’Œèªç¾©
            optimized_scores = {
                "keyword_score": row.get('KEYWORD_COVERAGE_OPTIMIZED', 0),
                "semantic_score": row.get('SEMANTIC_SIMILARITY_OPTIMIZED', 0),
                "gpt_relevance": 0,
                "gpt_completeness": 0,
                "gpt_accuracy": 0,
                "gpt_faithfulness": 0,
                "gpt_overall": 0,
                "gpt_reasoning": "",
                "final_score": row.get('FINAL_SCORE_OPTIMIZED', 0)
            }

        # ä¿å­˜åˆ°æ­·å²ç´€éŒ„ï¼ˆä½¿ç”¨å¯¦éš›åºè™Ÿï¼‰
        success = st.session_state.history_manager.save_evaluation(
            excel_filename=st.session_state.current_excel_filename,
            question_id=actual_question_id,
            question_text=row.get('æ¸¬è©¦å•é¡Œ', ''),
            reference_keywords=row.get('æ‡‰å›ç­”ä¹‹è©å½™', ''),
            original_answer=row.get('ANSWER_ORIGINAL', ''),
            optimized_answer=row.get('ANSWER_OPTIMIZED', ''),
            original_scores=original_scores,
            optimized_scores=optimized_scores,
            weights=weights,
            metadata={
                "evaluation_date": datetime.now().isoformat(),
                "improvement": optimized_scores['final_score'] - original_scores['final_score'],
                "has_original_gpt": has_original,
                "has_optimized_gpt": has_optimized,
                "gpt_raw": {
                    "original": gpt_raw_original,
                    "optimized": gpt_raw_optimized
                }
            }
        )

        if success:
            judge_rows = []
            excel_file = st.session_state.current_excel_filename
            question_text = row.get('æ¸¬è©¦å•é¡Œ', '')
            reference_text = row.get('æ‡‰å›ç­”ä¹‹è©å½™', '')

            if has_original and isinstance(gpt_raw_original, dict) and gpt_raw_original:
                judge_rows.extend(
                    create_llm_judge_rows(
                        excel_file=excel_file,
                        question_id=actual_question_id,
                        question_text=question_text,
                        reference_keywords=reference_text,
                        answer_text=row.get('ANSWER_ORIGINAL', ''),
                        version_label='original',
                        gpt_data=gpt_raw_original
                    )
                )

            if has_optimized and isinstance(gpt_raw_optimized, dict) and gpt_raw_optimized:
                judge_rows.extend(
                    create_llm_judge_rows(
                        excel_file=excel_file,
                        question_id=actual_question_id,
                        question_text=question_text,
                        reference_keywords=reference_text,
                        answer_text=row.get('ANSWER_OPTIMIZED', ''),
                        version_label='optimized',
                        gpt_data=gpt_raw_optimized
                    )
                )

            if judge_rows:
                st.session_state.history_manager.append_llm_judge_records(judge_rows)

        return success

    except Exception as e:
        print(f"âŒ è‡ªå‹•ä¿å­˜å¤±æ•—: {e}")
        return False

# å¾æ­·å²ç´€éŒ„è¼‰å…¥ GPT è©•åˆ†
def load_gpt_from_history(excel_filename):
    """å¾æ­·å²ç´€éŒ„è¼‰å…¥è©²æª”æ¡ˆçš„ GPT è©•åˆ†"""
    if not excel_filename:
        return

    try:
        evaluations = st.session_state.history_manager.get_evaluations_by_file(excel_filename)

        for eval_record in evaluations:
            # ä½¿ç”¨å¯¦éš› question_idï¼ˆä¸éœ€è¦è½‰æ›ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹åºè™Ÿï¼‰
            question_id = eval_record.get("question_id", 0)

            # è¼‰å…¥åŸå§‹ç‰ˆæœ¬ GPT è©•åˆ†
            original_scores = eval_record.get("scores", {}).get("original", {})
            gpt_raw_meta = (eval_record.get("metadata", {}) or {}).get("gpt_raw", {})
            legacy_gpt_raw = eval_record.get("gpt_raw", {})
            raw_original = gpt_raw_meta.get("original") if isinstance(gpt_raw_meta, dict) else {}
            if not raw_original:
                raw_original = legacy_gpt_raw.get("original") if isinstance(legacy_gpt_raw, dict) else {}
            if original_scores.get("gpt_overall", 0) > 0:
                if isinstance(raw_original, dict) and raw_original:
                    st.session_state.gpt_responses_original[question_id] = raw_original
                else:
                    st.session_state.gpt_responses_original[question_id] = {
                        "relevance": original_scores.get("gpt_relevance", 0),
                        "completeness": original_scores.get("gpt_completeness", 0),
                        "accuracy": original_scores.get("gpt_accuracy", 0),
                        "faithfulness": original_scores.get("gpt_faithfulness", 0),
                        "overall": original_scores.get("gpt_overall", 0),
                        "reasoning": original_scores.get("gpt_reasoning", "")
                    }

            # è¼‰å…¥å„ªåŒ–ç‰ˆæœ¬ GPT è©•åˆ†
            optimized_scores = eval_record.get("scores", {}).get("optimized", {})
            raw_optimized = gpt_raw_meta.get("optimized") if isinstance(gpt_raw_meta, dict) else {}
            if not raw_optimized:
                raw_optimized = legacy_gpt_raw.get("optimized") if isinstance(legacy_gpt_raw, dict) else {}
            if optimized_scores.get("gpt_overall", 0) > 0:
                if isinstance(raw_optimized, dict) and raw_optimized:
                    st.session_state.gpt_responses_optimized[question_id] = raw_optimized
                else:
                    st.session_state.gpt_responses_optimized[question_id] = {
                        "relevance": optimized_scores.get("gpt_relevance", 0),
                        "completeness": optimized_scores.get("gpt_completeness", 0),
                        "accuracy": optimized_scores.get("gpt_accuracy", 0),
                        "faithfulness": optimized_scores.get("gpt_faithfulness", 0),
                        "overall": optimized_scores.get("gpt_overall", 0),
                        "reasoning": optimized_scores.get("gpt_reasoning", "")
                    }

        if evaluations:
            print(f"âœ… å¾æ­·å²ç´€éŒ„è¼‰å…¥äº† {len(evaluations)} ç­† GPT è©•åˆ†")
            return len(evaluations)

    except Exception as e:
        print(f"âš ï¸ è¼‰å…¥æ­·å² GPT è©•åˆ†å¤±æ•—: {e}")

    return 0

# æ¨™é¡Œå’Œèªªæ˜
st.title("ğŸ†š RAG è©•ä¼°å„€è¡¨æ¿ v2.0")
st.markdown("### RAGè©•ä¼°æ¶æ§‹ï¼šé—œéµè© + èªç¾©ç›¸ä¼¼åº¦ + GPT äººå·¥è©•å¯©")

# å´é‚Šæ¬„é…ç½®
with st.sidebar:
    st.header("ğŸ“ è¨­å®šèˆ‡æª”æ¡ˆé¸æ“‡")

    # æª”æ¡ˆé¸æ“‡æ–¹å¼
    file_source = st.radio(
        "é¸æ“‡æª”æ¡ˆä¾†æº",
        ["ğŸ“‚ æœ¬åœ°è³‡æ–™å¤¾", "ğŸ“¤ ä¸Šå‚³æª”æ¡ˆ"],
        help="é¸æ“‡è¦å¾æœ¬åœ°è³‡æ–™å¤¾è¼‰å…¥æˆ–ä¸Šå‚³æ–°æª”æ¡ˆ"
    )

    selected_file_path = None
    uploaded_file = None

    if file_source == "ğŸ“‚ æœ¬åœ°è³‡æ–™å¤¾":
        data_folder = "test_data"
        if not os.path.exists(data_folder):
            os.makedirs(data_folder)

        st.caption(f"è³‡æ–™å¤¾è·¯å¾‘ï¼š{data_folder}")

        try:
            all_files = os.listdir(data_folder)
            excel_files = [f for f in all_files
                          if f.endswith(('.xlsx', '.xls', '.csv')) and not f.startswith('~') and not f.startswith('.')]
        except Exception as e:
            st.error(f"è®€å–è³‡æ–™å¤¾æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            excel_files = []

        if excel_files:
            selected_file = st.selectbox(
                "é¸æ“‡è¦è©•ä¼°çš„æª”æ¡ˆ",
                excel_files,
                help="å¾ test_data è³‡æ–™å¤¾ä¸­é¸æ“‡æª”æ¡ˆ"
            )
            selected_file_path = os.path.join(data_folder, selected_file)
            uploaded_file = selected_file_path

            file_info = os.stat(selected_file_path)
            st.info(f"æª”æ¡ˆå¤§å°ï¼š{file_info.st_size / 1024:.1f} KB")
            st.success(f"âœ… å·²è¼‰å…¥: {selected_file}")
        else:
            st.warning("âš ï¸ test_data è³‡æ–™å¤¾ä¸­æ²’æœ‰æ‰¾åˆ° Excel æˆ– CSV æª”æ¡ˆ")

    else:  # ä¸Šå‚³æª”æ¡ˆ
        uploaded_file = st.file_uploader(
            "ä¸Šå‚³æ¸¬è©¦çµæœExcel/CSVæª”æ¡ˆ",
            type=['xlsx', 'xls', 'csv'],
            help="è«‹ä¸Šå‚³åŒ…å«å‘é‡çŸ¥è­˜åº«(åŸå§‹ç‰ˆ)å’Œæ™ºæ…§æ–‡æª”çŸ¥è­˜åº«(å½™æ•´ç‰ˆ)å›ç­”çš„æ¸¬è©¦çµæœ"
        )

        if uploaded_file is not None:
            file_extension = uploaded_file.name.split('.')[-1].lower()
            selected_file_path = f"temp_uploaded.{file_extension}"
            with open(selected_file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())
            st.success(f"âœ… å·²è¼‰å…¥: {uploaded_file.name}")

    # çŸ¥è­˜åº«é¸æ“‡
    st.markdown("### ğŸ“š çŸ¥è­˜åº«è¨­å®š")
    col1, col2 = st.columns(2)

    with col1:
        original_kb = st.selectbox(
            "åŸå§‹ç‰ˆæœ¬",
            ["å‘é‡çŸ¥è­˜åº«", "æ™ºæ…§æ–‡æª”çŸ¥è­˜åº«"],
            index=0,
            help="é¸æ“‡åŸå§‹ç‰ˆæœ¬ä½¿ç”¨çš„çŸ¥è­˜åº«æŠ€è¡“"
        )

    with col2:
        optimized_kb = st.selectbox(
            "å„ªåŒ–ç‰ˆæœ¬",
            ["æ™ºæ…§æ–‡æª”çŸ¥è­˜åº«", "å‘é‡çŸ¥è­˜åº«+å„ªåŒ–"],
            index=0,
            help="é¸æ“‡å„ªåŒ–ç‰ˆæœ¬ä½¿ç”¨çš„çŸ¥è­˜åº«æŠ€è¡“"
        )

    # è©•ä¼°å±¤ç´šè¨­å®š
    st.markdown("### ğŸ¯ è©•ä¼°å±¤ç´šè¨­å®š")
    st.info("ğŸ” é¸æ“‡è¦å•Ÿç”¨çš„è©•ä¼°å±¤ç´š")

    enable_semantic = st.checkbox(
        "å•Ÿç”¨èªç¾©ç›¸ä¼¼åº¦è©•ä¼°",
        value=True,
        help="ä½¿ç”¨ Sentence Transformers è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦ï¼ˆæ¨è–¦ï¼‰"
    )

    enable_manual_gpt = st.checkbox(
        "å•Ÿç”¨ GPT äººå·¥è©•å¯©",
        value=True,
        help="ç”Ÿæˆ GPT promptsï¼Œæ‚¨å¯è¤‡è£½åˆ° ChatGPT ä¸¦è²¼å›è©•åˆ†ï¼ˆå®Œå…¨å…è²»ï¼Œç„¡éœ€ APIï¼‰"
    )

    if enable_manual_gpt:
        st.success("âœ… GPT äººå·¥è©•å¯©æ¨¡å¼")
        st.info("ğŸ’¡ ç³»çµ±æœƒç”Ÿæˆ Promptï¼Œè«‹ç›´æ¥è¤‡è£½åˆ° ChatGPT é€²è¡Œåˆ†æ")

    show_semantic_overview = st.checkbox(
        "æ¦‚è¦½é¡¯ç¤ºèªç¾©ç›¸ä¼¼åº¦",
        value=st.session_state.display_semantic_metric,
        help="åƒ…å½±éŸ¿è©•ä¼°ç¸½è¦½æŒ‡æ¨™å¡ç‰‡ï¼›å³ä½¿ä¸é¡¯ç¤ºï¼Œèªç¾©åˆ†æ•¸ä»æœƒè¨ˆç®—ä¸¦èƒ½åœ¨å…¶ä»–åˆ†é ä½¿ç”¨ã€‚"
    )
    st.session_state.display_semantic_metric = show_semantic_overview

    # è©•åˆ†æ¬Šé‡è¨­å®š
    st.markdown("### âš–ï¸ è©•åˆ†æ¬Šé‡è¨­å®š")

    if enable_semantic and enable_manual_gpt:
        st.info("ä¸‰ç¨®è©•ä¼°æ¨¡å¼")
        weight_keyword = st.slider("é—œéµè©æ¬Šé‡", 0.0, 1.0, 0.3, 0.1)
        weight_semantic = st.slider("èªç¾©æ¬Šé‡", 0.0, 1.0, 0.3, 0.1)
        weight_gpt = 1.0 - weight_keyword - weight_semantic
        st.metric("GPT æ¬Šé‡", f"{weight_gpt:.1f}")
    elif enable_semantic:
        st.info("é›™å±¤è©•ä¼°æ¨¡å¼ï¼ˆé—œéµè© + èªç¾©ï¼‰")
        weight_keyword = st.slider("é—œéµè©æ¬Šé‡", 0.0, 1.0, 0.5, 0.1)
        weight_semantic = 1.0 - weight_keyword
        weight_gpt = 0.0
        st.metric("èªç¾©æ¬Šé‡", f"{weight_semantic:.1f}")
    elif enable_manual_gpt:
        st.info("é›™å±¤è©•ä¼°æ¨¡å¼ï¼ˆé—œéµè© + GPTï¼‰")
        weight_keyword = st.slider("é—œéµè©æ¬Šé‡", 0.0, 1.0, 0.4, 0.1)
        weight_gpt = 1.0 - weight_keyword
        weight_semantic = 0.0
        st.metric("GPT æ¬Šé‡", f"{weight_gpt:.1f}")
    else:
        st.info("ï¿½ï¿½å±¤è©•ä¼°æ¨¡å¼ï¼ˆåƒ…é—œéµè©ï¼‰")
        weight_keyword = 1.0
        weight_semantic = 0.0
        weight_gpt = 0.0

    weights = {
        "keyword": weight_keyword,
        "semantic": weight_semantic,
        "gpt": weight_gpt
    }

    # é¡¯è‘—æ”¹å–„é–¾å€¼
    st.markdown("### ğŸ¯ åˆ†æè¨­å®š")
    improvement_threshold = st.slider(
        "é¡¯è‘—æ”¹å–„é–¾å€¼ (%)",
        min_value=5,
        max_value=50,
        value=10,
        help="ç•¶æ”¹å–„å¹…åº¦è¶…éæ­¤é–¾å€¼æ™‚ï¼Œæ¨™è¨˜ç‚ºé¡¯è‘—æ”¹å–„"
    )

    # æ­·å²ç´€éŒ„ç®¡ç†
    st.markdown("### ğŸ“š æ­·å²ç´€éŒ„")
    all_evaluations = st.session_state.history_manager.get_all_evaluations()
    stats = st.session_state.history_manager.get_statistics()

    st.metric("ç´¯è¨ˆè©•ä¼°é¡Œæ•¸", stats["total_evaluations"])
    st.metric("å·²è©•ä¼°æª”æ¡ˆæ•¸", stats["files_evaluated"])

    if stats["total_evaluations"] > 0:
        st.metric(
            "å¹³å‡æ”¹å–„å¹…åº¦",
            f"{stats['avg_improvement']:.1f}",
            delta=f"{stats['avg_improvement']:.1f}%"
        )

    # åŒ¯å‡ºæ­·å²ç´€éŒ„æŒ‰éˆ•
    if st.button("ğŸ“¥ åŒ¯å‡ºå®Œæ•´æ­·å²ç´€éŒ„", use_container_width=True):
        output_path = f"evaluation_history_{datetime.now().strftime('%Y%m%d_%H%M%S')}.xlsx"
        if st.session_state.history_manager.export_to_excel(output_path):
            st.success(f"âœ… å·²åŒ¯å‡ºåˆ° {output_path}")
        else:
            st.error("âŒ åŒ¯å‡ºå¤±æ•—")

    # æ¸…é™¤æ­·å²ç´€éŒ„æŒ‰éˆ•
    if st.button("ğŸ—‘ï¸ æ¸…é™¤æ­·å²ç´€éŒ„", type="secondary", use_container_width=True):
        if st.session_state.history_manager.clear_history():
            st.success("âœ… æ­·å²ç´€éŒ„å·²æ¸…é™¤")
            st.rerun()
        else:
            st.error("âŒ æ¸…é™¤å¤±æ•—")

# ä¸»è¦å…§å®¹å€
if uploaded_file is not None:
    # è™•ç†æª”æ¡ˆ
    if isinstance(uploaded_file, str):
        temp_file_path = uploaded_file
        # è¨­å®šç•¶å‰æª”æ¡ˆåç¨±ï¼ˆç”¨æ–¼æ­·å²ç´€éŒ„ï¼‰
        st.session_state.current_excel_filename = os.path.basename(uploaded_file)
    else:
        temp_file_path = "temp_comparison_file.xlsx"
        with open(temp_file_path, "wb") as f:
            f.write(uploaded_file.getbuffer())
        # è¨­å®šç•¶å‰æª”æ¡ˆåç¨±ï¼ˆç”¨æ–¼æ­·å²ç´€éŒ„ï¼‰
        st.session_state.current_excel_filename = uploaded_file.name

    # æ ¹æ“šé¸æ“‡çš„çŸ¥è­˜åº«é¡å‹å»ºç«‹è©•ä¼°å™¨
    try:
        if original_kb == "å‘é‡çŸ¥è­˜åº«" and optimized_kb == "æ™ºæ…§æ–‡æª”çŸ¥è­˜åº«":
            model_type = "cross"
        elif original_kb == "å‘é‡çŸ¥è­˜åº«":
            model_type = "vector"
        else:
            model_type = "smart_doc"

        evaluator = RAGEvaluatorV2(
            temp_file_path,
            model_type=model_type,
            enable_semantic=enable_semantic,
            enable_gpt=False,  # æˆ‘å€‘ä½¿ç”¨äººå·¥è©•å¯©ï¼Œä¸ä½¿ç”¨ API
            weights=weights
        )

        # å¦‚æœèªç¾©ç›¸ä¼¼åº¦åœ¨è¼‰å…¥éšæ®µè¢«åœç”¨ï¼Œæé†’ä½¿ç”¨è€…ä¸¦åŒæ­¥ç‹€æ…‹
        if enable_semantic and not evaluator.enable_semantic:
            st.warning("âš ï¸ èªç¾©ç›¸ä¼¼åº¦æ¨¡å‹æœªå•Ÿå‹•ï¼Œè«‹ç¢ºèªå·²å®‰è£ sentence-transformers èˆ‡ torch å¥—ä»¶ã€‚")

        enable_semantic = evaluator.enable_semantic
        weights = evaluator.weights

        st.session_state.evaluator_instance = evaluator

        # åŸ·è¡Œè©•ä¼°ï¼ˆä¸åŒ…å« GPTï¼ŒGPT ç”±äººå·¥æä¾›ï¼‰
        # æª¢æŸ¥æ˜¯å¦å·²ç¶“æœ‰è©•ä¼°çµæœï¼ˆé¿å…é‡è¤‡è©•ä¼°å°è‡´èªç¾©ç›¸ä¼¼åº¦ä¸Ÿå¤±ï¼‰
        if st.session_state.comparison_results is None:
            with st.spinner("ğŸ”„ æ­£åœ¨é€²è¡Œè©•ä¼°åˆ†æ..."):
                results_df = evaluator.evaluate_all()
                st.session_state.comparison_results = results_df
        else:
            # å·²ç¶“æœ‰è©•ä¼°çµæœï¼Œç›´æ¥ä½¿ç”¨
            results_df = st.session_state.comparison_results

        # å¾æ­·å²ç´€éŒ„è¼‰å…¥ GPT è©•åˆ†ï¼ˆå¦‚æœæœ‰çš„è©±ï¼‰
        if not st.session_state.gpt_responses_loaded and st.session_state.current_excel_filename:
            loaded_count = load_gpt_from_history(st.session_state.current_excel_filename)
            if loaded_count > 0:
                st.success(f"âœ… å¾æ­·å²ç´€éŒ„æ¢å¾©äº† {loaded_count} ç­† GPT è©•åˆ†")
            st.session_state.gpt_responses_loaded = True

        # æ¸…ç†è‡¨æ™‚æª”æ¡ˆ
        if os.path.exists(temp_file_path) and not isinstance(uploaded_file, str):
            os.remove(temp_file_path)

    except Exception as e:
        st.error(f"âŒ è©•ä¼°éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        if os.path.exists(temp_file_path) and not isinstance(uploaded_file, str):
            os.remove(temp_file_path)
        st.stop()

    # å»ºç«‹é ç±¤
    tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8, tab9 = st.tabs(
        [
            "ğŸ“Š è©•ä¼°ç¸½è¦½",
            "ğŸ¤– GPT äººå·¥è©•å¯©",
            "ğŸ“ˆ ç¶œåˆæ¯”è¼ƒåœ–è¡¨",
            "ğŸ”¤ èªç¾©åˆ†æ",
            "ğŸ’¬ GPTåˆ†æ",
            "ğŸ¯ é—œéµè©åˆ†æ",
            "ğŸ” ç¶œåˆç¯©é¸å™¨",
            "ğŸ“¥ ä¸‹è¼‰çµæœ",
            "ğŸ“ GPT è£œå……èªªæ˜"
        ]
    )

    with tab1:
        st.markdown("### ğŸ“Š è©•ä¼°ç¸½è¦½")

        # è¨ˆç®—åŒ…å« GPT çš„ç¶œåˆè©•åˆ†
        results_df = st.session_state.comparison_results.copy()

        selected_gpt_dims = get_selected_gpt_dimensions()
        selected_gpt_weights = get_gpt_dimension_weights(selected_gpt_dims)
        selected_weight_summary = format_gpt_weight_summary(selected_gpt_dims, selected_gpt_weights)

        # åŠ å…¥ GPT è©•åˆ†ï¼ˆå¦‚æœæœ‰ï¼‰- ä½¿ç”¨å¯¦éš›åºè™Ÿè€Œé DataFrame index
        for idx in range(len(results_df)):
            # å–å¾—è©²è¡Œçš„å¯¦éš›åºè™Ÿ
            actual_q_id = int(results_df.iloc[idx]['åºè™Ÿ'])

            if actual_q_id in st.session_state.gpt_responses_original:
                gpt_data = st.session_state.gpt_responses_original[actual_q_id]
                results_df.at[idx, 'GPT_OVERALL_ORIGINAL'] = compute_gpt_overall(
                    gpt_data, selected_gpt_dims, selected_gpt_weights
                )
                results_df.at[idx, 'GPT_OVERALL_ORIGINAL_RAW'] = gpt_data.get('overall', 0)
            else:
                results_df.at[idx, 'GPT_OVERALL_ORIGINAL'] = 0
                results_df.at[idx, 'GPT_OVERALL_ORIGINAL_RAW'] = 0

            if actual_q_id in st.session_state.gpt_responses_optimized:
                gpt_data = st.session_state.gpt_responses_optimized[actual_q_id]
                results_df.at[idx, 'GPT_OVERALL_OPTIMIZED'] = compute_gpt_overall(
                    gpt_data, selected_gpt_dims, selected_gpt_weights
                )
                results_df.at[idx, 'GPT_OVERALL_OPTIMIZED_RAW'] = gpt_data.get('overall', 0)
            else:
                results_df.at[idx, 'GPT_OVERALL_OPTIMIZED'] = 0
                results_df.at[idx, 'GPT_OVERALL_OPTIMIZED_RAW'] = 0

        # é‡æ–°è¨ˆç®—ç¶œåˆè©•åˆ†ï¼ˆåŒ…å« GPTï¼‰
        # æ³¨æ„ï¼šä¸è¦†è“‹åŸå§‹ results_dfï¼Œä¿ç•™åŸå§‹çš„èªç¾©ç›¸ä¼¼åº¦åˆ†æ•¸
        results_df['FINAL_SCORE_ORIGINAL'] = (
            results_df['KEYWORD_COVERAGE_ORIGINAL'] * weights['keyword'] +
            results_df['SEMANTIC_SIMILARITY_ORIGINAL'] * weights['semantic'] +
            results_df['GPT_OVERALL_ORIGINAL'] * weights['gpt']
        )

        results_df['FINAL_SCORE_OPTIMIZED'] = (
            results_df['KEYWORD_COVERAGE_OPTIMIZED'] * weights['keyword'] +
            results_df['SEMANTIC_SIMILARITY_OPTIMIZED'] * weights['semantic'] +
            results_df['GPT_OVERALL_OPTIMIZED'] * weights['gpt']
        )

        results_df['FINAL_IMPROVEMENT'] = (
            results_df['FINAL_SCORE_OPTIMIZED'] - results_df['FINAL_SCORE_ORIGINAL']
        )

        # âš ï¸ ä¸è¦è¦†è“‹ session_stateï¼ä¿ç•™åŸå§‹è©•ä¼°æ•¸æ“š
        # st.session_state.comparison_results = results_df  # <-- ç§»é™¤é€™è¡Œ

        # é—œéµæŒ‡æ¨™å¡ç‰‡
        overview_blocks = []

        def render_overall():
            st.markdown("**ğŸ“ˆ ç¶œåˆè©•åˆ†**")
            avg_orig = results_df['FINAL_SCORE_ORIGINAL'].mean()
            avg_opt = results_df['FINAL_SCORE_OPTIMIZED'].mean()
            improvement = avg_opt - avg_orig
            color = '#28a745' if improvement > 0 else '#dc3545'
            st.markdown(f"<h1 style='color: {color}; margin: 0;'>{avg_opt:.1f}åˆ†</h1>", unsafe_allow_html=True)
            st.markdown(f"<p style='color: {color}; font-size: 18px;'>{'â†‘' if improvement > 0 else 'â†“'} {abs(improvement):.1f}åˆ†</p>", unsafe_allow_html=True)
            st.caption(
                "ä¾ç…§ç›®å‰æ¬Šé‡ (é—œéµè© {keyword:.0%} / èªç¾© {semantic:.0%} / GPT {gpt:.0%})"
                " å°æ¯é¡Œçš„ä¸‰å±¤åˆ†æ•¸åšåŠ æ¬Šå¹³å‡å¾Œï¼Œå†å–æ‰€æœ‰é¡Œç›®çš„å¹³å‡åˆ†ã€‚".format(
                    keyword=weights['keyword'],
                    semantic=weights['semantic'],
                    gpt=weights['gpt']
                )
            )

        def render_keyword():
            st.markdown("**ğŸ¯ é—œéµè©è¦†è“‹ç‡**")
            keyword_improvement = results_df['KEYWORD_COVERAGE_OPTIMIZED'].mean() - results_df['KEYWORD_COVERAGE_ORIGINAL'].mean()
            color = '#28a745' if keyword_improvement > 0 else '#dc3545'
            st.markdown(f"<h1 style='color: {color}; margin: 0;'>{results_df['KEYWORD_COVERAGE_OPTIMIZED'].mean():.1f}%</h1>", unsafe_allow_html=True)
            st.markdown(f"<p style='color: {color}; font-size: 18px;'>{'â†‘' if keyword_improvement > 0 else 'â†“'} {abs(keyword_improvement):.1f}%</p>", unsafe_allow_html=True)
            st.caption("è‡ªå‹•æ¯”å°å›ç­”èˆ‡ã€æ‡‰å›ç­”ä¹‹è©å½™ã€çš„å‘½ä¸­æ¯”ä¾‹ï¼Œå¹³å‡æ‰€æœ‰é¡Œç›®å¾Œå–å¾—æ­¤æ•¸å€¼ã€‚")

        def render_semantic():
            if enable_semantic:
                st.markdown("**ğŸ”¤ èªç¾©ç›¸ä¼¼åº¦**")
                semantic_improvement = results_df['SEMANTIC_SIMILARITY_OPTIMIZED'].mean() - results_df['SEMANTIC_SIMILARITY_ORIGINAL'].mean()
                color = '#28a745' if semantic_improvement > 0 else '#dc3545'
                st.markdown(f"<h1 style='color: {color}; margin: 0;'>{results_df['SEMANTIC_SIMILARITY_OPTIMIZED'].mean():.1f}%</h1>", unsafe_allow_html=True)
                st.markdown(f"<p style='color: {color}; font-size: 18px;'>{'â†‘' if semantic_improvement > 0 else 'â†“'} {abs(semantic_improvement):.1f}%</p>", unsafe_allow_html=True)
                st.caption("ä½¿ç”¨ Sentence-Transformers é‡æ¸¬ã€æ‡‰å›ç­”å…§å®¹ã€èˆ‡å¯¦éš›å›ç­”çš„å‘é‡é¤˜å¼¦ç›¸ä¼¼åº¦ï¼Œå–æ‰€æœ‰é¡Œç›®å¹³å‡å€¼ã€‚")
            else:
                st.info("èªç¾©ç›¸ä¼¼åº¦æœªå•Ÿç”¨")

        def render_gpt():
            if enable_manual_gpt:
                st.markdown("**ğŸ¤– GPT è©•åˆ†**")

                score_container = st.container()
                controls_container = st.container()

                if 'gpt_overview_selected_dims' not in st.session_state:
                    st.session_state.gpt_overview_selected_dims = DEFAULT_GPT_DIMENSIONS.copy()
                if 'gpt_overview_weight_inputs' not in st.session_state:
                    st.session_state.gpt_overview_weight_inputs = {
                        dim: 0.25 for dim in DEFAULT_GPT_DIMENSIONS
                    }
                if 'overview_gpt_dims' not in st.session_state:
                    st.session_state.overview_gpt_dims = st.session_state.gpt_overview_selected_dims.copy()

                available_dims = list(GPT_DIMENSION_LABELS.keys())

                selected_dims = list(st.session_state.overview_gpt_dims)
                display_warning = False
                if not selected_dims:
                    display_warning = True
                    selected_dims = DEFAULT_GPT_DIMENSIONS.copy()
                    st.session_state.overview_gpt_dims = selected_dims.copy()
                else:
                    st.session_state.gpt_overview_selected_dims = selected_dims.copy()

                # Remove weights for dims no longer selected to avoid stale data
                removed_dims = [
                    dim for dim in list(st.session_state.gpt_overview_weight_inputs.keys())
                    if dim not in selected_dims
                ]
                for dim in removed_dims:
                    st.session_state.gpt_overview_weight_inputs.pop(dim, None)
                    weight_key = f"overview_weight_{dim}"
                    if weight_key in st.session_state:
                        del st.session_state[weight_key]

                # Sync number input state before rendering score so updatedæ¬Šé‡ç«‹å³ç”Ÿæ•ˆ
                for dim in selected_dims:
                    weight_key = f"overview_weight_{dim}"
                    if weight_key in st.session_state:
                        st.session_state.gpt_overview_weight_inputs[dim] = float(st.session_state[weight_key])
                    else:
                        st.session_state.gpt_overview_weight_inputs.setdefault(dim, 0.25)

                raw_weights = {
                    dim: st.session_state.gpt_overview_weight_inputs.get(dim, 0.0)
                    for dim in selected_dims
                }
                weight_sum = sum(raw_weights.values())
                if selected_dims:
                    if weight_sum <= 0:
                        dim_weights = {dim: 1.0 / len(selected_dims) for dim in selected_dims}
                    else:
                        dim_weights = {dim: weight / weight_sum for dim, weight in raw_weights.items()}
                    summary_text = format_gpt_weight_summary(selected_dims, dim_weights)
                else:
                    dim_weights = {}
                    summary_text = "é è¨­å››ç¶­å¹³å‡"

                judge_df = st.session_state.history_manager.load_llm_judge_table()
                if judge_df is None:
                    judge_df = pd.DataFrame()
                else:
                    judge_df = judge_df.copy()

                judge_df['question_id'] = pd.to_numeric(judge_df.get('question_id'), errors='coerce')

                def compute_weighted_average(version_label: str) -> float | None:
                    subset = judge_df[judge_df['version'].astype(str).str.lower() == version_label]
                    if subset.empty:
                        return None
                    per_question_scores = []
                    for qid, group in subset.groupby('question_id'):
                        if pd.isna(qid):
                            continue
                        score_total = 0.0
                        weight_total = 0.0
                        for dim in selected_dims:
                            dim_row = group[group['dimension'] == dim]
                            if dim_row.empty:
                                continue
                            score_val = pd.to_numeric(dim_row.iloc[0].get('score'), errors='coerce')
                            if pd.isna(score_val):
                                continue
                            weight = dim_weights.get(dim, 0.0)
                            score_total += float(score_val) * weight
                            weight_total += weight
                        if weight_total > 0:
                            per_question_scores.append(score_total / weight_total)
                    if not per_question_scores:
                        return None
                    return sum(per_question_scores) / len(per_question_scores)

                avg_original = compute_weighted_average('original')
                avg_optimized = compute_weighted_average('optimized')

                display_score = avg_optimized if avg_optimized is not None else avg_original

                with score_container:
                    if judge_df.empty:
                        st.info("å°šæœªæœ‰ GPT è©•åˆ†ç´€éŒ„")
                        st.caption(f"ç•¶å‰è¨­å®šï¼š{summary_text}")
                    elif display_score is None:
                        st.info("å°šæœªè¨ˆç®— GPT è©•åˆ†")
                        st.caption(f"äººå·¥ GPT è©•å¯©ä¾ç…§{summary_text}çš„åŠ æ¬Šå¹³å‡ï¼›è‹¥å…©ç‰ˆæœ¬çš†å®Œæˆè©•å¯©æœƒé¡¯ç¤ºæ”¹é€²å¹…åº¦ã€‚")
                    else:
                        color = '#2196F3'
                        delta_html = ""
                        if avg_original is not None and avg_optimized is not None:
                            improvement = avg_optimized - avg_original
                            if improvement > 0:
                                color = '#28a745'
                                delta_html = f"<p style='color: {color}; font-size: 18px;'>â†‘ {improvement:.1f}åˆ†</p>"
                            elif improvement < 0:
                                color = '#dc3545'
                                delta_html = f"<p style='color: {color}; font-size: 18px;'>â†“ {abs(improvement):.1f}åˆ†</p>"
                            else:
                                color = '#5f6368'
                                delta_html = f"<p style='color: {color}; font-size: 18px;'>â†’ 0.0åˆ†</p>"

                        st.markdown(
                            f"<h1 style='color: {color}; margin: 0;'>{display_score:.1f}åˆ†</h1>",
                            unsafe_allow_html=True
                        )
                        if delta_html:
                            st.markdown(delta_html, unsafe_allow_html=True)
                        st.caption(f"äººå·¥ GPT è©•å¯©ä¾ç…§{summary_text}çš„åŠ æ¬Šå¹³å‡ï¼›è‹¥å…©ç‰ˆæœ¬çš†å®Œæˆè©•å¯©æœƒé¡¯ç¤ºæ”¹é€²å¹…åº¦ã€‚")

                        evaluated_question_ids = judge_df[
                            judge_df['version'].astype(str).str.lower().isin(['original', 'optimized'])
                        ]['question_id'].dropna().unique()
                        st.markdown(
                            f"<p style='font-size: 16px;'>å·²è©•å¯©é¡Œæ•¸ï¼š{len(evaluated_question_ids)}/{len(results_df)}</p>",
                            unsafe_allow_html=True
                        )

                with controls_container:
                    selected_dims_widget = st.multiselect(
                        "é¸æ“‡ç´å…¥ GPT ç¸½åˆ†çš„ç¶­åº¦",
                        options=available_dims,
                        format_func=lambda d: GPT_DIMENSION_LABELS.get(d, d),
                        key="overview_gpt_dims"
                    )

                    if display_warning:
                        st.warning("è‡³å°‘é¸æ“‡ä¸€å€‹ GPT ç¶­åº¦æ‰å¯è¨ˆç®—ç¸½åˆ†ï¼Œå·²æš«æ™‚ä½¿ç”¨é è¨­å››ç¶­ã€‚")

                    weight_cols = st.columns(len(selected_dims)) if selected_dims else []
                    for col, dim in zip(weight_cols, selected_dims):
                        with col:
                            weight_key = f"overview_weight_{dim}"
                            if weight_key not in st.session_state:
                                st.session_state[weight_key] = float(st.session_state.gpt_overview_weight_inputs.get(dim, 0.25))
                            new_value = st.number_input(
                                GPT_DIMENSION_LABELS.get(dim, dim),
                                min_value=0.0,
                                value=float(st.session_state.get(weight_key, st.session_state.gpt_overview_weight_inputs.get(dim, 0.25))),
                                step=0.05,
                                key=weight_key
                            )
                            st.session_state.gpt_overview_weight_inputs[dim] = new_value
            else:
                st.info("GPT è©•å¯©æœªå•Ÿç”¨")

        overview_blocks.append(render_overall)
        overview_blocks.append(render_keyword)

        show_semantic_metric = st.session_state.display_semantic_metric
        if enable_semantic and show_semantic_metric:
            overview_blocks.append(render_semantic)

        overview_blocks.append(render_gpt)

        cols = st.columns(len(overview_blocks))
        for col, renderer in zip(cols, overview_blocks):
            with col:
                renderer()

        # è©•ä¼°å±¤ç´šé…ç½®é¡¯ç¤º
        st.markdown("### âš™ï¸ è©•ä¼°é…ç½®")
        config_col1, config_col2, config_col3 = st.columns(3)

        with config_col1:
            st.metric("é—œéµè©åŒ¹é…", "âœ… å•Ÿç”¨", f"æ¬Šé‡: {weights['keyword']:.0%}")

        with config_col2:
            status = "âœ… å•Ÿç”¨" if enable_semantic else "âŒ åœç”¨"
            st.metric("èªç¾©ç›¸ä¼¼åº¦", status, f"æ¬Šé‡: {weights['semantic']:.0%}")

        with config_col3:
            status = "âœ… å•Ÿç”¨" if enable_manual_gpt else "âŒ åœç”¨"
            st.metric("GPT äººå·¥è©•å¯©", status, f"æ¬Šé‡: {weights['gpt']:.0%}")

    with tab2:
        st.markdown("### ğŸ¤– GPT äººå·¥è©•å¯©åŠ©æ‰‹")
        st.info("ğŸ’¡ åœ¨é€™è£¡ç”Ÿæˆ prompt â†’ è¤‡è£½åˆ° ChatGPT â†’ è²¼å›è©•åˆ†çµæœ â†’ æ‰€æœ‰æŒ‡æ¨™å³æ™‚æ›´æ–°")

        st.markdown("#### ğŸ§® GPT ç¶œåˆè©•åˆ†ç¶­åº¦")
        available_gpt_dims = list(GPT_DIMENSION_LABELS.keys())
        selected_from_widget = st.multiselect(
            "é¸æ“‡è¦ç´å…¥ç¶œåˆè©•åˆ†çš„ç¶­åº¦",
            options=available_gpt_dims,
            default=st.session_state.gpt_selected_dimensions,
            format_func=lambda x: GPT_DIMENSION_LABELS[x],
            key="gpt_dimension_selector"
        )

        if selected_from_widget:
            st.session_state.gpt_selected_dimensions = selected_from_widget
        else:
            st.warning("è‡³å°‘éœ€è¦ä¿ç•™ä¸€å€‹ç¶­åº¦æ‰èƒ½è¨ˆç®—ç¶œåˆè©•åˆ†ï¼Œå·²æ²¿ç”¨å‰æ¬¡è¨­å®šã€‚")
            if not st.session_state.gpt_selected_dimensions:
                st.session_state.gpt_selected_dimensions = available_gpt_dims
            selected_from_widget = st.session_state.gpt_selected_dimensions
        weight_cols = st.columns(len(selected_from_widget)) if selected_from_widget else []
        new_weights = {}
        for col, dim in zip(weight_cols, selected_from_widget):
            with col:
                default_weight = st.session_state.gpt_dimension_weights.get(dim, 0.25)
                new_weights[dim] = st.number_input(
                    GPT_DIMENSION_LABELS[dim],
                    min_value=0.0,
                    value=float(default_weight),
                    step=0.05,
                    key=f"weight_input_{dim}"
                )

        if new_weights:
            st.session_state.gpt_dimension_weights.update(new_weights)

        selected_gpt_dims_tab2 = st.session_state.gpt_selected_dimensions
        selected_gpt_weights_tab2 = get_gpt_dimension_weights(selected_gpt_dims_tab2)
        selected_weight_summary_tab2 = format_gpt_weight_summary(selected_gpt_dims_tab2, selected_gpt_weights_tab2)

        st.caption(
            "ç³»çµ±æœƒè‡ªå‹•æ ¹æ“šå‹¾é¸çš„ç¶­åº¦èˆ‡æŒ‡å®šæ¬Šé‡é‡æ–°è¨ˆç®—æ¯é¡Œèˆ‡æ•´é«”çš„ GPT ç¶œåˆè©•åˆ†ã€‚"
        )
        st.caption(f"ç›®å‰åŠ æ¬Šè¨­å®šï¼š{selected_weight_summary_tab2}")

        # è©•åˆ†ä¸€è‡´æ€§æŒ‡å°
        with st.expander("ğŸ“ è©•åˆ†æŒ‡æ¨™èªªæ˜", expanded=False):
            st.markdown("""
            #### ğŸ¯ ç¢ºä¿è©•åˆ†ä¸€è‡´æ€§çš„ä½¿ç”¨æ–¹æ³•
            
            **1. *åš´è¬¹ä¸”å›ºå®šçš„è©•åˆ†æ©Ÿåˆ¶**  
            æˆ‘å€‘é‡å°å››å€‹æ ¸å¿ƒç¶­åº¦ï¼ˆğŸ¯ ç›¸é—œæ€§ã€ğŸ“‹ å®Œæ•´æ€§ã€âœ… æº–ç¢ºæ€§ã€ğŸ”’ å¿ èª åº¦ï¼‰å»ºç«‹äº†æ˜ç¢ºä¸”å¯é‡åŒ–çš„è©•ä¼°æµç¨‹ï¼Œç¢ºä¿æ¯ä¸€æ¬¡è©•åˆ†éƒ½åœ¨ç›¸åŒæ¨™æº–ä¸‹åŸ·è¡Œï¼š
            - (1)å›ºå®šåˆ†ç´šæ¨™æº–ï¼šåˆ†æ•¸å€é–“èˆ‡æ¯”ä¾‹ï¼ˆå¦‚ â‰¥90% ç‚ºæ»¿åˆ†ï¼‰äº‹å…ˆå®šç¾©ï¼Œæ¨¡å‹ä¸å¾—è‡ªè¡Œèª¿æ•´ã€‚
            - (2)æ˜ç¢ºè¨ˆç®—æ–¹å¼ï¼šæ¯å€‹ç¶­åº¦å‡ä»¥æ¯”ä¾‹å…¬å¼è¨ˆç®—ï¼ˆå¦‚è²¼é¡Œå¥ï¼ç¸½å¥ã€æ­£ç¢ºå¥ï¼(æ­£ç¢ºï¼‹éŒ¯èª¤) ç­‰ï¼‰ï¼Œå†å°ç…§æ¨™æº–åˆ†æ•¸è¡¨ã€‚
            - (3)çµ±ä¸€æ¨™è¨˜æµç¨‹ï¼šæ‰€æœ‰è©•åˆ†çš†ä¾ã€Œæ‹†å¥ â†’ æ¨™è¨˜ â†’ è¨ˆç®— â†’ å°ç…§åˆ†æ•¸æ®µ â†’ è¼¸å‡ºçµæœã€äº”æ­¥é©Ÿé€²è¡Œã€‚
            - (4)è¦æ±‚å…·é«”æ•¸æ“šï¼šæ¨¡å‹å¿…é ˆåœ¨ reasoning ä¸­åˆ—å‡ºå…·é«”æ¸…å–®èˆ‡è¨ˆç®—çµæœï¼ˆå¦‚å‘½ä¸­è¦é»æ•¸ã€Supported å¥ç­‰ï¼‰ï¼Œæ¯ä¸€åˆ†çš†æœ‰ä¾æ“šã€‚
            - (5)çµ±ä¸€è¼¸å‡ºæ ¼å¼ï¼šä»¥å›ºå®š JSON æ¬„ä½å‘ˆç¾è©•åˆ†èˆ‡ reasoningï¼Œç¦æ­¢æ–°å¢æˆ–åˆªæ”¹çµæ§‹ã€‚
            - (6)äººé¡èªæ°£èˆ‡é©—è­‰æç¤ºï¼šåœ¨è§£é‡‹ä¸­ä»¥è‡ªç„¶èªæ°£èªªæ˜æ‰“åˆ†åŸå› ï¼Œä¸¦æé†’ã€Œè«‹åˆ—å‡ºæ‰€æœ‰ä¸­é–“è¨ˆç®—æ•¸æ“šã€ï¼Œå¼·åŒ–é€æ˜åº¦ã€‚
            - (7)ä¸€è‡´æ€§æé†’ï¼šPrompt é–‹é ­è¨»æ˜åƒè€ƒæµç¨‹ï¼Œçµå°¾é‡ç”³ã€Œè«‹åš´æ ¼ä¾ä¸Šè¿°æ­¥é©Ÿèˆ‡å…¬å¼åŸ·è¡Œã€ï¼Œæœçµ•ä»»æ„è®Šå‹•ã€‚    
            
            **2. è©•åˆ†æŒ‡æ¨™èªªæ˜**
            - ğŸ¯ ç›¸é—œæ€§ï¼šå›ç­”æœ‰æ²’æœ‰é‡å°å•é¡Œä¾†å›ç­”
            - ğŸ“‹ å®Œæ•´æ€§ï¼šå›ç­”æœ‰æ²’æœ‰åŒ…å«æ‡‰è©²è¦æœ‰çš„é‡è¦è³‡è¨Šï¼ˆèˆ‡è¦†è“‹ç‡çš„å·®ç•°ç‚ºæ˜¯å¦æœ‰é‡å°å…§å®¹åšæ·±åº¦è£œå……ï¼‰
            - âœ… æº–ç¢ºæ€§ï¼šå›ç­”çš„å…§å®¹æ˜¯ä¸æ˜¯æ­£ç¢ºçš„ï¼Œæœ‰æ²’æœ‰éŒ¯èª¤è³‡è¨Šï¼ˆèˆ‡å®Œæ•´æ€§çš„å·®ç•°ç‚ºä¸ç®¡æœ‰æ²’æœ‰éºæ¼ï¼Œåªçœ‹èªªçš„å…§å®¹å°ä¸å°ï¼‰
            - ğŸ”’ å¿ èª åº¦ï¼šå›æ‡‰å…§å®¹åš´æ ¼æºè‡ªåŸå§‹è³‡æ–™ï¼Œæ‰€æœ‰é™³è¿°çš†æœ‰è³‡æ–™æ”¯æ’ï¼Œæœªæ·»åŠ ä»»ä½•æœªæª¢ç´¢å¾Œæˆ–æ˜¯å…¶ä»–è£œå……çš„è³‡è¨Šã€‚ 
            
            **3. è©•åˆ†æŒ‡æ¨™ç°¡å–®èªªæ˜**
            - **ğŸ¯ ç›¸é—œæ€§**ï¼šæŠŠå›ç­”æ‹†æˆå¥å­ï¼Œæ¨™è¨˜è²¼é¡Œï¼é›¢é¡Œä¸¦è¨ˆç®—è²¼é¡Œæ¯”ä¾‹ã€‚
            - **ğŸ“‹ å®Œæ•´æ€§**ï¼šé€é …æª¢æŸ¥ã€å¿…é ˆåŒ…å«çš„é—œéµè³‡è¨Šã€‘ï¼Œæ¨™è¨˜å‘½ä¸­ï¼éƒ¨åˆ†å‘½ä¸­ï¼ç¼ºæ¼ã€‚
            - **âœ… æº–ç¢ºæ€§**ï¼šåˆ—å‡ºå¯é©—è­‰é™³è¿°ï¼Œæ¨™è¨˜æ­£ç¢ºï¼éŒ¯èª¤ï¼ä¸å¯é©—è­‰ï¼Œè¨ˆç®—æ­£ç¢ºç‡ã€‚
            - **ğŸ”’ å¿ èª åº¦**ï¼šæª¢æŸ¥æ¯å¥æ˜¯å¦æœ‰è³‡æ–™ä½è­‰ï¼Œæ¨™è¨˜ Supportedï¼Partiallyï¼Unsupportedã€‚
            
            **4. è©•åˆ†æŒ‡æ¨™è©•åˆ†è¦å‰‡**
            - **ğŸ¯ ç›¸é—œæ€§**ï¼šå°‡æ¯ä¸€å¥è©±éƒ½åˆ†æˆã€Œæœ‰å›ç­”åˆ°é¡Œç›®ã€æˆ–ã€Œåé›¢é¡Œæ„ã€ï¼Œç®—å‡ºæœ‰å¹¾å¥æ˜¯å°é¡Œç›®çš„ï¼Œç„¶å¾Œçœ‹æ¯”ä¾‹æœ‰å¤šé«˜ï¼Œé«˜æ–¼ä¹æˆå°±æ‹¿æœ€é«˜åˆ†ã€‚
            - **ğŸ“‹ å®Œæ•´æ€§**ï¼šåˆ—å‡ºä¸€å®šè¦æåˆ°çš„é‡é»ï¼Œä¾‹å¦‚ã€ŒåŸå› ã€ã€Œæ­¥é©Ÿã€ã€Œçµæœã€ç­‰ï¼Œæª¢æŸ¥å“ªäº›å®Œå…¨å¯«åˆ°äº†ï¼ˆCoveredï¼‰ã€å“ªäº›åªå¯«åˆ°ä¸€åŠï¼ˆPartiallyï¼‰ã€å“ªäº›æ²’å¯«ï¼ˆMissingï¼‰ã€‚ç”¨ã€Œå…¨éƒ¨é‡é»å‘½ä¸­çš„æ•¸é‡ï¼‹ä¸€åŠç®—éƒ¨åˆ†å‘½ä¸­ã€é™¤ä»¥é‡é»ç¸½æ•¸ï¼Œå°±èƒ½å¾—å‡ºä¸€å€‹æ¯”ä¾‹ï¼Œæ¯”ä¾‹è¶Šé«˜åˆ†æ•¸è¶Šé«˜ã€‚
            - **âœ… æº–ç¢ºæ€§**ï¼šæŠŠå›ç­”è£¡èƒ½æŸ¥è­‰çš„æ¯ä»¶äº‹éƒ½æ‹†å‡ºä¾†ï¼Œçœ‹å“ªäº›æ˜¯çœŸçš„ï¼ˆCorrectï¼‰ã€å“ªäº›æ˜é¡¯éŒ¯äº†ï¼ˆIncorrectï¼‰ã€å“ªäº›æŸ¥ä¸åˆ°ï¼ˆUnverifiableï¼‰ã€‚è¨ˆç®—ã€Œæ­£ç¢º Ã· (æ­£ç¢ºï¼‹éŒ¯èª¤)ã€ï¼Œå¦‚æœæ­£ç¢ºç‡é”åˆ° 95% å°±æ‹¿æ»¿åˆ†ã€‚
            - **ğŸ”’ å¿ èª åº¦**ï¼šæŠŠæ¯ä¸€å¥è©±éƒ½å°ç…§åŸå§‹è³‡æ–™ï¼Œçœ‹æœ‰å¤šå°‘å¥å­æ˜¯ã€Œæœ‰ä¾†æºè­‰æ˜ã€ï¼ˆSupportedï¼‰ã€å¤šå°‘å¥å­åªæ˜¯ã€Œéƒ¨åˆ†ç¬¦åˆã€ï¼ˆPartially Supportedï¼‰ã€å¤šå°‘æ˜¯ã€Œæœæ’°æˆ–æ²’ä¾æ“šã€ï¼ˆUnsupportedï¼‰ã€‚åŒæ¨£ç”¨ã€Œæœ‰ä¾†æºï¼‹ä¸€åŠç®—éƒ¨åˆ†ã€é™¤ä»¥ç¸½å¥æ•¸çš„æ–¹å¼ç®—æ¯”ä¾‹ï¼Œæ¯”ä¾‹è¶Šé«˜ä»£è¡¨è¶Šå¿ å¯¦ã€‚
            """)
            
            st.success("âœ… éµå¾ªä»¥ä¸ŠæŒ‡å°ï¼Œå¯ç¢ºä¿è©•åˆ†çš„ä¸€è‡´æ€§å’Œæº–ç¢ºæ€§ï¼")

        # é¸æ“‡è¦è©•å¯©çš„å•é¡Œ
        question_selector = st.selectbox(
            "é¸æ“‡è¦è©•å¯©çš„å•é¡Œ",
            range(len(results_df)),
            format_func=lambda x: f"å•é¡Œ {results_df.iloc[x]['åºè™Ÿ']}: {results_df.iloc[x]['æ¸¬è©¦å•é¡Œ'][:40]}..."
        )

        selected_row = results_df.iloc[question_selector]
        # ä½¿ç”¨å¯¦éš›åºè™Ÿä½œç‚º GPT è©•åˆ†çš„ keyï¼ˆè€Œä¸æ˜¯ DataFrame indexï¼‰
        actual_question_id = int(selected_row['åºè™Ÿ'])

        # é¡¯ç¤ºå•é¡Œè³‡è¨Š
        st.markdown("#### ğŸ“ å•é¡Œè³‡è¨Š")
        st.info(f"**å•é¡Œ**: {selected_row['æ¸¬è©¦å•é¡Œ']}")
        st.success(f"**æ‡‰å›ç­”è©å½™**: {selected_row['æ‡‰å›ç­”ä¹‹è©å½™']}")

        # ç”Ÿï¿½ï¿½ prompt å€åŸŸ
        version_col1, version_col2 = st.columns(2)

        with version_col1:
            st.markdown("#### ğŸ”´ åŸå§‹ç‰ˆæœ¬")

            # é¡¯ç¤ºåŸå§‹å›ç­”
            with st.expander("æŸ¥çœ‹åŸå§‹å›ç­”"):
                st.text_area("", value=selected_row['ANSWER_ORIGINAL'], height=150, key="orig_answer_view", disabled=True)

            # ç”Ÿæˆ Prompt
            prompt_original = generate_gpt_prompt(
                selected_row['æ¸¬è©¦å•é¡Œ'],
                selected_row['æ‡‰å›ç­”ä¹‹è©å½™'],
                selected_row['ANSWER_ORIGINAL'],
                version="åŸå§‹",
                question_id=selected_row['åºè™Ÿ']
            )

            st.markdown("**ğŸ“‹ GPT Promptï¼ˆè¤‡è£½åˆ° ChatGPTï¼‰**")
            st.text_area("", value=prompt_original, height=200, key=f"prompt_orig_{question_selector}")

            if st.button("ğŸ“‹ è¤‡è£½ Prompt (åŸå§‹ç‰ˆæœ¬)", key=f"copy_orig_{question_selector}"):
                st.success("âœ… Prompt å·²è¤‡è£½ï¼è«‹è²¼åˆ° ChatGPT")

            # è²¼ä¸Š GPT å›æ‡‰
            st.markdown("**ğŸ“¥ è²¼ä¸Š ChatGPT çš„ JSON å›æ‡‰**")
            gpt_response_original = st.text_area(
                "",
                height=150,
                key=f"gpt_response_orig_{question_selector}",
                placeholder='è²¼ä¸Š ChatGPT å›æ‡‰çš„ JSONï¼Œä¾‹å¦‚ï¼š\n{\n  "question_id": 12,\n  "relevance": {"score": 92, "p": 0.92, "on_topic_examples": ["..."], "off_topic_examples": [], "score_drivers": {"positive": ["..."], "negative": ["..."]}, "reasoning": "..."},\n  "completeness": {"score": 88, "q": 0.9, "k": 0.95, "covered": ["..."], "missing": [], "score_drivers": {"positive": ["..."], "negative": ["..."]}, "reasoning": "..."},\n  ...\n}'
            )

            if st.button("âœ… ç¢ºèªä¸¦å„²å­˜è©•åˆ† (åŸå§‹ç‰ˆæœ¬)", key=f"save_orig_{question_selector}"):
                if gpt_response_original.strip():
                    parsed = parse_gpt_response(gpt_response_original)
                    if 'error' not in parsed:
                        # æ–°å¢ï¼šä¸€è‡´æ€§é©—è­‰
                        warnings, errors = validate_scoring_consistency(
                            parsed, 
                            selected_row['æ¸¬è©¦å•é¡Œ'], 
                            selected_row['ANSWER_ORIGINAL']
                        )
                        
                        if errors:
                            st.error("âŒ ç™¼ç¾åš´é‡å•é¡Œï¼Œç„¡æ³•å„²å­˜ï¼š")
                            for error in errors:
                                st.text(f"â€¢ {error}")
                            st.info("ğŸ’¡ è«‹é‡æ–°è«‹ChatGPTè©•åˆ†ï¼Œç¢ºä¿åŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½")
                        elif warnings:
                            st.warning("âš ï¸ ç™¼ç¾è©•åˆ†ä¸€è‡´æ€§å•é¡Œï¼š")
                            for warning in warnings:
                                st.text(f"â€¢ {warning}")
                            
                            col_a, col_b = st.columns(2)
                            with col_a:
                                if st.button("ğŸ”„ é‡æ–°è©•åˆ†", key=f"recheck_orig_{question_selector}"):
                                    st.info("å»ºè­°é‡æ–°è«‹ChatGPTè©•åˆ†ä»¥ç¢ºä¿ä¸€è‡´æ€§")
                            with col_b:
                                if st.button("ğŸ“¥ ä»è¦å„²å­˜", key=f"force_save_orig_{question_selector}"):
                                    st.session_state.gpt_responses_original[actual_question_id] = parsed
                                    st.success("âœ… åŸå§‹ç‰ˆæœ¬è©•åˆ†å·²å„²å­˜ï¼")
                                    
                                    # è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„
                                    if auto_save_evaluation(
                                        actual_question_id,
                                        results_df,
                                        weights,
                                        selected_gpt_dims_tab2,
                                        selected_gpt_weights_tab2
                                    ):
                                        st.info("ğŸ’¾ å·²è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„")
                                    
                                    st.rerun()
                        else:
                            st.session_state.gpt_responses_original[actual_question_id] = parsed
                            st.success("âœ… åŸå§‹ç‰ˆæœ¬è©•åˆ†å·²å„²å­˜ï¼è©•åˆ†æ ¼å¼å®Œå…¨æ­£ç¢º")
                            
                            # è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„
                            if auto_save_evaluation(
                                actual_question_id,
                                results_df,
                                weights,
                                selected_gpt_dims_tab2,
                                selected_gpt_weights_tab2
                            ):
                                st.info("ğŸ’¾ å·²è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„")
                            
                            st.rerun()
                    else:
                        st.error("âŒ ç„¡æ³•è§£æ JSONï¼Œè«‹æª¢æŸ¥æ ¼å¼")
                        st.text(f"éŒ¯èª¤è©³æƒ…ï¼š{parsed.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.warning("âš ï¸ è«‹å…ˆè²¼ä¸Š ChatGPT çš„å›æ‡‰")

            # é¡¯ç¤ºå·²å„²å­˜çš„è©•åˆ†
            if actual_question_id in st.session_state.gpt_responses_original:
                gpt_data = st.session_state.gpt_responses_original[actual_question_id]
                st.markdown("**ğŸ“Š å·²å„²å­˜çš„ GPT è©•åˆ†**")
                col_a, col_b = st.columns(2)
                with col_a:
                    rel_score = get_dimension_score(gpt_data, 'relevance')
                    acc_score = get_dimension_score(gpt_data, 'accuracy')
                    st.metric("ç›¸é—œæ€§", f"{rel_score:.0f}" if rel_score is not None else "0")
                    st.metric("æº–ç¢ºæ€§", f"{acc_score:.0f}" if acc_score is not None else "0")
                with col_b:
                    comp_score = get_dimension_score(gpt_data, 'completeness')
                    faith_score = get_dimension_score(gpt_data, 'faithfulness')
                    st.metric("å®Œæ•´æ€§", f"{comp_score:.0f}" if comp_score is not None else "0")
                    st.metric("å¿ èª åº¦", f"{faith_score:.0f}" if faith_score is not None else "0")
                computed_overall = compute_gpt_overall(
                    gpt_data,
                    selected_gpt_dims_tab2,
                    selected_gpt_weights_tab2
                )
                raw_overall = gpt_data.get('overall')
                try:
                    raw_overall_value = float(raw_overall)
                except (TypeError, ValueError):
                    raw_overall_value = None

                delta_text = None
                if raw_overall_value is not None and abs(computed_overall - raw_overall_value) > 0.01:
                    delta_text = f"{computed_overall - raw_overall_value:+.1f}"

                st.metric("ç¶œåˆè©•åˆ†", f"{computed_overall:.1f}", delta=delta_text)
                if raw_overall_value is not None and delta_text:
                    st.caption(f"åŸå§‹ ChatGPT overallï¼š{raw_overall_value:.1f} (æœªä¾é¸æ“‡ç¶­åº¦èª¿æ•´)")
                st.caption(f"ç›®å‰å– {selected_weight_summary_tab2} çš„åŠ æ¬Šå¹³å‡ã€‚")

        with version_col2:
            st.markdown("#### ğŸŸ¢ å„ªåŒ–ç‰ˆæœ¬")

            # é¡¯ç¤ºå„ªåŒ–å›ç­”
            with st.expander("æŸ¥çœ‹å„ªåŒ–å›ç­”"):
                st.text_area("", value=selected_row['ANSWER_OPTIMIZED'], height=150, key="opt_answer_view", disabled=True)

            # ç”Ÿæˆ Prompt
            prompt_optimized = generate_gpt_prompt(
                selected_row['æ¸¬è©¦å•é¡Œ'],
                selected_row['æ‡‰å›ç­”ä¹‹è©å½™'],
                selected_row['ANSWER_OPTIMIZED'],
                version="å„ªåŒ–",
                question_id=selected_row['åºè™Ÿ']
            )

            st.markdown("**ğŸ“‹ GPT Promptï¼ˆè¤‡è£½åˆ° ChatGPTï¼‰**")
            st.text_area("", value=prompt_optimized, height=200, key=f"prompt_opt_{question_selector}")

            if st.button("ğŸ“‹ è¤‡è£½ Prompt (å„ªåŒ–ç‰ˆæœ¬)", key=f"copy_opt_{question_selector}"):
                st.success("âœ… Prompt å·²è¤‡è£½ï¼è«‹è²¼åˆ° ChatGPT")

            # è²¼ä¸Š GPT å›æ‡‰
            st.markdown("**ğŸ“¥ è²¼ä¸Š ChatGPT çš„ JSON å›æ‡‰**")
            gpt_response_optimized = st.text_area(
                "",
                height=150,
                key=f"gpt_response_opt_{question_selector}",
                placeholder='è²¼ä¸Š ChatGPT å›æ‡‰çš„ JSONï¼Œä¾‹å¦‚ï¼š\n{\n  "question_id": 12,\n  "relevance": {"score": 92, "p": 0.92, "on_topic_examples": ["..."], "off_topic_examples": [], "score_drivers": {"positive": ["..."], "negative": ["..."]}, "reasoning": "..."},\n  "completeness": {"score": 88, "q": 0.9, "k": 0.95, "covered": ["..."], "missing": [], "score_drivers": {"positive": ["..."], "negative": ["..."]}, "reasoning": "..."},\n  ...\n}'
            )

            if st.button("âœ… ç¢ºèªä¸¦å„²å­˜è©•åˆ† (å„ªåŒ–ç‰ˆæœ¬)", key=f"save_opt_{question_selector}"):
                if gpt_response_optimized.strip():
                    parsed = parse_gpt_response(gpt_response_optimized)
                    if 'error' not in parsed:
                        # æ–°å¢ï¼šä¸€è‡´æ€§é©—è­‰
                        warnings, errors = validate_scoring_consistency(
                            parsed, 
                            selected_row['æ¸¬è©¦å•é¡Œ'], 
                            selected_row['ANSWER_OPTIMIZED']
                        )
                        
                        if errors:
                            st.error("âŒ ç™¼ç¾åš´é‡å•é¡Œï¼Œç„¡æ³•å„²å­˜ï¼š")
                            for error in errors:
                                st.text(f"â€¢ {error}")
                            st.info("ğŸ’¡ è«‹é‡æ–°è«‹ChatGPTè©•åˆ†ï¼Œç¢ºä¿åŒ…å«æ‰€æœ‰å¿…è¦æ¬„ä½")
                        elif warnings:
                            st.warning("âš ï¸ ç™¼ç¾è©•åˆ†ä¸€è‡´æ€§å•é¡Œï¼š")
                            for warning in warnings:
                                st.text(f"â€¢ {warning}")
                            
                            col_a, col_b = st.columns(2)
                            with col_a:
                                if st.button("ğŸ”„ é‡æ–°è©•åˆ†", key=f"recheck_opt_{question_selector}"):
                                    st.info("å»ºè­°é‡æ–°è«‹ChatGPTè©•åˆ†ä»¥ç¢ºä¿ä¸€è‡´æ€§")
                            with col_b:
                                if st.button("ğŸ“¥ ä»è¦å„²å­˜", key=f"force_save_opt_{question_selector}"):
                                    st.session_state.gpt_responses_optimized[actual_question_id] = parsed
                                    st.success("âœ… å„ªåŒ–ç‰ˆæœ¬è©•åˆ†å·²å„²å­˜ï¼")
                                    
                                    # è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„
                                    if auto_save_evaluation(
                                        actual_question_id,
                                        results_df,
                                        weights,
                                        selected_gpt_dims_tab2,
                                        selected_gpt_weights_tab2
                                    ):
                                        st.info("ğŸ’¾ å·²è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„")
                                    
                                    st.rerun()
                        else:
                            st.session_state.gpt_responses_optimized[actual_question_id] = parsed
                            st.success("âœ… å„ªåŒ–ç‰ˆæœ¬è©•åˆ†å·²å„²å­˜ï¼è©•åˆ†æ ¼å¼å®Œå…¨æ­£ç¢º")
                            
                            # è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„
                            if auto_save_evaluation(
                                actual_question_id,
                                results_df,
                                weights,
                                selected_gpt_dims_tab2,
                                selected_gpt_weights_tab2
                            ):
                                st.info("ğŸ’¾ å·²è‡ªå‹•ä¿å­˜åˆ°æ­·å²ç´€éŒ„")
                            
                            st.rerun()
                    else:
                        st.error("âŒ ç„¡æ³•è§£æ JSONï¼Œè«‹æª¢æŸ¥æ ¼å¼")
                        st.text(f"éŒ¯èª¤è©³æƒ…ï¼š{parsed.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
                else:
                    st.warning("âš ï¸ è«‹å…ˆè²¼ä¸Š ChatGPT çš„å›æ‡‰")

            # é¡¯ç¤ºå·²å„²å­˜çš„è©•åˆ†
            if actual_question_id in st.session_state.gpt_responses_optimized:
                gpt_data = st.session_state.gpt_responses_optimized[actual_question_id]
                st.markdown("**ğŸ“Š å·²å„²å­˜çš„ GPT è©•åˆ†**")
                col_a, col_b = st.columns(2)
                with col_a:
                    rel_score = get_dimension_score(gpt_data, 'relevance')
                    acc_score = get_dimension_score(gpt_data, 'accuracy')
                    st.metric("ç›¸é—œæ€§", f"{rel_score:.0f}" if rel_score is not None else "0")
                    st.metric("æº–ç¢ºæ€§", f"{acc_score:.0f}" if acc_score is not None else "0")
                with col_b:
                    comp_score = get_dimension_score(gpt_data, 'completeness')
                    faith_score = get_dimension_score(gpt_data, 'faithfulness')
                    st.metric("å®Œæ•´æ€§", f"{comp_score:.0f}" if comp_score is not None else "0")
                    st.metric("å¿ èª åº¦", f"{faith_score:.0f}" if faith_score is not None else "0")
                computed_overall = compute_gpt_overall(
                    gpt_data,
                    selected_gpt_dims_tab2,
                    selected_gpt_weights_tab2
                )
                raw_overall = gpt_data.get('overall')
                try:
                    raw_overall_value = float(raw_overall)
                except (TypeError, ValueError):
                    raw_overall_value = None

                delta_text = None
                if raw_overall_value is not None and abs(computed_overall - raw_overall_value) > 0.01:
                    delta_text = f"{computed_overall - raw_overall_value:+.1f}"

                st.metric("ç¶œåˆè©•åˆ†", f"{computed_overall:.1f}", delta=delta_text)
                if raw_overall_value is not None and delta_text:
                    st.caption(f"åŸå§‹ ChatGPT overallï¼š{raw_overall_value:.1f} (æœªä¾é¸æ“‡ç¶­åº¦èª¿æ•´)")
                st.caption(f"ç›®å‰å– {selected_weight_summary_tab2} çš„åŠ æ¬Šå¹³å‡ã€‚")

        # æ‰¹æ¬¡æ“ä½œæç¤º
        st.markdown("---")
        st.markdown("### ğŸ“Š æ‰¹æ¬¡è©•å¯©é€²åº¦")

        total_questions = len(results_df)
        evaluated_original = len(st.session_state.gpt_responses_original)
        evaluated_optimized = len(st.session_state.gpt_responses_optimized)

        progress_col1, progress_col2 = st.columns(2)

        with progress_col1:
            st.metric("åŸå§‹ç‰ˆæœ¬å·²è©•å¯©", f"{evaluated_original}/{total_questions}",
                     f"{evaluated_original/total_questions*100:.1f}%")
            st.progress(evaluated_original / total_questions)

        with progress_col2:
            st.metric("å„ªåŒ–ç‰ˆæœ¬å·²è©•å¯©", f"{evaluated_optimized}/{total_questions}",
                     f"{evaluated_optimized/total_questions*100:.1f}%")
            st.progress(evaluated_optimized / total_questions)

        col_btn1, col_btn2 = st.columns(2)

        with col_btn1:
            if st.button("ğŸ’¾ æ‰‹å‹•ä¿å­˜å…¨éƒ¨åˆ°æ­·å²ç´€éŒ„", key="manual_save_all", type="primary", use_container_width=True):
                saved_count = 0
                for idx in range(len(results_df)):
                    if auto_save_evaluation(
                        idx,
                        results_df,
                        weights,
                        selected_gpt_dims_tab2,
                        selected_gpt_weights_tab2
                    ):
                        saved_count += 1
                st.success(f"âœ… æˆåŠŸä¿å­˜ {saved_count} ç­†è©•ä¼°åˆ°æ­·å²ç´€éŒ„")
                st.rerun()

        with col_btn2:
            if st.button("ğŸ”„ æ¸…é™¤æ‰€æœ‰ GPT è©•åˆ†", key="clear_all_gpt", use_container_width=True):
                st.session_state.gpt_responses_original = {}
                st.session_state.gpt_responses_optimized = {}
                st.success("âœ… å·²æ¸…é™¤æ‰€æœ‰ GPT è©•åˆ†")
                st.rerun()

    with tab3:
        st.markdown("### ğŸ“ˆ è©³ç´°å°æ¯”åˆ†æ")
        st.info("æ•´åˆä¸‰å±¤è©•ä¼°çµæœçš„å®Œæ•´å°æ¯”ï¼ˆåŒ…å«æ‚¨æä¾›çš„ GPT è©•åˆ†ï¼‰")

        # å»ºç«‹å¤šå±¤ç´šå°æ¯”è¡¨æ ¼
        comparison_data = []

        metrics = [
            ('é—œéµè©è¦†è“‹ç‡', 'KEYWORD_COVERAGE'),
        ]

        if enable_semantic:
            metrics.append(('èªç¾©ç›¸ä¼¼åº¦', 'SEMANTIC_SIMILARITY'))

        if enable_manual_gpt:
            metrics.append(('GPT è©•åˆ†', 'GPT_OVERALL'))

        metrics.append(('ç¶œåˆè©•åˆ†', 'FINAL_SCORE'))

        for metric_name, metric_key in metrics:
            comparison_data.append({
                'è©•ä¼°æŒ‡æ¨™': f'ğŸ”´ åŸå§‹ç‰ˆæœ¬ - {metric_name}',
                'å¹³å‡åˆ†æ•¸': f"{results_df[f'{metric_key}_ORIGINAL'].mean():.1f}",
                'æœ€é«˜åˆ†': f"{results_df[f'{metric_key}_ORIGINAL'].max():.1f}",
                'æœ€ä½åˆ†': f"{results_df[f'{metric_key}_ORIGINAL'].min():.1f}",
                'æ¨™æº–å·®': f"{results_df[f'{metric_key}_ORIGINAL'].std():.1f}"
            })

            comparison_data.append({
                'è©•ä¼°æŒ‡æ¨™': f'ğŸŸ¢ å„ªåŒ–ç‰ˆæœ¬ - {metric_name}',
                'å¹³å‡åˆ†æ•¸': f"{results_df[f'{metric_key}_OPTIMIZED'].mean():.1f}",
                'æœ€é«˜åˆ†': f"{results_df[f'{metric_key}_OPTIMIZED'].max():.1f}",
                'æœ€ä½åˆ†': f"{results_df[f'{metric_key}_OPTIMIZED'].min():.1f}",
                'æ¨™æº–å·®': f"{results_df[f'{metric_key}_OPTIMIZED'].std():.1f}"
            })

            improvement = results_df[f'{metric_key}_OPTIMIZED'].mean() - results_df[f'{metric_key}_ORIGINAL'].mean()
            comparison_data.append({
                'è©•ä¼°æŒ‡æ¨™': f'ğŸ“Š æ”¹å–„å¹…åº¦ - {metric_name}',
                'å¹³å‡åˆ†æ•¸': f"{improvement:+.1f}",
                'æœ€é«˜åˆ†': "-",
                'æœ€ä½åˆ†': "-",
                'æ¨™æº–å·®': "-"
            })

        comparison_df = pd.DataFrame(comparison_data)
        st.dataframe(comparison_df, use_container_width=True, hide_index=True)

        # é›·é”åœ–å°æ¯”
        st.markdown("### ğŸ¯ å¤šç¶­åº¦é›·é”åœ–å°æ¯”")

        categories = ['é—œéµè©è¦†è“‹ç‡']
        original_scores = [results_df['KEYWORD_COVERAGE_ORIGINAL'].mean()]
        optimized_scores = [results_df['KEYWORD_COVERAGE_OPTIMIZED'].mean()]

        if enable_semantic:
            categories.append('èªç¾©ç›¸ä¼¼åº¦')
            original_scores.append(results_df['SEMANTIC_SIMILARITY_ORIGINAL'].mean())
            optimized_scores.append(results_df['SEMANTIC_SIMILARITY_OPTIMIZED'].mean())

        if enable_manual_gpt:
            categories.append('GPT è©•åˆ†')
            original_scores.append(results_df['GPT_OVERALL_ORIGINAL'].mean())
            optimized_scores.append(results_df['GPT_OVERALL_OPTIMIZED'].mean())

        fig_radar = go.Figure()

        fig_radar.add_trace(go.Scatterpolar(
            r=original_scores + [original_scores[0]],
            theta=categories + [categories[0]],
            fill='toself',
            name='åŸå§‹ç‰ˆæœ¬',
            line_color='#e57373'
        ))

        fig_radar.add_trace(go.Scatterpolar(
            r=optimized_scores + [optimized_scores[0]],
            theta=categories + [categories[0]],
            fill='toself',
            name='å„ªåŒ–ç‰ˆæœ¬',
            line_color='#81c784'
        ))

        fig_radar.update_layout(
            polar=dict(radialaxis=dict(visible=True, range=[0, 100])),
            showlegend=True,
            height=500
        )

        st.plotly_chart(fig_radar, use_container_width=True)

    with tab4:
        st.markdown("### ğŸ”¤ èªç¾©å·®ç•°åˆ†æ")

        if not enable_semantic:
            st.warning("èªç¾©ç›¸ä¼¼åº¦åŠŸèƒ½æœªå•Ÿç”¨ï¼Œè«‹åœ¨å·¦å´å‹¾é¸ä¸¦ç¢ºèªç’°å¢ƒå·²å®‰è£ sentence-transformersã€‚")
        elif not st.session_state.evaluator_instance or not st.session_state.evaluator_instance.enable_semantic:
            st.warning("èªç¾©æ¨¡å‹å°šæœªè¼‰å…¥æˆåŠŸï¼Œè«‹é‡æ–°åŸ·è¡Œè©•ä¼°æˆ–æª¢æŸ¥ç’°å¢ƒè¨­å®šã€‚")
        else:
            evaluator = st.session_state.evaluator_instance

            semantic_selector = st.selectbox(
                "é¸æ“‡è¦åˆ†æçš„å•é¡Œ",
                range(len(results_df)),
                format_func=lambda x: f"å•é¡Œ {results_df.iloc[x]['åºè™Ÿ']}: {results_df.iloc[x]['æ¸¬è©¦å•é¡Œ'][:40]}...",
                key="semantic_selector"
            )

            row = results_df.iloc[semantic_selector]
            question_id = int(row['åºè™Ÿ'])
            reference_text = row['æ‡‰å›ç­”ä¹‹è©å½™']
            answer_original = row['ANSWER_ORIGINAL']
            answer_optimized = row['ANSWER_OPTIMIZED']

            st.markdown(f"#### ğŸ§¾ å•é¡Œ {question_id}: {row['æ¸¬è©¦å•é¡Œ']}")
            with st.expander("æ‡‰å›ç­”ä¹‹è©å½™ / åƒè€ƒå…§å®¹", expanded=False):
                st.write(reference_text)

            with st.expander("æŸ¥çœ‹åŸå§‹ç‰ˆæœ¬å›ç­”", expanded=False):
                st.write(answer_original)

            with st.expander("æŸ¥çœ‹å„ªåŒ–ç‰ˆæœ¬å›ç­”", expanded=False):
                st.write(answer_optimized)

            st.info(
                "èªç¾©ç›¸ä¼¼åº¦æ˜¯å°‡ã€æ‡‰å›ç­”ä¹‹è©å½™ã€èˆ‡å¯¦éš›å›ç­”åšå‘é‡æ¯”å°ï¼Œå› æ­¤å³ä½¿é—œéµè©çš†å‘½ä¸­ï¼Œ"
                "è‹¥å¥å‹ã€ç”¨è©æˆ–è£œå……å…§å®¹èˆ‡åƒè€ƒè³‡æ–™å·®ç•°å¤§ï¼Œåˆ†æ•¸ä»æœƒé™ä½ã€‚"
            )

            ideal_lines = format_reference_to_list(reference_text)
            if ideal_lines:
                st.markdown("**ç†æƒ³èªç¾©ç¤ºä¾‹ï¼ˆèˆ‡åƒè€ƒå…§å®¹å°é½Šçš„å¯«æ³•ï¼‰**")
                ideal_text = "\n".join([f"{idx + 1}. {line}" for idx, line in enumerate(ideal_lines)])
                st.code(ideal_text, language="text")

            score_col1, score_col2 = st.columns(2)

            keywords = evaluator.extract_keywords(reference_text)
            orig_kw_score, orig_matched, orig_details = evaluator.calculate_keyword_coverage(answer_original, keywords)
            opt_kw_score, opt_matched, opt_details = evaluator.calculate_keyword_coverage(answer_optimized, keywords)

            orig_sem_score, orig_sem_details = evaluator.calculate_semantic_similarity(reference_text, answer_original)
            opt_sem_score, opt_sem_details = evaluator.calculate_semantic_similarity(reference_text, answer_optimized)

            ref_sentences = split_into_sentences(reference_text)
            orig_sentence_scores = compute_sentence_similarity(evaluator, ref_sentences, answer_original)
            opt_sentence_scores = compute_sentence_similarity(evaluator, ref_sentences, answer_optimized)

            def build_sentence_table(data):
                if not data:
                    return pd.DataFrame(columns=["å¥å­", "èˆ‡å›ç­”ç›¸ä¼¼åº¦"])
                sorted_data = sorted(data, key=lambda x: x[1])
                top_items = sorted_data[:3]
                return pd.DataFrame([
                    {"å¥å­": sent, "èˆ‡å›ç­”ç›¸ä¼¼åº¦": f"{score:.1f}%"} for sent, score in top_items
                ])

            with score_col1:
                st.markdown("##### ğŸ”´ åŸå§‹ç‰ˆæœ¬")
                st.metric("èªç¾©ç›¸ä¼¼åº¦", f"{row['SEMANTIC_SIMILARITY_ORIGINAL']:.1f}%")
                st.caption(
                    f"é¤˜å¼¦ç›¸ä¼¼åº¦ï¼š{orig_sem_details.get('raw_similarity', 0):.3f}ï½œåƒè€ƒé•·åº¦ï¼š{orig_sem_details.get('reference_length', len(reference_text))}ï½œå›ç­”é•·åº¦ï¼š{orig_sem_details.get('answer_length', len(str(answer_original)))}"
                )

                st.markdown("**ç¼ºæ¼é—œéµè©**")
                missing_keywords = orig_details.get('missing_list', [])
                if missing_keywords:
                    st.write('ã€'.join(missing_keywords))
                else:
                    st.success("é—œéµè©çš†å·²è¦†è“‹")

                st.markdown("**ä½ç›¸ä¼¼åº¦åƒè€ƒå¥**")
                sentence_table = build_sentence_table(orig_sentence_scores)
                if sentence_table.empty:
                    st.info("åƒè€ƒè³‡æ–™ç„¡å¯æ¯”è¼ƒå¥å­æˆ–å›ç­”ç‚ºç©ºã€‚")
                else:
                    st.table(sentence_table)

                if missing_keywords:
                    st.markdown(
                        "**å»ºè­°**ï¼šè£œå……ä¸Šè¿°ç¼ºæ¼é—œéµè©ï¼Œæˆ–å°‡å›ç­”ä¸­çš„æ•˜è¿°èª¿æ•´æˆè²¼è¿‘åƒè€ƒè³‡æ–™çš„èªå¥ï¼Œä»¥æå‡èªç¾©è¦†è“‹ç‡ã€‚"
                    )

            with score_col2:
                st.markdown("##### ğŸŸ¢ å„ªåŒ–ç‰ˆæœ¬")
                st.metric("èªç¾©ç›¸ä¼¼åº¦", f"{row['SEMANTIC_SIMILARITY_OPTIMIZED']:.1f}%")
                st.caption(
                    f"é¤˜å¼¦ç›¸ä¼¼åº¦ï¼š{opt_sem_details.get('raw_similarity', 0):.3f}ï½œåƒè€ƒé•·åº¦ï¼š{opt_sem_details.get('reference_length', len(reference_text))}ï½œå›ç­”é•·åº¦ï¼š{opt_sem_details.get('answer_length', len(str(answer_optimized)))}"
                )

                st.markdown("**ç¼ºæ¼é—œéµè©**")
                missing_keywords_opt = opt_details.get('missing_list', [])
                if missing_keywords_opt:
                    st.write('ã€'.join(missing_keywords_opt))
                else:
                    st.success("é—œéµè©çš†å·²è¦†è“‹")

                st.markdown("**ä½ç›¸ä¼¼åº¦åƒè€ƒå¥**")
                sentence_table_opt = build_sentence_table(opt_sentence_scores)
                if sentence_table_opt.empty:
                    st.info("åƒè€ƒè³‡æ–™ç„¡å¯æ¯”è¼ƒå¥å­æˆ–å›ç­”ç‚ºç©ºã€‚")
                else:
                    st.table(sentence_table_opt)

                if missing_keywords_opt:
                    st.markdown(
                        "**å»ºè­°**ï¼šè£œè¶³ç¼ºæ¼è©å½™ï¼Œä¸¦æ¯”å°ä½ç›¸ä¼¼å¥å­çš„è³‡è¨Šé‡é»ï¼Œè®“å›ç­”æ›´è²¼è¿‘åƒè€ƒå…§å®¹ã€‚"
                    )

            st.markdown("---")
            st.markdown("#### ğŸ§­ åƒè€ƒå¦‚ä½•æ’°å¯«æ›´ç†æƒ³çš„å›ç­”ï¼Ÿ")
            improvement_points = []

            if missing_keywords:
                improvement_points.append("åŸå§‹ç‰ˆæœ¬æ¼æ‰çš„é—œéµè©ï¼š" + 'ã€'.join(missing_keywords))
            if missing_keywords_opt:
                improvement_points.append("å„ªåŒ–ç‰ˆæœ¬ä»éœ€è£œå……çš„é—œéµè©ï¼š" + 'ã€'.join(missing_keywords_opt))

            if orig_sentence_scores:
                lowest_orig = sorted(orig_sentence_scores, key=lambda x: x[1])[:1]
                for sent, score in lowest_orig:
                    improvement_points.append(f"åŸå§‹ç‰ˆèˆ‡åƒè€ƒå¥ã€Œ{sent}ã€åƒ… {score:.1f}% ç›¸ä¼¼ï¼Œå¯åŠ å…¥ç›¸å°æ‡‰ç´°ç¯€ã€‚")

            if opt_sentence_scores:
                lowest_opt = sorted(opt_sentence_scores, key=lambda x: x[1])[:1]
                for sent, score in lowest_opt:
                    improvement_points.append(f"å„ªåŒ–ç‰ˆèˆ‡åƒè€ƒå¥ã€Œ{sent}ã€åƒ… {score:.1f}% ç›¸ä¼¼ï¼Œå¯å†è²¼è¿‘åŸå§‹æè¿°ã€‚")

            if improvement_points:
                for item in improvement_points:
                    st.markdown(f"- {item}")
            else:
                st.success("å…©å€‹ç‰ˆæœ¬èˆ‡åƒè€ƒå…§å®¹é«˜åº¦ä¸€è‡´ï¼Œç„¡æ˜é¡¯ç¼ºæ¼ã€‚")

    with tab5:
        st.markdown("### ğŸ’¬ GPTè©•åˆ†å°è¦½")
        st.info("ç€è¦½æ‰€æœ‰æ¸¬è©¦å•é¡Œçš„è©³ç´°è©•ä¼°çµæœ")

        # ç¯©é¸é¸é …
        filter_option = st.selectbox(
            "ç¯©é¸é¡¯ç¤º",
            ["æ‰€æœ‰å•é¡Œ", "é¡¯è‘—æ”¹å–„", "ç•¥æœ‰æ”¹å–„", "ç„¡è®ŠåŒ–", "æ•ˆæœé€€æ­¥", "å·²æœ‰ GPT è©•åˆ†", "æœªæœ‰ GPT è©•åˆ†"]
        )

        evaluated_question_ids = set(st.session_state.gpt_responses_original.keys()) | set(
            st.session_state.gpt_responses_optimized.keys()
        )

        # æ ¹æ“šæ¢ä»¶ç¯©é¸
        if filter_option == "é¡¯è‘—æ”¹å–„":
            filtered_df = results_df[results_df['FINAL_IMPROVEMENT'] >= improvement_threshold]
        elif filter_option == "ç•¥æœ‰æ”¹å–„":
            filtered_df = results_df[(results_df['FINAL_IMPROVEMENT'] > 0) & (results_df['FINAL_IMPROVEMENT'] < improvement_threshold)]
        elif filter_option == "ç„¡è®ŠåŒ–":
            filtered_df = results_df[results_df['FINAL_IMPROVEMENT'] == 0]
        elif filter_option == "æ•ˆæœé€€æ­¥":
            filtered_df = results_df[results_df['FINAL_IMPROVEMENT'] < 0]
        elif filter_option == "å·²æœ‰ GPT è©•åˆ†":
            filtered_df = results_df[results_df['åºè™Ÿ'].astype(int).isin(evaluated_question_ids)]
        elif filter_option == "æœªæœ‰ GPT è©•åˆ†":
            filtered_df = results_df[~results_df['åºè™Ÿ'].astype(int).isin(evaluated_question_ids)]
        else:
            filtered_df = results_df

        st.info(f"é¡¯ç¤º {len(filtered_df)} / {len(results_df)} å€‹å•é¡Œ")

        judge_table_df = pd.DataFrame()
        if enable_manual_gpt:
            try:
                judge_table_df = st.session_state.history_manager.load_llm_judge_table()
            except Exception:
                judge_table_df = pd.DataFrame()

        # é¡¯ç¤ºå•é¡Œåˆ—è¡¨
        for idx, row in filtered_df.iterrows():
            question_id = int(row['åºè™Ÿ'])
            improvement = row['FINAL_IMPROVEMENT']
            improvement_icon = "ğŸ“ˆ" if improvement > 0 else "ğŸ“‰" if improvement < 0 else "â¡ï¸"
            
            with st.expander(f"{improvement_icon} å•é¡Œ {row['åºè™Ÿ']}: {row['æ¸¬è©¦å•é¡Œ'][:50]}... (æ”¹å–„:{improvement:+.1f})"):
                st.markdown(f"**æ¸¬è©¦å•é¡Œ**: {row['æ¸¬è©¦å•é¡Œ']}")
                st.markdown(f"**æ‡‰å›ç­”è©å½™**: {row['æ‡‰å›ç­”ä¹‹è©å½™']}")

                # åŸºç¤è©•åˆ†å°æ¯”
                score_col1, score_col2, score_col3, score_col4 = st.columns(4)

                with score_col1:
                    st.metric(
                        "é—œéµè©è¦†è“‹ç‡",
                        f"{row['KEYWORD_COVERAGE_OPTIMIZED']:.1f}%",
                        f"{row['KEYWORD_COVERAGE_OPTIMIZED'] - row['KEYWORD_COVERAGE_ORIGINAL']:.1f}%"
                    )

                with score_col2:
                    if enable_semantic:
                        st.metric(
                            "èªç¾©ç›¸ä¼¼åº¦",
                            f"{row['SEMANTIC_SIMILARITY_OPTIMIZED']:.1f}%",
                            f"{row['SEMANTIC_SIMILARITY_OPTIMIZED'] - row['SEMANTIC_SIMILARITY_ORIGINAL']:.1f}%"
                        )

                with score_col3:
                    if enable_manual_gpt and question_id in st.session_state.gpt_responses_optimized:
                        st.metric(
                            "GPT è©•åˆ†",
                            f"{row['GPT_OVERALL_OPTIMIZED']:.1f}",
                            f"{row['GPT_OVERALL_OPTIMIZED'] - row['GPT_OVERALL_ORIGINAL']:.1f}"
                        )
                    elif enable_manual_gpt and question_id in st.session_state.gpt_responses_original:
                        st.info("åƒ…åŸå§‹ç‰ˆå·²è©•å¯©")
                    elif enable_manual_gpt:
                        st.warning("æœªè©•å¯©")

                with score_col4:
                    st.metric(
                        "ğŸ“Š ç¶œåˆè©•åˆ†",
                        f"{row['FINAL_SCORE_OPTIMIZED']:.1f}",
                        f"{row['FINAL_IMPROVEMENT']:.1f}"
                    )
                
                # å„æŒ‡æ¨™è®ŠåŒ–åŸå› åˆ†æ
                st.markdown("---")
                st.markdown("#### ğŸ” å„æŒ‡æ¨™è®ŠåŒ–åŸå› åˆ†æ")
                
                # é—œéµè©è¦†è“‹ç‡åˆ†æ
                keyword_change = row['KEYWORD_COVERAGE_OPTIMIZED'] - row['KEYWORD_COVERAGE_ORIGINAL']
                if abs(keyword_change) > 0.1:  # åªé¡¯ç¤ºæœ‰è®ŠåŒ–çš„æŒ‡æ¨™
                    st.markdown("**ğŸ¯ é—œéµè©è¦†è“‹ç‡è®ŠåŒ–åˆ†æ**")
                    
                    kw_col1, kw_col2 = st.columns(2)
                    with kw_col1:
                        st.info(f"ğŸ”´ **åŸå§‹ç‰ˆæœ¬**: {row['KEYWORD_COVERAGE_ORIGINAL']:.1f}%")
                    with kw_col2:
                        if keyword_change > 0:
                            st.success(f"ğŸŸ¢ **å„ªåŒ–ç‰ˆæœ¬**: {row['KEYWORD_COVERAGE_OPTIMIZED']:.1f}% (æå‡ {keyword_change:.1f}%)")
                        else:
                            st.error(f"ğŸ”´ **å„ªåŒ–ç‰ˆæœ¬**: {row['KEYWORD_COVERAGE_OPTIMIZED']:.1f}% (ä¸‹é™ {abs(keyword_change):.1f}%)")
                    
                    # é—œéµè©è®ŠåŒ–åŸå› åˆ†æ
                    if st.session_state.evaluator_instance:
                        evaluator = st.session_state.evaluator_instance
                        keywords = evaluator.extract_keywords(row['æ‡‰å›ç­”ä¹‹è©å½™'])
                        
                        # åŸå§‹ç‰ˆæœ¬é—œéµè©åˆ†æ
                        orig_score, orig_matched, orig_details = evaluator.calculate_keyword_coverage(
                            row['ANSWER_ORIGINAL'], keywords
                        )
                        
                        # å„ªåŒ–ç‰ˆæœ¬é—œéµè©åˆ†æ  
                        opt_score, opt_matched, opt_details = evaluator.calculate_keyword_coverage(
                            row['ANSWER_OPTIMIZED'], keywords
                        )
                        
                        # è©³ç´°é—œéµè©è¦†è“‹åˆ†æ
                        detail_col1, detail_col2 = st.columns(2)
                        
                        with detail_col1:
                            st.write("**åŸå§‹ç‰ˆæœ¬ - é—œéµè©è©³æƒ…:**")
                            orig_found = orig_matched or orig_details.get('found_list', [])
                            if orig_found:
                                st.write("âœ… **å·²è¦†è“‹é—œéµè©:**")
                                for kw in orig_found:
                                    st.write(f"  â€¢ {kw}")

                            orig_missing = [kw for kw in keywords if kw not in orig_found]
                            if orig_missing:
                                st.write("âŒ **æœªè¦†è“‹é—œéµè©:**")
                                for kw in orig_missing:
                                    st.write(f"  â€¢ {kw}")

                        with detail_col2:
                            st.write("**å„ªåŒ–ç‰ˆæœ¬ - é—œéµè©è©³æƒ…:**")
                            opt_found = opt_matched or opt_details.get('found_list', [])
                            if opt_found:
                                st.write("âœ… **å·²è¦†è“‹é—œéµè©:**")
                                for kw in opt_found:
                                    st.write(f"  â€¢ {kw}")
                            
                            opt_missing = [kw for kw in keywords if kw not in opt_found]
                            if opt_missing:
                                st.write("âŒ **æœªè¦†è“‹é—œéµè©:**")
                                for kw in opt_missing:
                                    st.write(f"  â€¢ {kw}")
                        
                        # è®ŠåŒ–æ‘˜è¦
                        if keyword_change > 0:
                            # æå‡åŸå› 
                            newly_found = set(opt_found) - set(orig_found)
                            if newly_found:
                                st.success(f"ğŸ†• **æ–°å¢å‘½ä¸­é—œéµè©**: {', '.join(newly_found)}")
                            else:
                                st.success("âœ… **æ”¹å–„åŸå› **: å„ªåŒ–ç‰ˆæœ¬æ›´å®Œæ•´åœ°åŒ…å«äº†æ—¢æœ‰é—œéµè©")
                        elif keyword_change < 0:
                            # ä¸‹é™åŸå› 
                            lost_keywords = set(orig_found) - set(opt_found)
                            if lost_keywords:
                                st.error(f"ğŸ“‰ **éºå¤±é—œéµè©**: {', '.join(lost_keywords)}")
                            else:
                                st.error("âŒ **ä¸‹é™åŸå› **: å„ªåŒ–ç‰ˆæœ¬å¯èƒ½éåº¦ç°¡åŒ–äº†é—œéµè³‡è¨Š")
                        else:
                            st.info("â¡ï¸ **é—œéµè©è¦†è“‹ç‡ç„¡è®ŠåŒ–**")
                
                # èªç¾©ç›¸ä¼¼åº¦åˆ†æ
                if enable_semantic:
                    semantic_change = row['SEMANTIC_SIMILARITY_OPTIMIZED'] - row['SEMANTIC_SIMILARITY_ORIGINAL']
                    if abs(semantic_change) > 0.1:  # åªé¡¯ç¤ºæœ‰è®ŠåŒ–çš„æŒ‡æ¨™
                        st.markdown("**ğŸ”¤ èªç¾©ç›¸ä¼¼åº¦è®ŠåŒ–åˆ†æ**")
                        
                        sem_col1, sem_col2 = st.columns(2)
                        with sem_col1:
                            st.info(f"ğŸ”´ **åŸå§‹ç‰ˆæœ¬**: {row['SEMANTIC_SIMILARITY_ORIGINAL']:.1f}%")
                        with sem_col2:
                            if semantic_change > 0:
                                st.success(f"ğŸŸ¢ **å„ªåŒ–ç‰ˆæœ¬**: {row['SEMANTIC_SIMILARITY_OPTIMIZED']:.1f}% (æå‡ {semantic_change:.1f}%)")
                            else:
                                st.error(f"ğŸŸ¢ **å„ªåŒ–ç‰ˆæœ¬**: {row['SEMANTIC_SIMILARITY_OPTIMIZED']:.1f}% (ä¸‹é™ {abs(semantic_change):.1f}%)")
                        
                        # èªç¾©ç›¸ä¼¼åº¦è®ŠåŒ–åŸå› åˆ†æ
                        if st.session_state.evaluator_instance and st.session_state.evaluator_instance.enable_semantic:
                            evaluator = st.session_state.evaluator_instance
                            
                            # è¨ˆç®—è©³ç´°èªç¾©ç›¸ä¼¼åº¦
                            orig_sem_score, orig_sem_details = evaluator.calculate_semantic_similarity(
                                row['æ‡‰å›ç­”ä¹‹è©å½™'], row['ANSWER_ORIGINAL']
                            )
                            opt_sem_score, opt_sem_details = evaluator.calculate_semantic_similarity(
                                row['æ‡‰å›ç­”ä¹‹è©å½™'], row['ANSWER_OPTIMIZED']
                            )
                            
                            if semantic_change > 0:
                                st.success(
                                    "âœ… **èªç¾©æ”¹å–„åŸå› **: å„ªåŒ–ç‰ˆæœ¬æ›´è²¼è¿‘åƒè€ƒå…§å®¹çš„è¡¨é”æ–¹å¼å’Œè©å½™é¸æ“‡"
                                )
                                st.caption(f"åŸå§‹é¤˜å¼¦ç›¸ä¼¼åº¦: {orig_sem_details.get('raw_similarity', 0):.3f} â†’ å„ªåŒ–å¾Œ: {opt_sem_details.get('raw_similarity', 0):.3f}")
                            else:
                                st.error(
                                    "âŒ **èªç¾©ä¸‹é™åŸå› **: å„ªåŒ–ç‰ˆæœ¬å¯èƒ½ä½¿ç”¨äº†è¼ƒä¸å¸¸è¦‹çš„è©å½™æˆ–è¡¨é”æ–¹å¼"
                                )
                                st.caption(f"åŸå§‹é¤˜å¼¦ç›¸ä¼¼åº¦: {orig_sem_details.get('raw_similarity', 0):.3f} â†’ å„ªåŒ–å¾Œ: {opt_sem_details.get('raw_similarity', 0):.3f}")


                # è©³ç´° GPT è©•åˆ†è§£æï¼ˆåƒ…ä½¿ç”¨ GPT çµæœï¼‰
                if enable_manual_gpt and (
                    question_id in st.session_state.gpt_responses_original
                    or question_id in st.session_state.gpt_responses_optimized
                ):
                    st.markdown("---")
                    st.markdown("#### ğŸ¤– GPT è©•ä¼°çµæœï¼ˆåƒ… GPT è³‡è¨Šï¼‰")

                    gpt_orig = st.session_state.gpt_responses_original.get(question_id, {})
                    gpt_opt = st.session_state.gpt_responses_optimized.get(question_id, {})

                    original_view = prepare_version_view(
                        judge_table_df,
                        question_id,
                        'original',
                        row['ANSWER_ORIGINAL'],
                        gpt_orig
                    )
                    optimized_view = prepare_version_view(
                        judge_table_df,
                        question_id,
                        'optimized',
                        row['ANSWER_OPTIMIZED'],
                        gpt_opt
                    )

                    if not original_view['dimensions'] and not optimized_view['dimensions']:
                        st.info("å°šæœªæœ‰ GPT è©•åˆ†")
                    else:
                        overall_cols = st.columns(2)
                        with overall_cols[0]:
                            if original_view['overall'] is not None:
                                st.metric(
                                    "åŸå§‹ç‰ˆæœ¬ GPT ç¶œåˆåˆ†æ•¸",
                                    format_score(original_view['overall'])
                                )
                            if original_view['overall_reasoning']:
                                st.caption(original_view['overall_reasoning'])
                        with overall_cols[1]:
                            delta_overall = None
                            if (
                                optimized_view['overall'] is not None
                                and original_view['overall'] is not None
                            ):
                                delta_overall = optimized_view['overall'] - original_view['overall']
                            if optimized_view['overall'] is not None:
                                st.metric(
                                    "å„ªåŒ–ç‰ˆæœ¬ GPT ç¶œåˆåˆ†æ•¸",
                                    format_score(optimized_view['overall']),
                                    format_delta(delta_overall)
                                )
                            elif optimized_view['dimensions']:
                                st.metric("å„ªåŒ–ç‰ˆæœ¬ GPT ç¶œåˆåˆ†æ•¸", "â€”")
                            if optimized_view['overall_reasoning']:
                                st.caption(optimized_view['overall_reasoning'])

                        comparison_data: List[Dict[str, Any]] = []
                        for dim in GPT_DIMENSION_KEYS:
                            orig_dim = original_view['dimensions'].get(dim)
                            opt_dim = optimized_view['dimensions'].get(dim)
                            if not orig_dim and not opt_dim:
                                continue
                            orig_score = orig_dim.get('score') if orig_dim else None
                            opt_score = opt_dim.get('score') if opt_dim else None
                            delta_dim = None
                            if orig_score is not None and opt_score is not None:
                                delta_dim = opt_score - orig_score
                            comparison_data.append({
                                'dimension': dim,
                                'label': GPT_DIMENSION_LABELS.get(dim, dim),
                                'orig_score': orig_score,
                                'opt_score': opt_score,
                                'delta': delta_dim,
                                'orig_info': orig_dim,
                                'opt_info': opt_dim,
                            })

                        if comparison_data:
                            table_df = pd.DataFrame({
                                'æŒ‡æ¨™': [item['label'] for item in comparison_data],
                                'åŸå§‹ç‰ˆæœ¬': [format_score(item['orig_score']) for item in comparison_data],
                                'å„ªåŒ–ç‰ˆæœ¬': [format_score(item['opt_score']) for item in comparison_data],
                                'å·®ç•°': [format_delta(item['delta']) for item in comparison_data],
                            }).set_index('æŒ‡æ¨™')
                            st.table(table_df)

                        detail_col1, detail_col2 = st.columns(2)

                        def render_version_detail(container, title: str, view_data: Dict[str, Any]):
                            with container:
                                st.markdown(title)
                                if not view_data['dimensions']:
                                    st.info("å°šæœªæœ‰ GPT è©•åˆ†")
                                    return
                                first_section = True
                                for dim_key in GPT_DIMENSION_KEYS:
                                    dim_info = view_data['dimensions'].get(dim_key)
                                    if not dim_info:
                                        continue
                                    if not first_section:
                                        st.markdown("--------")
                                    first_section = False
                                    label = GPT_DIMENSION_LABELS.get(dim_key, dim_key)
                                    score_text = format_score(dim_info.get('score'))
                                    st.markdown(f"**{label} â€” {score_text}åˆ†**")

                                    metric_items: List[str] = []
                                    for metric_key, metric_val in (dim_info.get('metrics') or {}).items():
                                        if metric_val is None:
                                            continue
                                        if metric_key in ['p', 'q', 'r', 'g']:
                                            metric_items.append(f"{METRIC_LABELS.get(metric_key, metric_key)} {metric_val*100:.1f}%")
                                        elif metric_key == 'k':
                                            metric_items.append(f"{METRIC_LABELS.get(metric_key, metric_key)} {metric_val:.2f}")
                                        else:
                                            metric_items.append(f"{metric_key} {metric_val}")
                                    if dim_info.get('shallow_flag'):
                                        metric_items.append("æ·ºè–„ä¸Šé™ï¼šæ˜¯")
                                    if metric_items:
                                        st.caption('ï½œ'.join(metric_items))

                                    quality_notes = dim_info.get('quality_notes')
                                    if isinstance(quality_notes, dict) and quality_notes:
                                        qnote_items: List[str] = []
                                        for note_key, note_label in QUALITY_NOTE_LABELS.items():
                                            note_val = safe_float(quality_notes.get(note_key))
                                            if note_val is not None:
                                                qnote_items.append(f"{note_label} {note_val:.2f}")
                                        if qnote_items:
                                            st.caption("å“è³ªåˆ†é …ï¼š" + 'ï½œ'.join(qnote_items))

                                    positive = dim_info.get('positive') or []
                                    if positive:
                                        st.write("â¬†ï¸ **åŠ åˆ†å› ç´ **")
                                        for item in positive[:5]:
                                            st.markdown(f"- {item}")
                                        if len(positive) > 5:
                                            st.caption(f"...ï¼ˆå…± {len(positive)} é …ï¼‰")

                                    negative = dim_info.get('negative') or []
                                    if negative:
                                        st.write("â¬‡ï¸ **æ‰£åˆ†å› ç´ **")
                                        for item in negative[:5]:
                                            st.markdown(f"- {item}")
                                        if len(negative) > 5:
                                            st.caption(f"...ï¼ˆå…± {len(negative)} é …ï¼‰")

                                    reasoning = dim_info.get('reasoning')
                                    if reasoning:
                                        st.markdown("**ğŸ§  ç†ç”±**")
                                        st.write(reasoning)

                        render_version_detail(detail_col1, "##### ğŸ”´ åŸå§‹ç‰ˆæœ¬", original_view)
                        render_version_detail(detail_col2, "##### ğŸŸ¢ å„ªåŒ–ç‰ˆæœ¬", optimized_view)

                        improvements: List[str] = []
                        concerns: List[str] = []
                        for item in comparison_data:
                            delta_dim = item['delta']
                            if delta_dim is None or abs(delta_dim) <= 5:
                                continue
                            label = item['label']
                            if delta_dim > 5:
                                opt_info = item['opt_info'] or {}
                                summary = opt_info.get('reasoning') or (opt_info.get('positive') or [''])[0]
                                improvements.append(
                                    f"{label} æå‡ {delta_dim:.0f}åˆ†  \n> {summary or 'ï¼ˆGPT æœªæä¾›è©³ç´°èªªæ˜ï¼‰'}"
                                )
                            elif delta_dim < -5:
                                orig_info = item['orig_info'] or {}
                                summary = orig_info.get('reasoning') or (orig_info.get('negative') or [''])[0]
                                concerns.append(
                                    f"{label} ä¸‹é™ {delta_dim:.0f}åˆ†  \n> {summary or 'ï¼ˆGPT æœªæä¾›è©³ç´°èªªæ˜ï¼‰'}"
                                )

                        if improvements:
                            st.markdown("**âœ… ä¸»è¦æ”¹é€²**")
                            for text in improvements:
                                st.markdown(f"- {text}")
                        if concerns:
                            st.markdown("**âš ï¸ éœ€è¦æ³¨æ„**")
                            for text in concerns:
                                st.markdown(f"- {text}")
                        if not improvements and not concerns and comparison_data:
                            st.info("ğŸ’¡ å…©å€‹ç‰ˆæœ¬åœ¨å„ç¶­åº¦è¡¨ç¾ç›¸ç•¶ï¼Œå·®ç•°ä¸å¤§")

                        st.markdown("**ğŸ” æŸ¥çœ‹åŸå§‹ GPT JSON**")
                        json_col1, json_col2 = st.columns(2)
                        with json_col1:
                            st.markdown("###### åŸå§‹ç‰ˆæœ¬")
                            if gpt_orig:
                                if st.checkbox("é¡¯ç¤ºåŸå§‹ JSON", key=f"show_orig_json_{question_id}"):
                                    st.code(json.dumps(gpt_orig, ensure_ascii=False, indent=2), language="json")
                            else:
                                st.info("å°šæœªè²¼ä¸ŠåŸå§‹ç‰ˆæœ¬ GPT JSON")
                        with json_col2:
                            st.markdown("###### å„ªåŒ–ç‰ˆæœ¬")
                            if gpt_opt:
                                if st.checkbox("é¡¯ç¤ºå„ªåŒ– JSON", key=f"show_opt_json_{question_id}"):
                                    st.code(json.dumps(gpt_opt, ensure_ascii=False, indent=2), language="json")
                            else:
                                st.info("å°šæœªè²¼ä¸Šå„ªåŒ–ç‰ˆæœ¬ GPT JSON")

                        st.markdown("#### ğŸ“ å›ç­”å…§å®¹å°æ¯”")
                        answer_col1, answer_col2 = st.columns(2)
                        with answer_col1:
                            st.markdown("##### ğŸ”´ åŸå§‹ç‰ˆæœ¬å›ç­”")
                            show_orig = st.checkbox("é¡¯ç¤ºåŸå§‹å›ç­”", key=f"show_orig_{question_id}")
                            if show_orig:
                                st.text_area("", value=row['ANSWER_ORIGINAL'], height=150, key=f"orig_answer_{question_id}", disabled=True)
                        with answer_col2:
                            st.markdown("##### ğŸŸ¢ å„ªåŒ–ç‰ˆæœ¬å›ç­”")
                            show_opt = st.checkbox("é¡¯ç¤ºå„ªåŒ–å›ç­”", key=f"show_opt_{question_id}")
                            if show_opt:
                                st.text_area("", value=row['ANSWER_OPTIMIZED'], height=150, key=f"opt_answer_{question_id}", disabled=True)
    with tab7:
        render_combined_filter_tab(
            st.session_state.comparison_results,
            enable_semantic,
            enable_manual_gpt,
            history_manager=st.session_state.history_manager,
            selected_dims=selected_gpt_dims,
            dim_weights=selected_gpt_weights,
        )

    with tab8:
        st.markdown("### ğŸ“¥ ä¸‹è¼‰çµæœ")
        st.info("åŒ¯å‡ºå®Œæ•´è©•ä¼°å ±å‘Šï¼ˆåŒ…å« GPT äººå·¥è©•å¯©çµæœï¼‰")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ“Š å®Œæ•´è©•ä¼°å ±å‘Šï¼ˆExcelï¼‰")

            if st.button("ç”Ÿæˆå®Œæ•´å ±å‘Š", type="primary"):
                export_df = results_df.copy()

                gpt_columns = ['GPT_RELEVANCE', 'GPT_COMPLETENESS', 'GPT_ACCURACY', 'GPT_FAITHFULNESS']
                for col in gpt_columns:
                    export_df[f'{col}_ORIGINAL'] = 0
                    export_df[f'{col}_OPTIMIZED'] = 0

                for idx, row in export_df.iterrows():
                    question_id = int(row['åºè™Ÿ'])

                    if question_id in st.session_state.gpt_responses_original:
                        gpt_data = st.session_state.gpt_responses_original[question_id]
                        export_df.at[idx, 'GPT_RELEVANCE_ORIGINAL'] = get_dimension_score(gpt_data, 'relevance') or 0
                        export_df.at[idx, 'GPT_COMPLETENESS_ORIGINAL'] = get_dimension_score(gpt_data, 'completeness') or 0
                        export_df.at[idx, 'GPT_ACCURACY_ORIGINAL'] = get_dimension_score(gpt_data, 'accuracy') or 0
                        export_df.at[idx, 'GPT_FAITHFULNESS_ORIGINAL'] = get_dimension_score(gpt_data, 'faithfulness') or 0

                    if question_id in st.session_state.gpt_responses_optimized:
                        gpt_data = st.session_state.gpt_responses_optimized[question_id]
                        export_df.at[idx, 'GPT_RELEVANCE_OPTIMIZED'] = get_dimension_score(gpt_data, 'relevance') or 0
                        export_df.at[idx, 'GPT_COMPLETENESS_OPTIMIZED'] = get_dimension_score(gpt_data, 'completeness') or 0
                        export_df.at[idx, 'GPT_ACCURACY_OPTIMIZED'] = get_dimension_score(gpt_data, 'accuracy') or 0
                        export_df.at[idx, 'GPT_FAITHFULNESS_OPTIMIZED'] = get_dimension_score(gpt_data, 'faithfulness') or 0

                filename = f'RAGå®Œæ•´è©•ä¼°_v2_{datetime.now().strftime("%Y%m%d_%H%M%S")}.xlsx'

                with pd.ExcelWriter(filename, engine='xlsxwriter') as writer:
                    export_df.to_excel(writer, sheet_name='è©•ä¼°çµæœ', index=False)

                with open(filename, 'rb') as f:
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰å®Œæ•´å ±å‘Š",
                        data=f,
                        file_name=filename,
                        mime='application/vnd.openxmlformats-officedocument.spreadsheetml.sheet'
                    )

                if os.path.exists(filename):
                    os.remove(filename)

                st.success("âœ… å®Œæ•´å ±å‘Šå·²ç”Ÿæˆ")

        with col2:
            st.markdown("#### ğŸ“ˆ GPT è©•åˆ†æ‘˜è¦ï¼ˆJSONï¼‰")

            if st.button("åŒ¯å‡º GPT è©•åˆ†", type="secondary"):
                gpt_export = {
                    "original": st.session_state.gpt_responses_original,
                    "optimized": st.session_state.gpt_responses_optimized,
                    "metadata": {
                        "total_questions": len(results_df),
                        "evaluated_original": len(st.session_state.gpt_responses_original),
                        "evaluated_optimized": len(st.session_state.gpt_responses_optimized),
                        "export_date": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                }

                json_filename = f'GPTè©•åˆ†æ‘˜è¦_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'

                with open(json_filename, 'w', encoding='utf-8') as f:
                    json.dump(gpt_export, f, ensure_ascii=False, indent=2)

                with open(json_filename, 'rb') as f:
                    st.download_button(
                        label="ğŸ“¥ ä¸‹è¼‰ GPT è©•åˆ†",
                        data=f,
                        file_name=json_filename,
                        mime='application/json'
                    )

                if os.path.exists(json_filename):
                    os.remove(json_filename)

                st.success("âœ… GPT è©•åˆ†å·²åŒ¯å‡º")

    with tab6:
            st.markdown("### ğŸ¯ é—œéµè©åˆ†æ")
            st.info("ğŸ” é€é¡Œæª¢è¦–é—œéµè©è¦†è“‹ç‡çš„è©³ç´°è¡¨ç¾ï¼ŒåŒ…å«å·²è¦†è“‹å’Œæœªè¦†è“‹çš„é—œéµè©åˆ—è¡¨")
            
            if 'comparison_results' in st.session_state and st.session_state.evaluator_instance:
                results_df = st.session_state.comparison_results
                evaluator = st.session_state.evaluator_instance
                
                # é¡Œç›®é¸æ“‡å™¨
                keyword_selector = st.selectbox(
                    "é¸æ“‡è¦åˆ†æçš„å•é¡Œ",
                    range(len(results_df)),
                    format_func=lambda x: f"å•é¡Œ {results_df.iloc[x]['åºè™Ÿ']}: {results_df.iloc[x]['æ¸¬è©¦å•é¡Œ'][:40]}...",
                    key="keyword_selector"
                )
                
                row = results_df.iloc[keyword_selector]
                question_id = int(row['åºè™Ÿ'])
                reference_text = row['æ‡‰å›ç­”ä¹‹è©å½™']
                answer_original = row['ANSWER_ORIGINAL']
                answer_optimized = row['ANSWER_OPTIMIZED']
                
                st.markdown(f"#### ğŸ“ å•é¡Œ {question_id}: {row['æ¸¬è©¦å•é¡Œ']}")
                
                with st.expander("æ‡‰å›ç­”ä¹‹è©å½™ / åƒè€ƒå…§å®¹", expanded=False):
                    st.write(reference_text)
                
                with st.expander("æŸ¥çœ‹åŸå§‹ç‰ˆæœ¬å›ç­”", expanded=False):
                    st.write(answer_original)
                
                with st.expander("æŸ¥çœ‹å„ªåŒ–ç‰ˆæœ¬å›ç­”", expanded=False):
                    st.write(answer_optimized)
                
                st.info(
                    "ğŸ“ˆ é—œéµè©è¦†è“‹ç‡æ˜¯æª¢æŸ¥å›ç­”ä¸­æ˜¯å¦åŒ…å«ã€æ‡‰å›ç­”ä¹‹è©å½™ã€ä¸­çš„é—œéµè©å½™ã€‚"
                    "æœ¬åˆ†æå°‡æ¸…æ¥šé¡¯ç¤ºå“ªäº›è©å½™å·²è¦†è“‹ã€å“ªäº›å°šæœªè¦†è“‹ã€‚"
                )
                
                # æå–é—œéµè©å’Œè¨ˆç®—è¦†è“‹ç‡
                keywords = evaluator.extract_keywords(reference_text)
                orig_kw_score, orig_matched, orig_details = evaluator.calculate_keyword_coverage(answer_original, keywords)
                opt_kw_score, opt_matched, opt_details = evaluator.calculate_keyword_coverage(answer_optimized, keywords)
                
                # é¡¯ç¤ºæ‰€æœ‰é—œéµè©åˆ—è¡¨
                if keywords:
                    st.markdown("**ğŸ—’ï¸ æ‰€æœ‰é—œéµè©åˆ—è¡¨**")
                    keyword_list = "ã€".join(keywords)
                    st.code(keyword_list, language="text")
                else:
                    st.warning("âš ï¸ ç„¡æ³•æå–é—œéµè©")
                    st.stop()
                
                # å°æ¯”åˆ†æèˆ‡è¦†è“‹ç‡èªªæ˜
                total_keywords = len(keywords)
                orig_found = orig_matched or []
                opt_found = opt_matched or []
                orig_missing = (orig_details or {}).get('missing_list', [])
                opt_missing = (opt_details or {}).get('missing_list', [])
                found_count = len(orig_found)
                opt_found_count = len(opt_found)
                orig_hit_pct = (found_count / total_keywords * 100) if total_keywords else 0.0
                opt_hit_pct = (opt_found_count / total_keywords * 100) if total_keywords else 0.0
                change = row['KEYWORD_COVERAGE_OPTIMIZED'] - row['KEYWORD_COVERAGE_ORIGINAL']

                score_col1, score_col2 = st.columns(2)

                with score_col1:
                    st.markdown("##### ğŸ”´ åŸå§‹ç‰ˆæœ¬")
                    st.metric("é—œéµè©è¦†è“‹ç‡", f"{row['KEYWORD_COVERAGE_ORIGINAL']:.1f}%")

                    st.caption(f"å‘½ä¸­é—œéµè©æ•¸é‡ï¼š{found_count}/{total_keywords}ï¼ˆå‘½ä¸­ç‡ {orig_hit_pct:.1f}%ï¼‰")

                    st.markdown("**ğŸ“Š è¦†è“‹ç‡è¨ˆç®—èªªæ˜**")
                    if total_keywords:
                        st.write(
                            f"æ‡‰è¦†è“‹é—œéµè©å…± {total_keywords} å€‹ï¼Œå¯¦éš›å‘½ä¸­ {found_count} å€‹ï¼Œ"
                            f"è¦†è“‹ç‡ = {found_count}/{total_keywords} Ã— 100 = {row['KEYWORD_COVERAGE_ORIGINAL']:.1f}%ã€‚"
                        )
                    else:
                        st.warning("æœ¬é¡Œæœªèƒ½æ“·å–åˆ°å¯ç”¨çš„é—œéµè©ï¼Œå› æ­¤ç„¡æ³•è¨ˆç®—è¦†è“‹ç‡ã€‚")

                    st.markdown("**âœ… å·²è¦†è“‹é—œéµè©**")
                    if orig_found:
                        for kw in orig_found:
                            st.success(f"â€¢ {kw}")
                    else:
                        st.info("ç„¡å‘½ä¸­é—œéµè©")

                    st.markdown("**âŒ æœªè¦†è“‹é—œéµè©**")
                    if orig_missing:
                        for kw in orig_missing:
                            st.error(f"â€¢ {kw}")
                        st.markdown(
                            "**å»ºè­°**ï¼šåœ¨å›ç­”ä¸­åŠ å…¥ä¸Šè¿°ç¼ºæ¼çš„é—œéµè©ï¼Œæé«˜è¦†è“‹ç‡ã€‚"
                        )
                    else:
                        st.success("å…¨éƒ¨é—œéµè©å·²è¦†è“‹ï¼")

                with score_col2:
                    st.markdown("##### ğŸŸ¢ å„ªåŒ–ç‰ˆæœ¬")
                    st.metric("é—œéµè©è¦†è“‹ç‡", f"{row['KEYWORD_COVERAGE_OPTIMIZED']:.1f}%", f"{change:+.1f}%")

                    st.caption(f"å‘½ä¸­é—œéµè©æ•¸é‡ï¼š{opt_found_count}/{total_keywords}ï¼ˆå‘½ä¸­ç‡ {opt_hit_pct:.1f}%ï¼‰")

                    st.markdown("**ğŸ“Š è¦†è“‹ç‡è¨ˆç®—èªªæ˜**")
                    if total_keywords:
                        st.write(
                            f"æ‡‰è¦†è“‹é—œéµè©å…± {total_keywords} å€‹ï¼Œå„ªåŒ–ç‰ˆæœ¬å‘½ä¸­ {opt_found_count} å€‹ï¼Œ"
                            f"è¦†è“‹ç‡ = {opt_found_count}/{total_keywords} Ã— 100 = {row['KEYWORD_COVERAGE_OPTIMIZED']:.1f}%ã€‚"
                        )
                    else:
                        st.warning("æœ¬é¡Œæœªèƒ½æ“·å–åˆ°å¯ç”¨çš„é—œéµè©ï¼Œå› æ­¤ç„¡æ³•è¨ˆç®—è¦†è“‹ç‡ã€‚")

                    st.markdown("**âœ… å·²è¦†è“‹é—œéµè©**")
                    if opt_found:
                        for kw in opt_found:
                            st.success(f"â€¢ {kw}")
                    else:
                        st.info("ç„¡å‘½ä¸­é—œéµè©")

                    st.markdown("**âŒ æœªè¦†è“‹é—œéµè©**")
                    if opt_missing:
                        for kw in opt_missing:
                            st.error(f"â€¢ {kw}")
                        st.markdown(
                            "**å»ºè­°**ï¼šé€™äº›è©å½™ä»éœ€è¦è¢«æåŠä»¥é€²ä¸€æ­¥æå‡è¦†è“‹ç‡ã€‚"
                        )
                    else:
                        st.success("å…¨éƒ¨é—œéµè©å·²è¦†è“‹ï¼")

                st.markdown("---")

                st.markdown("#### ğŸ“‹ é—œéµè©å‘½ä¸­å°ç…§è¡¨")
                if keywords:
                    comparison_rows = []
                    for kw in keywords:
                        orig_status = "âœ… å‘½ä¸­" if kw in orig_found else "âŒ ç¼ºæ¼"
                        opt_status = "âœ… å‘½ä¸­" if kw in opt_found else "âŒ ç¼ºæ¼"
                        if kw in orig_found and kw not in opt_found:
                            reason = "å„ªåŒ–ç‰ˆæœ¬éºå¤±"
                        elif kw not in orig_found and kw in opt_found:
                            reason = "å„ªåŒ–ç‰ˆæœ¬è£œä¸Š"
                        elif kw not in orig_found and kw not in opt_found:
                            reason = "å…©ç‰ˆæœ¬çš†ç¼ºæ¼"
                        else:
                            reason = "å…©ç‰ˆæœ¬çš†å‘½ä¸­"
                        comparison_rows.append({
                            "é—œéµè©": kw,
                            "åŸå§‹ç‰ˆæœ¬": orig_status,
                            "å„ªåŒ–ç‰ˆæœ¬": opt_status,
                            "å·®ç•°èªªæ˜": reason
                        })

                    st.table(pd.DataFrame(comparison_rows))

                st.markdown("---")

                # è®ŠåŒ–åˆ†æ
                st.markdown("#### ğŸ”„ é—œéµè©è¦†è“‹è®ŠåŒ–åˆ†æ")

                newly_covered = sorted(set(opt_found) - set(orig_found))
                newly_lost = sorted(set(orig_found) - set(opt_found))
                remained_covered = sorted(set(orig_found) & set(opt_found))
                remained_missing = sorted(set(orig_missing) & set(opt_missing))

                change_col1, change_col2 = st.columns(2)

                with change_col1:
                    st.markdown("**ğŸ†• æ–°å¢è¦†è“‹**")
                    if newly_covered:
                        for kw in newly_covered:
                            st.success(f"â€¢ {kw}")
                        st.success(f"ğŸ‰ æ–°å¢è¦†è“‹ {len(newly_covered)} å€‹é—œéµè©ï¼")
                    else:
                        st.info("ç„¡æ–°å¢è¦†è“‹é—œéµè©")

                    st.markdown("**â¡ï¸ æŒçºŒè¦†è“‹**")
                    if remained_covered:
                        st.write(f"æŒçºŒä¿æŒè¦†è“‹ {len(remained_covered)} å€‹é—œéµè©")
                        for kw in remained_covered:
                            st.write(f"  â€¢ {kw}")
                    else:
                        st.info("ç„¡æŒçºŒè¦†è“‹çš„é—œéµè©")

                with change_col2:
                    st.markdown("**ğŸ“‰ å¤±å»è¦†è“‹**")
                    if newly_lost:
                        for kw in newly_lost:
                            st.error(f"â€¢ {kw}")
                        st.error(f"âš ï¸ å¤±å»äº† {len(newly_lost)} å€‹é—œéµè©è¦†è“‹ï¼")
                    else:
                        st.info("ç„¡å¤±å»è¦†è“‹é—œéµè©")

                    st.markdown("**âŒ æŒçºŒç¼ºæ¼**")
                    if remained_missing:
                        st.write(f"ä»æœªè¦†è“‹ {len(remained_missing)} å€‹é—œéµè©")
                        for kw in remained_missing:
                            st.write(f"  â€¢ {kw}")
                    else:
                        st.success("ç„¡æŒçºŒç¼ºæ¼çš„é—œéµè©")
                
                # çµè«–å’Œå»ºè­°
                st.markdown("---")
                st.markdown("#### ğŸ’¡ çµè«–å’Œå»ºè­°")
                
                improvement_points = []
                
                if change > 5:
                    st.success(f"ğŸ‰ **å„ªç§€è¡¨ç¾**: æœ¬é¡Œé—œéµè©è¦†è“‹ç‡å¤§å¹…æ”¹å–„ {change:.1f}%ï¼")
                elif change > 0:
                    st.success(f"âœ… **æ­£å‘æ”¹å–„**: æœ¬é¡Œé—œéµè©è¦†è“‹ç‡æå‡ {change:.1f}%")
                elif change == 0:
                    st.info("â¡ï¸ **ç¶­æŒç¾ç‹€**: æœ¬é¡Œé—œéµè©è¦†è“‹ç‡ç„¡è®ŠåŒ–")
                else:
                    st.warning(f"âš ï¸ **éœ€è¦æ”¹é€²**: æœ¬é¡Œé—œéµè©è¦†è“‹ç‡ä¸‹é™ {abs(change):.1f}%")
                
                # å…·é«”å»ºè­°
                if opt_missing:
                    improvement_points.append(f"éœ€è¦åœ¨å›ç­”ä¸­åŠ å…¥ï¼š{'ã€'.join(opt_missing)}")
                
                if newly_lost:
                    improvement_points.append(f"é¿å…éºå¤±é€™äº›é‡è¦è©å½™ï¼š{'ã€'.join(newly_lost)}")
                
                if newly_covered:
                    improvement_points.append(f"ç¹¼çºŒä¿æŒé€™äº›æ–°å¢çš„å„ªé»ï¼š{'ã€'.join(newly_covered)}")
                
                if improvement_points:
                    st.markdown("**ğŸ“ å…·é«”å»ºè­°**")
                    for point in improvement_points:
                        st.markdown(f"- {point}")
                else:
                    st.success("ğŸ† æœ¬é¡Œé—œéµè©è¦†è“‹ç‡å·²é”åˆ°ç†æƒ³ç‹€æ…‹ï¼")

            else:
                st.warning("ğŸ˜” ç„¡æ³•è¼‰å…¥è³‡æ–™ï¼Œè«‹å…ˆåœ¨ã€Œè©•ä¼°ç¸½è¦½ã€åˆ†é ä¸­å®Œæˆè©•ä¼°")

    with tab9:
        st.markdown("### ğŸ“ GPT è©•åˆ†è£œå……èªªæ˜")

        supplement_path = Path(__file__).resolve().parent / "GPTè£œå……èªªæ˜.md"

        if supplement_path.exists():
            try:
                supplement_content = supplement_path.read_text(encoding="utf-8")
                st.markdown(supplement_content)
            except Exception as exc:
                st.error(f"ç„¡æ³•è®€å– GPTè£œå……èªªæ˜.mdï¼š{exc}")
        else:
            st.warning("æ‰¾ä¸åˆ° GPTè£œå……èªªæ˜.mdï¼Œè«‹ç¢ºèªæª”æ¡ˆä½æ–¼å°ˆæ¡ˆæ ¹ç›®éŒ„ã€‚")

else:
    # æœªä¸Šå‚³æª”æ¡ˆæ™‚çš„æç¤º
    st.info("ğŸ‘ˆ è«‹å¾å´é‚Šæ¬„ä¸Šå‚³æ¸¬è©¦çµæœæª”æ¡ˆé–‹å§‹è©•ä¼°")

    # ä½¿ç”¨èªªæ˜
    with st.expander("ğŸ“– ä½¿ç”¨èªªæ˜ v2.0 - æ•´åˆäººå·¥ GPT è©•å¯©", expanded=True):
        st.markdown("""
        ### ğŸ¯ ç³»çµ±ç‰¹æ€§

        æœ¬ç³»çµ±æ¡ç”¨**ä¸‰ç¨®è©•ä¼°æ¶æ§‹ + äººå·¥ GPT è©•å¯©**ï¼Œå®Œç¾å¹³è¡¡è‡ªå‹•åŒ–èˆ‡æº–ç¢ºåº¦ï¼š

        #### ğŸ“Š è©•ä¼°æµç¨‹

        1. **ç¬¬ä¸€ç¨®ï¼šé—œéµè©åŒ¹é…**
           - ç³»çµ±è‡ªå‹•è¨ˆç®—é—œéµè©è¦†è“‹ç‡

        2. **ç¬¬äºŒç¨®ï¼šèªç¾©ç›¸ä¼¼åº¦**
           - ç³»çµ±è‡ªå‹•è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦

        3. **ç¬¬ä¸‰ç¨®ï¼šGPT as a Judge**
           - ï¼ˆä¸€ï¼‰ç³»çµ±ç”Ÿæˆæ¨™æº–åŒ– prompt
           - ï¼ˆäºŒï¼‰æ‚¨è¤‡è£½ prompt åˆ° ChatGPT
           - ï¼ˆä¸‰ï¼‰è²¼å› ChatGPT çš„ JSON å›æ‡‰
           - ï¼ˆå››ï¼‰ç³»çµ±è‡ªå‹•æ•´åˆä¸¦å³æ™‚æ›´æ–°æ‰€æœ‰æŒ‡æ¨™

        #### ğŸš€ ä½¿ç”¨æ­¥é©Ÿ

        1. ä¸Šå‚³æ¸¬è©¦çµæœæª”æ¡ˆ
        2. ç³»çµ±è‡ªå‹•å®Œæˆé—œéµè©å’Œèªç¾©è©•ä¼°
        3. é€²å…¥ã€ŒGPT äººå·¥è©•å¯©ã€é ç±¤
        4. è¤‡è£½ prompt â†’ è²¼åˆ° ChatGPT â†’ è²¼å›çµæœ
        5. åœ¨ã€Œè©•ä¼°ç¸½è¦½ã€æŸ¥çœ‹æ•´åˆå¾Œçš„å®Œæ•´çµæœ
        """)

# é å°¾
st.markdown("---")
st.markdown("""
<div style='text-align: center; color: #666;'>
    <p>RAG è©•ä¼°å„€è¡¨æ¿ v2.0 - æ•´åˆäººå·¥ GPT è©•å¯©</p>
    <p>Â© 2025 | é—œéµè© + èªç¾©ç›¸ä¼¼åº¦ + GPT äººå·¥è©•å¯© | </p>
</div>
""", unsafe_allow_html=True)
