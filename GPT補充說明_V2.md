
## 1️⃣ 相關性 (Relevance)

**評分意義**  
透過相關性 (Relevance) 評估模型「有沒有回到問題的主軸、回答的是不是題目所問」。

**參考文獻**：  
- *Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering* (FiRA) — 探討檢索／文檔排名中之細粒度相關性標註。 [連結](https://arxiv.org/abs/2008.05363)  
- *Computational Sentence-Level Metrics Predicting Human Sentence Comprehension* (Sun & Wang, 2024) — 探討句子層級「語義相關／句子貼合度」的量化方法。 [連結](https://arxiv.org/html/2403.15822v1)  
> **註記**：本框架中「拆分生成回答為句子、判斷 On-Topic/Off-Topic 並計算貼題比例 \(p\)」為 **延伸／改作設計**，基於上述文獻理念但非文獻中原始流程。

**原始方法說明（含改作部分）**：  
1. 將生成回答拆分為句子。  
2. 標註每句是否與提問相關（On-Topic）或不相關（Off-Topic）。  
3. 計算貼題比例：  
    $$
    p = \frac{\text{相關句子數}}{\text{總句數}}
    $$

4. 根據 \(p\) 給予分數。此方式強調句子層級的細粒度判斷，有效捕捉答案之覆蓋範圍與語義集中度。

**評分表**：

| 貼題比例 \(p\)         | 分數區間       | 等級說明                            |
|------------------------|----------------|-------------------------------------|
| \(p \ge 0.90\)          | 90 – 100 分     | 幾乎所有句子皆與主題密切相關         |
| \(0.80 \le p < 0.90\)   | 80 – 89 分      | 高度相關，僅少數偏離主題             |
| \(0.70 \le p < 0.80\)   | 70 – 79 分      | 大多相關，但有部分句子離題           |
| \(0.60 \le p < 0.70\)   | 60 – 69 分      | 一半以上符合主題，主軸仍可辨識       |
| \(0.50 \le p < 0.60\)   | 50 – 59 分      | 僅部分相關，主題焦點模糊             |
| \(0.40 \le p < 0.50\)   | 40 – 49 分      | 偏離主題為多，資訊混雜               |
| \(0.30 \le p < 0.40\)   | 30 – 39 分      | 少數句子相關，主題偏差明顯           |
| \(0.20 \le p < 0.30\)   | 20 – 29 分      | 幾乎未聚焦主題                       |
| \(0.10 \le p < 0.20\)   | 10 – 19 分      | 僅極少內容相關                       |
| \(p < 0.10\)            | 0 – 9 分        | 完全離題                              |

---

## 2️⃣ 完整性 (Completeness)

**評分意義**  
透過完整性（Completeness）衡量大型語言模型回答的資訊覆蓋率與內容深度。

**參考文獻**：  
- AWS Bedrock — 工具／服務導向指標：Coverage + Depth 等概念。 [連結](https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/)  
- *A framework for human evaluation of large language models in healthcare derived from literature review* (Tam et al., 2024) — 提出 QUEST 框架，涵蓋 LLM 評估之多維面向。 [連結](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11437138/)  
> **註記**：本框架中「列出必須包含的關鍵資訊要點 + 計算命中率 \(q\)」及後續分數區間設計，為 **延伸／改作設計**，基於上述文獻理念但並非原文中直接流程。

**原始方法說明（含改作部分）**：  
- 參考 AWS Bedrock 指標，將完整性拆分為以下面向：  
  - **Coverage（覆蓋率）**：答案是否包含所有關鍵要點  
  - **Depth（深度）**：對重要要點的說明是否深入  
  - **Context Utilization**：是否有效利用上下文資訊  
  - **Information Synthesis**：是否結合資料進行綜合回答  
- 參考 QUEST 框架，對回答所涉及資訊進行全面性評估。  
- 實務操作流程：  
  1. 列出【必須包含的關鍵資訊】每個要點，標記為 Covered / Partially / Missing。  
  2. 計算命中率：  
     \[
       q = \frac{\text{Covered} + 0.5 \times \text{Partially}}{\text{總要點}}
     \]  
  3. 對每個 Covered 或 Partially 的要點，額外評估是否提供充分細節、上下文或資料整合，並在 reasoning 中具體說明深度表現。  
  4. 若整體內容雖覆蓋完整但說明明顯淺薄（僅名詞羅列、缺乏邏輯或補充），則整體分數上限為 **89 分**。  
  5. 最終依下表給分。區間內可進行線性內插並四捨五入為整數。在 reasoning 中列出命中、部分命中、缺漏的要點及深度分析，最後用人類語氣總結整體影響。

**評分表**：

| 命中率 \(q\)           | 分數區間       | 等級說明                                                     |
|------------------------|----------------|--------------------------------------------------------------|
| \(q \ge 0.90\)          | 90 – 100 分     | 所有關鍵要點皆完整覆蓋，且每項具體說明充分；邏輯清晰、層次分明、具備資料整合。若內容淺薄（僅列關鍵字），最高上限 89。 |
| \(0.80 \le q < 0.90\)   | 80 – 89 分      | 幾乎涵蓋所有重點，僅極少細節遺漏或闡述不足；整體結構連貫、深度尚可。         |
| \(0.70 \le q < 0.80\)   | 70 – 79 分      | 大部分要點已覆蓋，但部分內容過於簡略；個別段落缺少背景或例證。                 |
| \(0.60 \le q < 0.70\)   | 60 – 69 分      | 覆蓋率中等（約 2/3）；關鍵說明不足、脈絡不完整。                           |
| \(0.50 \le q < 0.60\)   | 50 – 59 分      | 僅涵蓋部分重要資訊；缺少核心段落或步驟，深度不足。                         |
| \(0.40 \le q < 0.50\)   | 40 – 49 分      | 覆蓋率不足一半；內容片段化、關鍵細節缺失，理解受限。                         |
| \(0.30 \le q < 0.40\)   | 30 – 39 分      | 僅少數要點被觸及；缺乏完整框架與重點串接。                               |
| \(0.20 \le q < 0.30\)   | 20 – 29 分      | 僅零碎觸及議題；核心資訊大多遺漏，難以形成結論。                           |
| \(0.10 \le q < 0.20\)   | 10 – 19 分      | 幾乎未涵蓋主要要點；內容多為不相關或重複。                               |
| \(q < 0.10\)            | 0 – 9 分        | 完全未涵蓋任何必要資訊；缺乏結構與意義。                                 |

---

## 3️⃣ 準確性 (Accuracy)

**評分意義**  
透過準確性（Accuracy）衡量大型語言模型生成內容的事實正確性與真實性。

**參考文獻**：  
- *FActScore: Factual Consistency in LLMs* (Min et al., 2023) — 探討將生成回答拆解為可驗證「原子事實 (atomic facts)」並進行比對。 [連結](https://aclanthology.org/2023.emnlp-main.741/)  
> **註記**：若本框架在公式或分數區間方面有改作，請註明「**延伸／改作設計**」。

**原始方法說明（含改作部分）**：  
- 將回答拆解為多個可驗證的原子事實 (atomic facts)。  
- 對每個原子事實，與參考資料或可靠來源進行比對，判斷該事實是否正確。  
- 計算正確率：  
  \[
    r = \frac{\text{正確事實數}}{\text{總原子事實數}}
  \]  
- 根據 \(r\) 給予分數。此評估方式強調「事實拆解＋檢核」的可操作性。

**評分表**：

| 正確率 \(r\)            | 分數區間       | 等級說明                        |
|-------------------------|----------------|---------------------------------|
| \(r \ge 0.90\)           | 90 – 100 分     | 完全正確，無錯誤                  |
| \(0.80 \le r < 0.90\)    | 80 – 89 分      | 高正確率，極少輕微錯誤            |
| \(0.70 \le r < 0.80\)    | 70 – 79 分      | 多數正確，少數細節有誤            |
| \(0.60 \le r < 0.70\)    | 60 – 69 分      | 約 2/3 正確，錯誤開始影響判斷      |
| \(0.50 \le r < 0.60\)    | 50 – 59 分      | 一半以上正確，但錯誤明顯            |
| \(0.40 \le r < 0.50\)    | 40 – 49 分      | 正確與錯誤比例相近                |
| \(0.30 \le r < 0.40\)    | 30 – 39 分      | 錯誤為多，資訊可信度低             |
| \(0.20 \le r < 0.30\)    | 20 – 29 分      | 僅少數正確陳述                    |
| \(0.10 \le r < 0.20\)    | 10 – 19 分      | 幾乎全錯                        |
| \(r < 0.10\)             | 0 – 9 分        | 完全錯誤或虛構內容                 |

---

## 4️⃣ 忠誠度 (Faithfulness)

**評分意義**  
透過忠誠度 (Faithfulness) 衡量大型語言模型生成內容是否忠於原始檢索資料／來源文本，避免過度補充或虛構資訊。

**參考文獻**：  
- *On Faithfulness and Factuality in Abstractive Summarization* (Maynez et al., 2020) — 探討摘要生成是否忠實於原文、是否出現虛構資訊。 [連結](https://aclanthology.org/2020.acl-main.173/)  
- *Faithfulness-Aware Uncertainty Quantification for Fact-Checking the Output of Retrieval-Augmented Generation* (Fadeeva et al., 2025) — 提出在 RAG 系統中檢測忠誠性與虛構資訊（hallucination）的方法。 [連結](https://arxiv.org/abs/2505.21072)  
> **註記**：本框架中「句子層級 Supported/Partially/Unsupported 判斷 + 計算支撐比例 \(f\)」為 **延伸／改作設計**，基於上述文獻理念但非文獻中完全流程。

**原始方法說明（含改作部分）**：  
- 以句子為單位，對生成回答中每句與來源檢索資料／文本進行判斷，標註為 Supported（支撐）、Partially Supported 或 Unsupported（不支撐）。  
- 計算支撐比例：  
  \[
    f = \frac{\text{Supported 句子數}}{\text{總句子數}}
  \]  
> 若你同時考慮 Partially 支撐或做不同權重，請註明該為你延伸設計。  
- 根據 \(f\) 給予分數。此方式強調「生成內容對檢索資料／來源文本的忠實性」與「避免無支持資訊的延伸或虛構」。

**評分表**：

| 支撐比例 \(f\)           | 分數區間       | 等級說明                         |
|--------------------------|----------------|----------------------------------|
| \(f \ge 0.90\)            | 90 – 100 分     | 完全忠於原文，無誤引或虛構           |
| \(0.80 \le f < 0.90\)     | 80 – 89 分      | 絕大部分支撐正確                   |
| \(0.70 \le f < 0.80\)     | 70 – 79 分      | 多數內容忠實，少數過度延伸           |
| \(0.60 \le f < 0.70\)     | 60 – 69 分      | 約 2/3 為檢索支撐內容，部分誤引        |
| \(0.50 \le f < 0.60\)     | 50 – 59 分      | 支撐度不穩定，引用混雜               |
| \(0.40 \le f < 0.50\)     | 40 – 49 分      | 多數內容不符原文                    |
| \(0.30 \le f < 0.40\)     | 30 – 39 分      | 僅少數句子有支撐                    |
| \(0.20 \le f < 0.30\)     | 20 – 29 分      | 多數內容失真                       |
| \(0.10 \le f < 0.20\)     | 10 – 19 分      | 幾乎全部虛構或誤引                 |
| \(f < 0.10\)              | 0 – 9 分        | 完全虛構，無支撐依據                |

---

**來源彙整**：  
FiRA；句子層級相關性研究 (Sun & Wang, 2024)；AWS Bedrock 指標；Tam et al. (2024) QUEST 框架；Min et al. (2023) FActScore；Maynez et al. (2020) 忠誠度研究；Fadeeva et al. (2025) RAG 忠誠性研究。

---