這四種評分方法的參考文獻來源以及原始方法說明如下：

---

## 1️⃣ 相關性 (Relevance)

**評分意義**
透過相關性 (Relevance)評估模型「有沒有回到問題的主軸、回答的是不是題目所問」。

**參考文獻**：  
- Fine-Grained Relevance Annotations for Multi-Task Document Ranking and Question Answering (FiRA)
       [連結](https://arxiv.org/abs/2008.05363)


**原始方法說明**：  
將生成回答拆分為句子，使用GPT來模型判斷每句是否與提問相關（On-Topic）或不相關（Off-Topic）。  
以相關句子數除以總句數計算貼題比例 $$p$$，據此給予分數。  
這種方法強調句子層級的細粒度判斷，有效捕捉答案覆蓋範圍與語義集中度。

**評分表**：

| 貼題比例 $$p$$ | 分數區間 | 等級說明 |
|----------------|-----------|-----------|
| $$p \geq 0.90$$ | 90 - 100 分 | 幾乎所有句子皆與主題密切相關 |
| $$0.80 \leq p < 0.90$$ | 80 - 89 分 | 高度相關，僅少數偏離主題 |
| $$0.70 \leq p < 0.80$$ | 70 - 79 分 | 大多相關，但有部分句子離題 |
| $$0.60 \leq p < 0.70$$ | 60 - 69 分 | 一半以上符合主題，主軸仍可辨識 |
| $$0.50 \leq p < 0.60$$ | 50 - 59 分 | 僅部分相關，主題焦點模糊 |
| $$0.40 \leq p < 0.50$$ | 40 - 49 分 | 偏離主題為多，資訊混雜 |
| $$0.30 \leq p < 0.40$$ | 30 - 39 分 | 少數句子相關，主題偏差明顯 |
| $$0.20 \leq p < 0.30$$ | 20 - 29 分 | 幾乎未聚焦主題 |
| $$0.10 \leq p < 0.20$$ | 10 - 19 分 | 僅極少內容相關 |
| $$p < 0.10$$ | 0 - 9 分 | 完全離題 |

---

## 2️⃣ 完整性 (Completeness)

**評分意義**
透過完整性（Completeness）衡量大型語言模型回答的資訊覆蓋率與內容深度。

**參考文獻**：  
- AWS Bedrock — *Information Comprehensiveness* [Coverage + Depth]  
  [AWS 部落格連結](https://aws.amazon.com/blogs/machine-learning/use-custom-metrics-to-evaluate-your-generative-ai-application-with-amazon-bedrock/)  
- Tam et al., 2024 — *QUEST: A Framework for Human Evaluation of LLMs* [Comprehensiveness]  
  [npj Digital Medicine 連結](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11437138/)

**原始方法說明**：  
AWS Bedrock 將完整性拆分為四個評估面向：  
- **Coverage（覆蓋率）**：答案是否包含所有關鍵要點  
- **Depth（深度）**：對重要要點的說明是否深入  
- **Context Utilization**：是否有效利用上下文資訊  
- **Information Synthesis**：是否結合資料進行綜合回答  

QUEST 框架則通過列出任務必須包含的重點，標記答案是否涵蓋每個點，  
並同時考慮是否提供詳細解釋與補充信息，綜合評價完整性。

📋 **評分規則**：  
1. 列出【必須包含的關鍵資訊】的每個要點，並標記為 Covered / Partially / Missing。  
2. 命中率 $$q = \dfrac{\text{Covered} + 0.5 \times \text{Partially}}{\text{總要點}}$$。  
3. 對每個 Covered 或 Partially 的要點，額外評估是否提供了充分細節、上下文或資料整合，並在 reasoning 中具體說明深度表現。  
4. 若整體內容雖覆蓋完整但說明明顯淺薄（僅名詞羅列、缺乏邏輯或補充），則整體分數上限為 **89 分**。  
5. 依下表給分；區間內可線性內插並四捨五入為整數。在 reasoning 中列出命中、部分命中、缺漏的要點及深度分析，最後用人類語氣總結整體影響。  

**評分表**：

| 命中率 $$q$$ | 分數區間 | 等級說明 |
|---------------|-----------|-----------|
| $$q \geq 0.90$$ | 90 - 100 分 | 所有關鍵要點皆完整覆蓋，且每項具體說明充分；邏輯清晰、層次分明、具備資料整合。若內容淺薄（僅列關鍵字），最高上限 89。 |
| $$0.80 \leq q < 0.90$$ | 80 - 89 分 | 幾乎涵蓋所有重點，僅極少細節遺漏或闡述不足；整體結構連貫、深度尚可。 |
| $$0.70 \leq q < 0.80$$ | 70 - 79 分 | 大部分要點已覆蓋，但部分內容過於簡略；個別段落缺少背景或例證。 |
| $$0.60 \leq q < 0.70$$ | 60 - 69 分 | 覆蓋率中等（約 2/3）；關鍵說明不足、脈絡不完整。 |
| $$0.50 \leq q < 0.60$$ | 50 - 59 分 | 僅涵蓋部分重要資訊；缺少核心段落或步驟，深度不足。 |
| $$0.40 \leq q < 0.50$$ | 40 - 49 分 | 覆蓋率不足一半；內容片段化、關鍵細節缺失，理解受限。 |
| $$0.30 \leq q < 0.40$$ | 30 - 39 分 | 僅少數要點被觸及；缺乏完整框架與重點串接。 |
| $$0.20 \leq q < 0.30$$ | 20 - 29 分 | 僅零碎觸及議題；核心資訊大多遺漏，難以形成結論。 |
| $$0.10 \leq q < 0.20$$ | 10 - 19 分 | 幾乎未涵蓋主要要點；內容多為不相關或重複。 |
| $$q < 0.10$$ | 0 - 9 分 | 完全未涵蓋任何必要資訊；缺乏結構與意義。 |

---

## 3️⃣ 準確性 (Accuracy)

**評分意義**
透過準確性（Accuracy）衡量大型語言模型生成內容的事實正確性和真實性。

**參考文獻**：  
- Min et al., 2023 — *FactScore: Factual Consistency in LLMs*  
  [EMNLP 連結](https://aclanthology.org/2023.emnlp-main.741/)  
- Lee et al., 2023 — *RLAIF vs. RLHF*  
  [arXiv 連結](https://arxiv.org/abs/2309.00267)


**原始方法說明**：  
FactScore 將回答拆解為多個可驗證的「原子事實」（atomic facts），對每個事實與參考資料進行比對，判斷是否正確。  

**評分表**：

| 正確率 $$r$$ | 分數區間 | 等級說明 |
|---------------|-----------|-----------|
| $$r \geq 0.90$$ | 90 - 100 分 | 完全正確，無錯誤 |
| $$0.80 \leq r < 0.90$$ | 80 - 89 分 | 高正確率，極少輕微錯誤 |
| $$0.70 \leq r < 0.80$$ | 70 - 79 分 | 多數正確，少數細節有誤 |
| $$0.60 \leq r < 0.70$$ | 60 - 69 分 | 約 2/3 正確，錯誤開始影響判斷 |
| $$0.50 \leq r < 0.60$$ | 50 - 59 分 | 一半以上正確，但錯誤明顯 |
| $$0.40 \leq r < 0.50$$ | 40 - 49 分 | 正確與錯誤比例相近 |
| $$0.30 \leq r < 0.40$$ | 30 - 39 分 | 錯誤為多，資訊可信度低 |
| $$0.20 \leq r < 0.30$$ | 20 - 29 分 | 僅少數正確陳述 |
| $$0.10 \leq r < 0.20$$ | 10 - 19 分 | 幾乎全錯 |
| $$r < 0.10$$ | 0 - 9 分 | 完全錯誤或虛構內容 |

---

## 4️⃣ 忠誠度 (Faithfulness)

**評分意義**
透過忠誠度 (Faithfulness)衡量大型語言模型生成內容的是否會額外的過度補充相關資訊。

**參考文獻**：  
- Maynez et al., 2020 — *On Faithfulness and Factuality in Abstractive Summarization*  
  [ACL Anthology 連結](https://aclanthology.org/2020.acl-main.173/)  
- Lee et al., 2021 — *Evaluation of RAG Metrics for QA*  
  [arXiv 連結](https://arxiv.org/pdf/2407.12873.pdf)


**原始方法說明**：  
Maynez 等人使用自然語言推理（NLI）模型，對回答中每句進行判斷，  
標記為 Supported（支撐）、Partially Supported 或 Unsupported（不支撐），  
以 Supported 句子比例 $$f$$ 作為忠誠度指標。  
Lee 等人則在 RAG 問答架構中，以檢索資料為基準，分析生成回答與來源文本的一致性，  
降低非檢索結果的現象。

**評分表**：

| 支撐比例 $$f$$ | 分數區間 | 等級說明 |
|----------------|-----------|-----------|
| $$f \geq 0.90$$ | 90 - 100 分 | 完全忠於原文，無誤引或虛構 |
| $$0.80 \leq f < 0.90$$ | 80 - 89 分 | 絕大部分支撐正確 |
| $$0.70 \leq f < 0.80$$ | 70 - 79 分 | 多數內容忠實，少數過度延伸 |
| $$0.60 \leq f < 0.70$$ | 60 - 69 分 | 約 2/3 之檢索內容，部分誤引 |
| $$0.50 \leq f < 0.60$$ | 50 - 59 分 | 支撐度不穩定，引用混雜 |
| $$0.40 \leq f < 0.50$$ | 40 - 49 分 | 多數內容不符原文 |
| $$0.30 \leq f < 0.40$$ | 30 - 39 分 | 僅少數句子有支撐 |
| $$0.20 \leq f < 0.30$$ | 20 - 29 分 | 多數內容失真 |
| $$0.10 \leq f < 0.20$$ | 10 - 19 分 | 幾乎全部虛構或誤引 |
| $$f < 0.10$$ | 0 - 9 分 | 完全虛構，無支撐依據 |

---

**來源彙整**：  
FiRA, n.d., AWS Bedrock, Tam et al. (2024), Min et al. (2023), Lee et al. (2023, 2021), Maynez et al. (2020)